{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DDiRGuzgok_p"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOmu1w2HWiw35cZTHr5owBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batu-el/l65_be301_dc755/blob/main/Notebook4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "5M23IeQ3okpT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGWkHdVEobAu",
        "outputId": "07a1307e-b011-4e50-e63b-9c7dbea7c406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dgl in /usr/local/lib/python3.10/dist-packages (1.1.3)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing PyTorch Geometric\n",
            "Installing other libraries\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: lovely-tensors in /usr/local/lib/python3.10/dist-packages (0.1.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lovely-tensors) (2.1.0+cu121)\n",
            "Requirement already satisfied: lovely-numpy>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from lovely-tensors) (0.2.11)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.25.2)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.5.29)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (7.34.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (2.1.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (23.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lovely-tensors) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lovely-tensors) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl torch_geometric torch\n",
        "\n",
        "# Install required python libraries\n",
        "import os\n",
        "\n",
        "# Install PyTorch Geometric and other libraries\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    print(\"Installing PyTorch Geometric\")\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-geometric\n",
        "    print(\"Installing other libraries\")\n",
        "    !pip install networkx\n",
        "    !pip install lovely-tensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
        "\n",
        "from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n",
        "\n",
        "import lovely_tensors as lt\n",
        "lt.monkey_patch()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1oY0Vvfoomm",
        "outputId": "f2e0b79b-03a7-4a4a-ffad-af74be1a8edf"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports succeeded.\n",
            "Python version 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "PyTorch version 2.1.0+cu121\n",
            "PyG version 2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ],
      "metadata": {
        "id": "DDiRGuzgok_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Outline ###\n",
        "\n",
        "# STEP 1. - Datasets\n",
        "\n",
        "# 1.1 Synthetic Datasets\n",
        "# 1.1.1 Homophilic Node Classification\n",
        "# 1.1.2 Heterophilic Node Classification\n",
        "# 1.1.3 Homophilic Graph Classification\n",
        "# 1.1.4 Heterophilic Graph Classification\n",
        "\n",
        "# 1.2 Real Datasets\n",
        "# 1.2.1 Homophilic Node Classification - Cora\n",
        "# 1.2.2 Heterophilic Node Classification - Texas\n",
        "# 1.2.3 Homophilic Graph Classification - QM9\n",
        "# 1.2.4 Heterophilic Graph Classification - (?)\n",
        "\n",
        "# STEP 2. Models\n",
        "\n",
        "# 2.1 Baselines to Compare Model Accuracies\n",
        "# 2.1.1 GCN\n",
        "# 2.1.2 Sparse Transformer\n",
        "# 2.1.3 MPNN\n",
        "# 2.1.4 Dense Transformer with Attention Mask\n",
        "# 2.1.5 Dense Transformer with Positional Encodings\n",
        "\n",
        "# 2.2 Comparison of 2 Models: Dense (w/ PosEnc) & Sparse Transformer\n",
        "# 2.2.1 1 Head 1 Layer\n",
        "# 2.2.1 4 Head 1 Layer\n",
        "# 2.2.1 1 Head 3 Layer\n",
        "# 2.2.1 4 Head 3 Layer\n",
        "\n",
        "# STEP 3. Evaluation\n",
        "\n",
        "# Comparisons:\n",
        "# A: Adjacency vs Sparse Attention\n",
        "# B: Adjacency vs Dense Attention\n",
        "# C: Sparse Attention vs Dense Attention\n",
        "\n",
        "# 3.1 Combining Multiple Attention Matrices from 2.2\n",
        "# 3.1.1 If Edge Exists\n",
        "# 3.1.2 PCA\n",
        "\n",
        "# 3.2 1D (Vector) Similarity Comparison\n",
        "# 3.2.1 Node Degree (histogram)\n",
        "# 3.2.2 Substructures (histogram)\n",
        "\n",
        "# 3.3 2D (Matrix) Similarity Comparison\n",
        "# 3.3.1 Adjacency Matrix (Graph Edit Dist & Kernel 1 WL)\n",
        "# 3.3.2 Shortest Path (Graph Edit Dist & Kernel 1 WL)\n",
        "\n",
        "# STEP 4. Discussion\n",
        "# Note: Future research can look at how attention evolves over the course of training\n"
      ],
      "metadata": {
        "id": "Ovpc5rhYoyWS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Dataset Generation"
      ],
      "metadata": {
        "id": "UW8OLIQDolHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import numpy as np\n",
        "\n",
        "def preprocess(data, train_ratio = 0.7, val_ratio = 0.15, test_ratio = 0.15):\n",
        "    g = dataset[0]\n",
        "    y = g.ndata['label']\n",
        "    feat = g.ndata['feat']\n",
        "\n",
        "    num_nodes = len(y)\n",
        "    indices = torch.randperm(num_nodes)\n",
        "\n",
        "    num_train, num_val = int(num_nodes * train_ratio), int(num_nodes * val_ratio)\n",
        "    num_test = num_nodes - num_train - num_val\n",
        "\n",
        "    train_mask, val_mask, test_mask = torch.zeros(num_nodes, dtype=torch.bool), torch.zeros(num_nodes, dtype=torch.bool), torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[indices[:num_train]] = True\n",
        "    val_mask[indices[num_train:num_train+num_val]] = True\n",
        "    test_mask[indices[num_train+num_val:]] = True\n",
        "\n",
        "    # Convert NetworkX graph to edge list\n",
        "    src, dst = g.edges()\n",
        "    edge_list = list(zip(src.tolist(), dst.tolist()))\n",
        "    # Create a set for symmetric edges to avoid duplicates\n",
        "    symmetric_edges = set()\n",
        "\n",
        "    # Add each edge and its reverse to the set\n",
        "    for u, v in edge_list:\n",
        "        symmetric_edges.add((u, v))\n",
        "        symmetric_edges.add((v, u))\n",
        "    edge_list = list(symmetric_edges)\n",
        "\n",
        "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "    node_features = feat #[g.nodes[node]['feat'] for node in G.nodes()]\n",
        "    # Create a Data object\n",
        "    if len(np.array(node_features).shape) == 1:\n",
        "      data = Data(x=torch.tensor(np.array(node_features)).unsqueeze(1), edge_index=torch.tensor(np.array(edge_index)), y=torch.tensor(np.array(y)), train_mask=torch.tensor(np.array(train_mask)), val_mask=torch.tensor(np.array(val_mask)), test_mask=torch.tensor(np.array(test_mask)))\n",
        "    else:\n",
        "      data = Data(x=torch.tensor(np.array(node_features, dtype=float)).float(), edge_index=torch.tensor(np.array(edge_index)), y=torch.tensor(np.array(y)), train_mask=torch.tensor(np.array(train_mask)), val_mask=torch.tensor(np.array(val_mask)), test_mask=torch.tensor(np.array(test_mask)))\n",
        "    return data"
      ],
      "metadata": {
        "id": "2dNIAgTNo1lA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Synthetic graph datasets.\"\"\"\n",
        "import math\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from dgl.data.dgl_dataset import DGLBuiltinDataset\n",
        "from dgl.data.utils import save_graphs, load_graphs, _get_dgl_url, download\n",
        "from dgl import backend as F\n",
        "from dgl.batch import batch\n",
        "from dgl.convert import graph\n",
        "from dgl.transforms import reorder_graph\n",
        "\n",
        "class BAShapeDataset(DGLBuiltinDataset):\n",
        "    r\"\"\"BA-SHAPES dataset from `GNNExplainer: Generating Explanations for Graph Neural Networks\n",
        "    <https://arxiv.org/abs/1903.03894>`__\n",
        "\n",
        "    This is a synthetic dataset for node classification. It is generated by performing the\n",
        "    following steps in order.\n",
        "\n",
        "    - Construct a base Barabási–Albert (BA) graph.\n",
        "    - Construct a set of five-node house-structured network motifs.\n",
        "    - Attach the motifs to randomly selected nodes of the base graph.\n",
        "    - Perturb the graph by adding random edges.\n",
        "    - Nodes are assigned to 4 classes. Nodes of label 0 belong to the base BA graph. Nodes of\n",
        "      label 1, 2, 3 are separately at the middle, bottom, or top of houses.\n",
        "    - Generate constant feature for all nodes, which is 1.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_base_nodes : int, optional\n",
        "        Number of nodes in the base BA graph. Default: 300\n",
        "    num_base_edges_per_node : int, optional\n",
        "        Number of edges to attach from a new node to existing nodes in constructing the base BA\n",
        "        graph. Default: 5\n",
        "    num_motifs : int, optional\n",
        "        Number of house-structured network motifs to use. Default: 80\n",
        "    perturb_ratio : float, optional\n",
        "        Number of random edges to add in perturbation divided by the number of edges in the\n",
        "        original graph. Default: 0.01\n",
        "    seed : integer, random_state, or None, optional\n",
        "        Indicator of random number generation state. Default: None\n",
        "    raw_dir : str, optional\n",
        "        Raw file directory to store the processed data. Default: ~/.dgl/\n",
        "    force_reload : bool, optional\n",
        "        Whether to always generate the data from scratch rather than load a cached version.\n",
        "        Default: False\n",
        "    verbose : bool, optional\n",
        "        Whether to print progress information. Default: True\n",
        "    transform : callable, optional\n",
        "        A transform that takes in a :class:`~dgl.DGLGraph` object and returns\n",
        "        a transformed version. The :class:`~dgl.DGLGraph` object will be\n",
        "        transformed before every access. Default: None\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    num_classes : int\n",
        "        Number of node classes\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "\n",
        "    >>> from dgl.data import BAShapeDataset\n",
        "    >>> dataset = BAShapeDataset()\n",
        "    >>> dataset.num_classes\n",
        "    4\n",
        "    >>> g = dataset[0]\n",
        "    >>> label = g.ndata['label']\n",
        "    >>> feat = g.ndata['feat']\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_base_nodes=300,\n",
        "                 num_base_edges_per_node=5,\n",
        "                 num_motifs=80,\n",
        "                 perturb_ratio=0.01,\n",
        "                 seed=None,\n",
        "                 raw_dir=None,\n",
        "                 force_reload=False,\n",
        "                 verbose=True,\n",
        "                 transform=None):\n",
        "        self.num_base_nodes = num_base_nodes\n",
        "        self.num_base_edges_per_node = num_base_edges_per_node\n",
        "        self.num_motifs = num_motifs\n",
        "        self.perturb_ratio = perturb_ratio\n",
        "        self.seed = seed\n",
        "        super(BAShapeDataset, self).__init__(name='BA-SHAPES',\n",
        "                                             url=None,\n",
        "                                             raw_dir=raw_dir,\n",
        "                                             force_reload=force_reload,\n",
        "                                             verbose=verbose,\n",
        "                                             transform=transform)\n",
        "\n",
        "    def process(self):\n",
        "        g = nx.barabasi_albert_graph(self.num_base_nodes, self.num_base_edges_per_node, self.seed)\n",
        "        edges = list(g.edges())\n",
        "        src, dst = map(list, zip(*edges))\n",
        "        n = self.num_base_nodes\n",
        "\n",
        "\n",
        "        # Nodes in the base BA graph belong to class 0\n",
        "        node_labels = [0] * n\n",
        "        # The motifs will be evenly attached to the nodes in the base graph.\n",
        "        spacing = math.floor(n / self.num_motifs)\n",
        "\n",
        "        ##########################\n",
        "        # for motif_id in range(self.num_motifs):\n",
        "        #     # Construct a five-node house-structured network motif\n",
        "        #     motif_edges = [\n",
        "        #         (n, n + 1),\n",
        "        #         (n + 1, n + 2),\n",
        "        #         (n + 2, n + 3),\n",
        "        #         (n + 3, n),\n",
        "        #         (n + 4, n),\n",
        "        #         (n + 4, n + 1)\n",
        "        #     ]\n",
        "        #     motif_src, motif_dst = map(list, zip(*motif_edges))\n",
        "        #     src.extend(motif_src)\n",
        "        #     dst.extend(motif_dst)\n",
        "\n",
        "        #     # Nodes at the middle of a house belong to class 1\n",
        "        #     # Nodes at the bottom of a house belong to class 2\n",
        "        #     # Nodes at the top of a house belong to class 3\n",
        "        #     node_labels.extend([1, 1, 2, 2, 3])\n",
        "        #     # node_labels.extend([1, 1, 1, 1, 1])\n",
        "\n",
        "        #     # Attach the motif to the base BA graph\n",
        "        #     src.append(n)\n",
        "        #     dst.append(int(motif_id * spacing))\n",
        "        #     n += 5\n",
        "        # g = graph((src, dst), num_nodes=n)\n",
        "        ##########################\n",
        "\n",
        "\n",
        "        ##########################\n",
        "        # Construct an n-by-n grid\n",
        "        self.grid_size = 3\n",
        "        motif_g = nx.grid_graph([self.grid_size, self.grid_size])\n",
        "        grid_size = nx.number_of_nodes(motif_g)\n",
        "        motif_g = nx.convert_node_labels_to_integers(motif_g, first_label=0)\n",
        "        motif_edges = list(motif_g.edges())\n",
        "        motif_src, motif_dst = map(list, zip(*motif_edges))\n",
        "        motif_src, motif_dst = np.array(motif_src), np.array(motif_dst)\n",
        "\n",
        "        print(motif_edges)\n",
        "        for motif_id in range(self.num_motifs):\n",
        "            src.extend((motif_src + n).tolist())\n",
        "            dst.extend((motif_dst + n).tolist())\n",
        "            # Nodes in grids belong to class 1\n",
        "            node_labels.extend([1,1,1,1,2,1,1,1,1])\n",
        "            # Attach the motif to the base tree graph\n",
        "            src.append(n)\n",
        "            dst.append(int(motif_id * spacing))\n",
        "\n",
        "            n += grid_size\n",
        "        g = graph((src, dst), num_nodes=n)\n",
        "        ############################\n",
        "\n",
        "        # Perturb the graph by adding non-self-loop random edges\n",
        "        num_real_edges = g.num_edges()\n",
        "        max_ratio = (n * (n - 1) - num_real_edges) / num_real_edges\n",
        "        assert self.perturb_ratio <= max_ratio, \\\n",
        "            'perturb_ratio cannot exceed {:.4f}'.format(max_ratio)\n",
        "        num_random_edges = int(num_real_edges * self.perturb_ratio)\n",
        "\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        for _ in range(num_random_edges):\n",
        "            while True:\n",
        "                u = np.random.randint(0, n)\n",
        "                v = np.random.randint(0, n)\n",
        "                if (not g.has_edges_between(u, v)) and (u != v):\n",
        "                    break\n",
        "            g.add_edges(u, v)\n",
        "\n",
        "        g.ndata['label'] = F.tensor(node_labels, F.int64)\n",
        "        g.ndata['feat'] = F.ones((n, 1), F.float32, F.cpu())\n",
        "        self._graph = reorder_graph(\n",
        "            g, node_permute_algo='rcmk', edge_permute_algo='dst', store_ids=False)\n",
        "\n",
        "    @property\n",
        "    def graph_path(self):\n",
        "        return os.path.join(self.save_path, '{}_dgl_graph.bin'.format(self.name))\n",
        "\n",
        "    def save(self):\n",
        "        save_graphs(str(self.graph_path), self._graph)\n",
        "\n",
        "    def has_cache(self):\n",
        "        return os.path.exists(self.graph_path)\n",
        "\n",
        "    def load(self):\n",
        "        graphs, _ = load_graphs(str(self.graph_path))\n",
        "        self._graph = graphs[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx == 0, \"This dataset has only one graph.\"\n",
        "        if self._transform is None:\n",
        "            return self._graph\n",
        "        else:\n",
        "            return self._transform(self._graph)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return 4\n",
        "\n",
        "\n",
        "class BACommunityDataset(DGLBuiltinDataset):\n",
        "    r\"\"\"BA-COMMUNITY dataset from `GNNExplainer: Generating Explanations for Graph Neural Networks\n",
        "    <https://arxiv.org/abs/1903.03894>`__\n",
        "\n",
        "    This is a synthetic dataset for node classification. It is generated by performing the\n",
        "    following steps in order.\n",
        "\n",
        "    - Construct a base Barabási–Albert (BA) graph.\n",
        "    - Construct a set of five-node house-structured network motifs.\n",
        "    - Attach the motifs to randomly selected nodes of the base graph.\n",
        "    - Perturb the graph by adding random edges.\n",
        "    - Nodes are assigned to 4 classes. Nodes of label 0 belong to the base BA graph. Nodes of\n",
        "      label 1, 2, 3 are separately at the middle, bottom, or top of houses.\n",
        "    - Generate normally distributed features of length 10\n",
        "    - Repeat the above steps to generate another graph. Its nodes are assigned to class\n",
        "      4, 5, 6, 7. Its node features are generated with a distinct normal distribution.\n",
        "    - Join the two graphs by randomly adding edges between them.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_base_nodes : int, optional\n",
        "        Number of nodes in each base BA graph. Default: 300\n",
        "    num_base_edges_per_node : int, optional\n",
        "        Number of edges to attach from a new node to existing nodes in constructing a base BA\n",
        "        graph. Default: 4\n",
        "    num_motifs : int, optional\n",
        "        Number of house-structured network motifs to use in constructing each graph. Default: 80\n",
        "    perturb_ratio : float, optional\n",
        "        Number of random edges to add to a graph in perturbation divided by the number of original\n",
        "        edges in it. Default: 0.01\n",
        "    num_inter_edges : int, optional\n",
        "        Number of random edges to add between the two graphs. Default: 350\n",
        "    seed : integer, random_state, or None, optional\n",
        "        Indicator of random number generation state. Default: None\n",
        "    raw_dir : str, optional\n",
        "        Raw file directory to store the processed data. Default: ~/.dgl/\n",
        "    force_reload : bool, optional\n",
        "        Whether to always generate the data from scratch rather than load a cached version.\n",
        "        Default: False\n",
        "    verbose : bool, optional\n",
        "        Whether to print progress information. Default: True\n",
        "    transform : callable, optional\n",
        "        A transform that takes in a :class:`~dgl.DGLGraph` object and returns\n",
        "        a transformed version. The :class:`~dgl.DGLGraph` object will be\n",
        "        transformed before every access. Default: None\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    num_classes : int\n",
        "        Number of node classes\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "\n",
        "    >>> from dgl.data import BACommunityDataset\n",
        "    >>> dataset = BACommunityDataset()\n",
        "    >>> dataset.num_classes\n",
        "    8\n",
        "    >>> g = dataset[0]\n",
        "    >>> label = g.ndata['label']\n",
        "    >>> feat = g.ndata['feat']\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_base_nodes=300,\n",
        "                 num_base_edges_per_node=4,\n",
        "                 num_motifs=80,\n",
        "                 perturb_ratio=0.01,\n",
        "                 num_inter_edges=350,\n",
        "                 seed=None,\n",
        "                 raw_dir=None,\n",
        "                 force_reload=False,\n",
        "                 verbose=True,\n",
        "                 transform=None):\n",
        "        self.num_base_nodes = num_base_nodes\n",
        "        self.num_base_edges_per_node = num_base_edges_per_node\n",
        "        self.num_motifs = num_motifs\n",
        "        self.perturb_ratio = perturb_ratio\n",
        "        self.num_inter_edges = num_inter_edges\n",
        "        self.seed = seed\n",
        "        super(BACommunityDataset, self).__init__(name='BA-COMMUNITY',\n",
        "                                                 url=None,\n",
        "                                                 raw_dir=raw_dir,\n",
        "                                                 force_reload=force_reload,\n",
        "                                                 verbose=verbose,\n",
        "                                                 transform=transform)\n",
        "\n",
        "    def process(self):\n",
        "        if self.seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(self.seed)\n",
        "\n",
        "        # Construct two BA-SHAPES graphs\n",
        "        g1 = BAShapeDataset(self.num_base_nodes,\n",
        "                            self.num_base_edges_per_node,\n",
        "                            self.num_motifs,\n",
        "                            self.perturb_ratio,\n",
        "                            force_reload=True,\n",
        "                            verbose=False)[0]\n",
        "        g2 = BAShapeDataset(self.num_base_nodes,\n",
        "                            self.num_base_edges_per_node,\n",
        "                            self.num_motifs,\n",
        "                            self.perturb_ratio,\n",
        "                            force_reload=True,\n",
        "                            verbose=False)[0]\n",
        "\n",
        "        # Join them and randomly add edges between them\n",
        "        g = batch([g1, g2])\n",
        "        num_nodes = g.num_nodes() // 2\n",
        "        src = np.random.randint(0, num_nodes, (self.num_inter_edges,))\n",
        "        dst = np.random.randint(num_nodes, 2 * num_nodes, (self.num_inter_edges,))\n",
        "        src = F.astype(F.zerocopy_from_numpy(src), g.idtype)\n",
        "        dst = F.astype(F.zerocopy_from_numpy(dst), g.idtype)\n",
        "        g.add_edges(src, dst)\n",
        "        # print(g1.ndata['label'] )\n",
        "        # print(g2.ndata['label'] + 2)\n",
        "        # print(torch.tensor(np.concatenate([g1.ndata['label'], g2.ndata['label'] + 2])))\n",
        "        g.ndata['label'] = F.cat([g1.ndata['label'], g2.ndata['label'] + 3], dim=0)\n",
        "\n",
        "        # feature generation\n",
        "        random_mu = [0.0] * 8\n",
        "        random_sigma = [1.0] * 8\n",
        "\n",
        "        mu_1, sigma_1 = np.array([-1.0] * 2 + random_mu), np.array([0.5] * 2 + random_sigma)\n",
        "        feat1 = np.random.multivariate_normal(mu_1, np.diag(sigma_1), num_nodes)\n",
        "\n",
        "        mu_2, sigma_2 = np.array([1.0] * 2 + random_mu), np.array([0.5] * 2 + random_sigma)\n",
        "        feat2 = np.random.multivariate_normal(mu_2, np.diag(sigma_2), num_nodes)\n",
        "\n",
        "        feat = np.concatenate([feat1, feat2])\n",
        "        g.ndata['feat'] = F.zerocopy_from_numpy(feat)\n",
        "        self._graph = reorder_graph(\n",
        "            g, node_permute_algo='rcmk', edge_permute_algo='dst', store_ids=False)\n",
        "\n",
        "    @property\n",
        "    def graph_path(self):\n",
        "        return os.path.join(self.save_path, '{}_dgl_graph.bin'.format(self.name))\n",
        "\n",
        "    def save(self):\n",
        "        save_graphs(str(self.graph_path), self._graph)\n",
        "\n",
        "    def has_cache(self):\n",
        "        return os.path.exists(self.graph_path)\n",
        "\n",
        "    def load(self):\n",
        "        graphs, _ = load_graphs(str(self.graph_path))\n",
        "        self._graph = graphs[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx == 0, \"This dataset has only one graph.\"\n",
        "        if self._transform is None:\n",
        "            return self._graph\n",
        "        else:\n",
        "            return self._transform(self._graph)\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return 8"
      ],
      "metadata": {
        "id": "WlMm54JRo8EX"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = BAShapeDataset(num_base_nodes=160,\n",
        "                             num_base_edges_per_node=1,\n",
        "                             num_motifs=int(160/5),\n",
        "                             perturb_ratio=0.00,\n",
        "                             seed=None,\n",
        "                             raw_dir=None,\n",
        "                             force_reload=True,\n",
        "                             verbose=True,\n",
        "                             transform=None)\n",
        "data = preprocess(dataset)\n",
        "data.x = torch.tensor(np.concatenate([data.x]*8, axis=1))\n",
        "# np.mean(data.y.numpy())\n",
        "import pandas as pd\n",
        "pd.DataFrame(data.y.cpu().numpy()).value_counts(), data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsaCioU_vZPG",
        "outputId": "b3e57257-813d-4454-d45c-9b4fa8426f50"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 3), (0, 1), (1, 4), (1, 2), (2, 5), (3, 6), (3, 4), (4, 7), (4, 5), (5, 8), (6, 7), (7, 8)]\n",
            "Done saving data into cached files.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1    256\n",
              " 0    160\n",
              " 2     32\n",
              " dtype: int64,\n",
              " Data(x=[448, 8], edge_index=[2, 1150], y=[448], train_mask=[448], val_mask=[448], test_mask=[448]))"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = BACommunityDataset(num_base_nodes=160,\n",
        "                             num_base_edges_per_node=4,\n",
        "                             num_motifs=80,\n",
        "                             perturb_ratio=0.00,\n",
        "                             num_inter_edges=1000,\n",
        "                             seed=None,\n",
        "                             raw_dir=None,\n",
        "                             force_reload=True,\n",
        "                             verbose=True,\n",
        "                             transform=None)\n",
        "data = preprocess(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEjkmxc6qAOs",
        "outputId": "2b45052e-7fd7-4a58-f7c7-1f19f34746c5"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 3), (0, 1), (1, 4), (1, 2), (2, 5), (3, 6), (3, 4), (4, 7), (4, 5), (5, 8), (6, 7), (7, 8)]\n",
            "[(0, 3), (0, 1), (1, 4), (1, 2), (2, 5), (3, 6), (3, 4), (4, 7), (4, 5), (5, 8), (6, 7), (7, 8)]\n",
            "Done saving data into cached files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNNModel"
      ],
      "metadata": {
        "id": "-wTyuaSWolSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n",
        "import torch.nn.functional as F\n",
        "class GNNModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                GCNConv(hidden_dim, hidden_dim)\n",
        "                # GATConv(hidden_dim, hidden_dim // num_heads, num_heads)\n",
        "                # GATv2Conv(hidden_dim, hidden_dim // num_heads, num_heads)\n",
        "\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # conv -> activation ->  dropout -> residual\n",
        "            x_in = x\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)"
      ],
      "metadata": {
        "id": "Ice-ZLzhqSfW"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train GCN"
      ],
      "metadata": {
        "id": "_8fpWvjwolZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = GNNModel(num_heads=1, num_layers=4).to(device)\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,  weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.5)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
        "    class_correct = torch.zeros(data.y.max() + 1)\n",
        "    class_total = torch.zeros(data.y.max() + 1)\n",
        "\n",
        "    class_accs = {}\n",
        "    for mask_name, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        mask_pred = pred[mask]\n",
        "        mask_true = data.y[mask]\n",
        "\n",
        "        for i in range(data.y.max() + 1):\n",
        "            class_total[i] += (mask_true == i).sum().item()\n",
        "            class_correct[i] += ((mask_pred == i) & (mask_true == i)).sum().item()\n",
        "        class_accs[mask_name] = list(class_correct / class_total)\n",
        "    return class_accs\n",
        "\n",
        "best_val_acc = [0] * (data.y.max() + 1)\n",
        "test_acc = [0] * (data.y.max() + 1)\n",
        "times = []\n",
        "\n",
        "num_epochs = 40000\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    if (epoch % 200 == 0 or epoch == num_epochs):\n",
        "        print(\"Epoch: \", epoch, \" class accuracies: \", test())\n",
        "\n",
        "    # train_accs, val_accs, tmp_test_acc = test()\n",
        "    # Update the best validation and test accuracy\n",
        "    # for i, (val_acc, test_acc) in enumerate(zip(val_accs, test_accs)):\n",
        "    #     if val_acc > best_val_acc[i]:\n",
        "    #         best_val_acc[i] = val_acc\n",
        "    #         test_acc[i] = test_acc\n",
        "\n",
        "    # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f}')\n",
        "    # for i, (train_acc, val_acc, tmp_test_acc, best_test_acc) in enumerate(zip(train_accs, val_accs, test_accs, test_acc)):\n",
        "    #     print(f'Class {i}: Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, Best Test: {best_test_acc:.4f}')\n",
        "\n",
        "    times.append(time.time() - start)\n",
        "    scheduler.step()\n",
        "\n",
        "# Print the median time per epoch\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "FgN-ZJSFrEw7",
        "outputId": "d2ffd179-09a5-4382-a908-107edd48df72"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  200  class accuracies:  {'train_mask': [tensor 1.000, tensor 0.993, tensor 0.119, tensor 0.991, tensor 0.986, tensor 0.121], 'val_mask': [tensor 0.993, tensor 0.984, tensor 0.110, tensor 0.978, tensor 0.985, tensor 0.118], 'test_mask': [tensor 0.988, tensor 0.980, tensor 0.100, tensor 0.950, tensor 0.986, tensor 0.100]}\n",
            "Epoch:  400  class accuracies:  {'train_mask': [tensor 1.000, tensor 0.996, tensor 0.525, tensor 1.000, tensor 1.000, tensor 0.517], 'val_mask': [tensor 0.985, tensor 0.982, tensor 0.452, tensor 0.985, tensor 0.994, tensor 0.471], 'test_mask': [tensor 0.981, tensor 0.978, tensor 0.425, tensor 0.975, tensor 0.994, tensor 0.400]}\n",
            "Epoch:  600  class accuracies:  {'train_mask': [tensor 1.000, tensor 0.998, tensor 0.864, tensor 1.000, tensor 0.998, tensor 0.948], 'val_mask': [tensor 0.993, tensor 0.982, tensor 0.740, tensor 0.985, tensor 0.991, tensor 0.868], 'test_mask': [tensor 0.994, tensor 0.975, tensor 0.688, tensor 0.975, tensor 0.989, tensor 0.738]}\n",
            "Epoch:  800  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.985, tensor 0.982, tensor 0.849, tensor 0.985, tensor 0.996, tensor 0.897], 'test_mask': [tensor 0.981, tensor 0.978, tensor 0.788, tensor 0.975, tensor 0.989, tensor 0.762]}\n",
            "Epoch:  1000  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.978, tensor 0.849, tensor 0.985, tensor 0.994, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.973, tensor 0.788, tensor 0.975, tensor 0.988, tensor 0.750]}\n",
            "Epoch:  1200  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.982, tensor 0.836, tensor 0.985, tensor 0.994, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.978, tensor 0.775, tensor 0.969, tensor 0.991, tensor 0.750]}\n",
            "Epoch:  1400  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.980, tensor 0.849, tensor 0.985, tensor 0.991, tensor 0.897], 'test_mask': [tensor 0.994, tensor 0.977, tensor 0.788, tensor 0.969, tensor 0.984, tensor 0.762]}\n",
            "Epoch:  1600  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.978, tensor 0.849, tensor 0.985, tensor 0.996, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.975, tensor 0.788, tensor 0.975, tensor 0.992, tensor 0.750]}\n",
            "Epoch:  1800  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.982, tensor 0.836, tensor 0.978, tensor 0.996, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.978, tensor 0.775, tensor 0.962, tensor 0.988, tensor 0.762]}\n",
            "Epoch:  2000  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.982, tensor 0.836, tensor 0.985, tensor 0.996, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.980, tensor 0.775, tensor 0.975, tensor 0.989, tensor 0.762]}\n",
            "Epoch:  2200  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.984, tensor 0.822, tensor 0.985, tensor 0.993, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.980, tensor 0.762, tensor 0.969, tensor 0.989, tensor 0.762]}\n",
            "Epoch:  2400  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.978, tensor 0.982, tensor 0.836, tensor 0.985, tensor 0.994, tensor 0.882], 'test_mask': [tensor 0.975, tensor 0.980, tensor 0.775, tensor 0.975, tensor 0.988, tensor 0.762]}\n",
            "Epoch:  2600  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.985, tensor 0.980, tensor 0.836, tensor 0.978, tensor 0.993, tensor 0.882], 'test_mask': [tensor 0.981, tensor 0.978, tensor 0.775, tensor 0.956, tensor 0.984, tensor 0.762]}\n",
            "Epoch:  2800  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.978, tensor 0.984, tensor 0.836, tensor 0.978, tensor 0.991, tensor 0.882], 'test_mask': [tensor 0.975, tensor 0.978, tensor 0.775, tensor 0.956, tensor 0.986, tensor 0.762]}\n",
            "Epoch:  3000  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.985, tensor 0.836, tensor 0.985, tensor 0.994, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.984, tensor 0.775, tensor 0.975, tensor 0.989, tensor 0.762]}\n",
            "Epoch:  3200  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.984, tensor 0.836, tensor 0.985, tensor 0.994, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.981, tensor 0.775, tensor 0.975, tensor 0.988, tensor 0.762]}\n",
            "Epoch:  3400  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.985, tensor 0.985, tensor 0.822, tensor 0.985, tensor 0.993, tensor 0.868], 'test_mask': [tensor 0.981, tensor 0.983, tensor 0.762, tensor 0.975, tensor 0.991, tensor 0.750]}\n",
            "Epoch:  3600  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.984, tensor 0.836, tensor 0.978, tensor 0.993, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.978, tensor 0.775, tensor 0.962, tensor 0.991, tensor 0.762]}\n",
            "Epoch:  3800  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.978, tensor 0.985, tensor 0.836, tensor 0.985, tensor 0.993, tensor 0.868], 'test_mask': [tensor 0.975, tensor 0.983, tensor 0.775, tensor 0.975, tensor 0.988, tensor 0.750]}\n",
            "Epoch:  4000  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.985, tensor 0.836, tensor 0.978, tensor 0.993, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.983, tensor 0.775, tensor 0.969, tensor 0.988, tensor 0.762]}\n",
            "Epoch:  4200  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.985, tensor 0.836, tensor 0.978, tensor 0.993, tensor 0.868], 'test_mask': [tensor 0.994, tensor 0.983, tensor 0.775, tensor 0.969, tensor 0.991, tensor 0.750]}\n",
            "Epoch:  4400  class accuracies:  {'train_mask': [tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000, tensor 1.000], 'val_mask': [tensor 0.993, tensor 0.985, tensor 0.836, tensor 0.985, tensor 0.993, tensor 0.882], 'test_mask': [tensor 0.994, tensor 0.984, tensor 0.775, tensor 0.981, tensor 0.988, tensor 0.762]}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-202-41c5abaad115>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" class accuracies: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-202-41c5abaad115>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-201-ba919be16f30>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# conv -> activation ->  dropout -> residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_edge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n\u001b[0m\u001b[1;32m    242\u001b[0m                         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                         self.improved, self.add_self_loops, self.flow, x.dtype)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mflow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'source_to_target'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mdeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "DZm3IeXPolkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n",
        "\n",
        "class GNNModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                # GCNConv(hidden_dim, hidden_dim)\n",
        "                GATConv(hidden_dim, hidden_dim // num_heads, num_heads)\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # conv -> activation ->  dropout -> residual\n",
        "            x_in = x\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "class SparseGraphTransformerModel(Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, dense_adj):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = ~dense_adj.bool())\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # boolean mask enforces graph structure\n",
        "            # x = layer(x, src_mask = ~dense_adj.bool())\n",
        "\n",
        "            # MHSA layer\n",
        "            # boolean mask enforces graph structure\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = ~dense_adj.bool(),\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "class DenseGraphTransformerModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        # x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "            (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "            nan=0, posinf=0, neginf=0\n",
        "        )\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # float mask adds learnable additive attention bias\n",
        "            # x = layer(x, src_mask = attn_bias)\n",
        "\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "class DenseGraphTransformerModel_V2(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        # x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        # attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "        #     (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "        #     nan=0, posinf=0, neginf=0\n",
        "        # )\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # float mask adds learnable additive attention bias\n",
        "            # x = layer(x, src_mask = attn_bias)\n",
        "\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                # attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)"
      ],
      "metadata": {
        "id": "AGjECAxyruQ9"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DenseGraphTransformerModel_V2(num_heads=1, num_layers=3).to(device)\n",
        "\n",
        "data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n",
        "# data = T.AddRandomWalkPE(walk_length = 16, attr_name = 'pos_enc')(data)\n",
        "data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "# data.dense_sp_matrix = dense_shortest_path_matrix.float()  # pre-computed in previous cell\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,  weight_decay=1e-4)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.pos_enc, 0)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred, accs = model(data.x, data.pos_enc, 0).argmax(dim=-1), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "times = []\n",
        "for epoch in range(1, 10000):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "          f'Final Test: {test_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "\n",
        "# Notes\n",
        "# - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "# - Node positional encodings are not particularly useful\n",
        "# - Edge distance encodings are very useful\n",
        "# - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNlfEITFK0AV",
        "outputId": "809b097f-aca9-4b58-ff2b-42a6e1e75900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 0609, Loss: 0.3887 Train: 0.8701, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0610, Loss: 0.3932 Train: 0.8628, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0611, Loss: 0.3950 Train: 0.8628, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0612, Loss: 0.3763 Train: 0.8636, Val: 0.7765, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0613, Loss: 0.3635 Train: 0.8580, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0614, Loss: 0.3637 Train: 0.8539, Val: 0.7727, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 0615, Loss: 0.3745 Train: 0.8547, Val: 0.7803, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 0616, Loss: 0.3624 Train: 0.8636, Val: 0.7841, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0617, Loss: 0.3857 Train: 0.8644, Val: 0.7803, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0618, Loss: 0.3924 Train: 0.8677, Val: 0.7727, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0619, Loss: 0.3742 Train: 0.8604, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0620, Loss: 0.3749 Train: 0.8620, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0621, Loss: 0.3764 Train: 0.8612, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0622, Loss: 0.3517 Train: 0.8596, Val: 0.7727, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0623, Loss: 0.3516 Train: 0.8620, Val: 0.7765, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 0624, Loss: 0.3811 Train: 0.8612, Val: 0.7689, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 0625, Loss: 0.3693 Train: 0.8571, Val: 0.7614, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 0626, Loss: 0.3752 Train: 0.8685, Val: 0.7765, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 0627, Loss: 0.3595 Train: 0.8636, Val: 0.7689, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0628, Loss: 0.3797 Train: 0.8677, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0629, Loss: 0.3801 Train: 0.8653, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0630, Loss: 0.3663 Train: 0.8571, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0631, Loss: 0.3573 Train: 0.8644, Val: 0.7841, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0632, Loss: 0.3526 Train: 0.8636, Val: 0.7841, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0633, Loss: 0.3805 Train: 0.8661, Val: 0.7955, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0634, Loss: 0.3671 Train: 0.8685, Val: 0.7727, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0635, Loss: 0.3569 Train: 0.8628, Val: 0.7803, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0636, Loss: 0.3746 Train: 0.8653, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0637, Loss: 0.3657 Train: 0.8653, Val: 0.7765, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0638, Loss: 0.3638 Train: 0.8701, Val: 0.7727, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0639, Loss: 0.3606 Train: 0.8693, Val: 0.7689, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0640, Loss: 0.3667 Train: 0.8596, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0641, Loss: 0.3934 Train: 0.8531, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 0642, Loss: 0.3868 Train: 0.8653, Val: 0.7576, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0643, Loss: 0.3585 Train: 0.8726, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0644, Loss: 0.3565 Train: 0.8734, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0645, Loss: 0.3736 Train: 0.8685, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0646, Loss: 0.3638 Train: 0.8555, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0647, Loss: 0.3770 Train: 0.8604, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0648, Loss: 0.3687 Train: 0.8653, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0649, Loss: 0.3585 Train: 0.8636, Val: 0.7652, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0650, Loss: 0.3629 Train: 0.8677, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0651, Loss: 0.3772 Train: 0.8604, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0652, Loss: 0.3497 Train: 0.8620, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0653, Loss: 0.3663 Train: 0.8693, Val: 0.7765, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0654, Loss: 0.3669 Train: 0.8661, Val: 0.7727, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0655, Loss: 0.3649 Train: 0.8661, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0656, Loss: 0.3573 Train: 0.8612, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0657, Loss: 0.3453 Train: 0.8620, Val: 0.7689, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 0658, Loss: 0.3487 Train: 0.8669, Val: 0.7689, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0659, Loss: 0.3426 Train: 0.8685, Val: 0.7765, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0660, Loss: 0.3595 Train: 0.8709, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0661, Loss: 0.3550 Train: 0.8750, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0662, Loss: 0.3558 Train: 0.8734, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0663, Loss: 0.3405 Train: 0.8734, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0664, Loss: 0.3357 Train: 0.8734, Val: 0.7727, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0665, Loss: 0.3516 Train: 0.8693, Val: 0.7652, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0666, Loss: 0.3507 Train: 0.8685, Val: 0.7689, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 0667, Loss: 0.3405 Train: 0.8718, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0668, Loss: 0.3531 Train: 0.8734, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0669, Loss: 0.3378 Train: 0.8734, Val: 0.7576, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0670, Loss: 0.3526 Train: 0.8718, Val: 0.7652, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0671, Loss: 0.3556 Train: 0.8644, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0672, Loss: 0.3662 Train: 0.8628, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0673, Loss: 0.3446 Train: 0.8644, Val: 0.7765, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0674, Loss: 0.3594 Train: 0.8693, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0675, Loss: 0.3695 Train: 0.8734, Val: 0.7689, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0676, Loss: 0.3564 Train: 0.8612, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0677, Loss: 0.3661 Train: 0.8604, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0678, Loss: 0.3634 Train: 0.8644, Val: 0.7727, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 0679, Loss: 0.3524 Train: 0.8604, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0680, Loss: 0.3432 Train: 0.8596, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0681, Loss: 0.3545 Train: 0.8636, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0682, Loss: 0.3533 Train: 0.8620, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0683, Loss: 0.3592 Train: 0.8718, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0684, Loss: 0.3463 Train: 0.8636, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0685, Loss: 0.3559 Train: 0.8661, Val: 0.7727, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0686, Loss: 0.3418 Train: 0.8701, Val: 0.7689, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0687, Loss: 0.3444 Train: 0.8669, Val: 0.7727, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0688, Loss: 0.3596 Train: 0.8628, Val: 0.7652, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 0689, Loss: 0.3504 Train: 0.8604, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0690, Loss: 0.3620 Train: 0.8661, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0691, Loss: 0.3526 Train: 0.8685, Val: 0.7689, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0692, Loss: 0.3482 Train: 0.8709, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0693, Loss: 0.3319 Train: 0.8653, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0694, Loss: 0.3330 Train: 0.8661, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0695, Loss: 0.3660 Train: 0.8709, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0696, Loss: 0.3381 Train: 0.8693, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0697, Loss: 0.3350 Train: 0.8685, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0698, Loss: 0.3629 Train: 0.8677, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0699, Loss: 0.3448 Train: 0.8669, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0700, Loss: 0.3652 Train: 0.8709, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0701, Loss: 0.3646 Train: 0.8718, Val: 0.7689, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0702, Loss: 0.3399 Train: 0.8718, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0703, Loss: 0.3470 Train: 0.8709, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0704, Loss: 0.3579 Train: 0.8685, Val: 0.7689, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0705, Loss: 0.3520 Train: 0.8693, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0706, Loss: 0.3274 Train: 0.8669, Val: 0.7652, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0707, Loss: 0.3383 Train: 0.8644, Val: 0.7689, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0708, Loss: 0.3634 Train: 0.8685, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0709, Loss: 0.3550 Train: 0.8669, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0710, Loss: 0.3404 Train: 0.8693, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0711, Loss: 0.3391 Train: 0.8653, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0712, Loss: 0.3512 Train: 0.8653, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0713, Loss: 0.3443 Train: 0.8677, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0714, Loss: 0.3527 Train: 0.8693, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0715, Loss: 0.3579 Train: 0.8677, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0716, Loss: 0.3576 Train: 0.8628, Val: 0.7727, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0717, Loss: 0.3508 Train: 0.8604, Val: 0.7614, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 0718, Loss: 0.3527 Train: 0.8531, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 0719, Loss: 0.3528 Train: 0.8612, Val: 0.7652, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0720, Loss: 0.3487 Train: 0.8734, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0721, Loss: 0.3349 Train: 0.8726, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0722, Loss: 0.3572 Train: 0.8734, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0723, Loss: 0.3255 Train: 0.8669, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0724, Loss: 0.3335 Train: 0.8669, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0725, Loss: 0.3461 Train: 0.8669, Val: 0.7576, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0726, Loss: 0.3323 Train: 0.8718, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0727, Loss: 0.3522 Train: 0.8766, Val: 0.7689, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0728, Loss: 0.3415 Train: 0.8677, Val: 0.7614, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 0729, Loss: 0.3641 Train: 0.8758, Val: 0.7689, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0730, Loss: 0.3561 Train: 0.8791, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0731, Loss: 0.3304 Train: 0.8782, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0732, Loss: 0.3394 Train: 0.8750, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0733, Loss: 0.3418 Train: 0.8709, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0734, Loss: 0.3603 Train: 0.8709, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0735, Loss: 0.3369 Train: 0.8807, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0736, Loss: 0.3464 Train: 0.8791, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0737, Loss: 0.3430 Train: 0.8807, Val: 0.7765, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0738, Loss: 0.3432 Train: 0.8726, Val: 0.7765, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0739, Loss: 0.3473 Train: 0.8742, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0740, Loss: 0.3293 Train: 0.8774, Val: 0.7614, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 0741, Loss: 0.3450 Train: 0.8742, Val: 0.7614, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 0742, Loss: 0.3272 Train: 0.8734, Val: 0.7727, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0743, Loss: 0.3396 Train: 0.8718, Val: 0.7727, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0744, Loss: 0.3406 Train: 0.8685, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0745, Loss: 0.3401 Train: 0.8693, Val: 0.7727, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0746, Loss: 0.3340 Train: 0.8718, Val: 0.7727, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0747, Loss: 0.3456 Train: 0.8791, Val: 0.7765, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0748, Loss: 0.3194 Train: 0.8774, Val: 0.7689, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0749, Loss: 0.3520 Train: 0.8758, Val: 0.7727, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0750, Loss: 0.3263 Train: 0.8807, Val: 0.7652, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 0751, Loss: 0.3296 Train: 0.8758, Val: 0.7652, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0752, Loss: 0.3481 Train: 0.8701, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0753, Loss: 0.3356 Train: 0.8734, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0754, Loss: 0.3452 Train: 0.8734, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0755, Loss: 0.3360 Train: 0.8766, Val: 0.7689, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0756, Loss: 0.3459 Train: 0.8823, Val: 0.7727, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 0757, Loss: 0.3114 Train: 0.8799, Val: 0.7727, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0758, Loss: 0.3262 Train: 0.8750, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0759, Loss: 0.3307 Train: 0.8791, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0760, Loss: 0.3144 Train: 0.8782, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0761, Loss: 0.3370 Train: 0.8774, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0762, Loss: 0.3244 Train: 0.8799, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0763, Loss: 0.3304 Train: 0.8726, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0764, Loss: 0.3113 Train: 0.8685, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0765, Loss: 0.3296 Train: 0.8669, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0766, Loss: 0.3678 Train: 0.8604, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0767, Loss: 0.3383 Train: 0.8677, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0768, Loss: 0.3247 Train: 0.8742, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0769, Loss: 0.3331 Train: 0.8766, Val: 0.7689, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0770, Loss: 0.3318 Train: 0.8726, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0771, Loss: 0.3085 Train: 0.8734, Val: 0.7689, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0772, Loss: 0.3176 Train: 0.8734, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0773, Loss: 0.3215 Train: 0.8718, Val: 0.7614, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0774, Loss: 0.3344 Train: 0.8750, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0775, Loss: 0.3223 Train: 0.8750, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0776, Loss: 0.3230 Train: 0.8726, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0777, Loss: 0.3316 Train: 0.8750, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0778, Loss: 0.3441 Train: 0.8823, Val: 0.7500, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 0779, Loss: 0.3233 Train: 0.8742, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0780, Loss: 0.3285 Train: 0.8734, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0781, Loss: 0.3435 Train: 0.8815, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0782, Loss: 0.3291 Train: 0.8831, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0783, Loss: 0.3371 Train: 0.8782, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0784, Loss: 0.3363 Train: 0.8709, Val: 0.7576, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 0785, Loss: 0.3364 Train: 0.8636, Val: 0.7424, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 0786, Loss: 0.3149 Train: 0.8563, Val: 0.7386, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0787, Loss: 0.3507 Train: 0.8701, Val: 0.7386, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 0788, Loss: 0.3420 Train: 0.8807, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0789, Loss: 0.3226 Train: 0.8750, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0790, Loss: 0.3383 Train: 0.8766, Val: 0.7689, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0791, Loss: 0.3182 Train: 0.8726, Val: 0.7689, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0792, Loss: 0.3438 Train: 0.8685, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0793, Loss: 0.3448 Train: 0.8807, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0794, Loss: 0.3341 Train: 0.8815, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0795, Loss: 0.3139 Train: 0.8766, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0796, Loss: 0.3344 Train: 0.8782, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0797, Loss: 0.3274 Train: 0.8847, Val: 0.7765, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0798, Loss: 0.3236 Train: 0.8799, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0799, Loss: 0.3300 Train: 0.8693, Val: 0.7689, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0800, Loss: 0.3127 Train: 0.8628, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0801, Loss: 0.3169 Train: 0.8596, Val: 0.7727, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0802, Loss: 0.3261 Train: 0.8661, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0803, Loss: 0.3368 Train: 0.8661, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0804, Loss: 0.3142 Train: 0.8791, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0805, Loss: 0.3121 Train: 0.8782, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0806, Loss: 0.3326 Train: 0.8831, Val: 0.7689, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0807, Loss: 0.3225 Train: 0.8782, Val: 0.7803, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0808, Loss: 0.3339 Train: 0.8872, Val: 0.7727, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0809, Loss: 0.3268 Train: 0.8766, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0810, Loss: 0.3272 Train: 0.8693, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0811, Loss: 0.3312 Train: 0.8693, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0812, Loss: 0.3355 Train: 0.8807, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0813, Loss: 0.3526 Train: 0.8782, Val: 0.7727, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 0814, Loss: 0.3180 Train: 0.8807, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0815, Loss: 0.3405 Train: 0.8750, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0816, Loss: 0.3526 Train: 0.8718, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0817, Loss: 0.3216 Train: 0.8669, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0818, Loss: 0.3482 Train: 0.8636, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0819, Loss: 0.3436 Train: 0.8644, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0820, Loss: 0.3340 Train: 0.8693, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0821, Loss: 0.3235 Train: 0.8693, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0822, Loss: 0.3296 Train: 0.8677, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0823, Loss: 0.3260 Train: 0.8701, Val: 0.7727, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0824, Loss: 0.3180 Train: 0.8677, Val: 0.7614, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 0825, Loss: 0.3311 Train: 0.8685, Val: 0.7500, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 0826, Loss: 0.3509 Train: 0.8734, Val: 0.7424, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0827, Loss: 0.3306 Train: 0.8750, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0828, Loss: 0.3266 Train: 0.8750, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0829, Loss: 0.3277 Train: 0.8815, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0830, Loss: 0.3247 Train: 0.8823, Val: 0.7803, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0831, Loss: 0.3286 Train: 0.8766, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0832, Loss: 0.3122 Train: 0.8726, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0833, Loss: 0.3247 Train: 0.8677, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0834, Loss: 0.3110 Train: 0.8685, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0835, Loss: 0.3236 Train: 0.8718, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0836, Loss: 0.3169 Train: 0.8726, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0837, Loss: 0.3206 Train: 0.8758, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0838, Loss: 0.3067 Train: 0.8766, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0839, Loss: 0.3217 Train: 0.8815, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0840, Loss: 0.3548 Train: 0.8807, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0841, Loss: 0.3229 Train: 0.8774, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0842, Loss: 0.3444 Train: 0.8718, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0843, Loss: 0.3302 Train: 0.8677, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0844, Loss: 0.3374 Train: 0.8807, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0845, Loss: 0.2982 Train: 0.8750, Val: 0.7727, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0846, Loss: 0.3288 Train: 0.8734, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0847, Loss: 0.3308 Train: 0.8791, Val: 0.7727, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0848, Loss: 0.3185 Train: 0.8831, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0849, Loss: 0.3076 Train: 0.8815, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0850, Loss: 0.3326 Train: 0.8847, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0851, Loss: 0.3138 Train: 0.8815, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0852, Loss: 0.3112 Train: 0.8856, Val: 0.7614, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 0853, Loss: 0.3230 Train: 0.8864, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0854, Loss: 0.3160 Train: 0.8831, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0855, Loss: 0.3122 Train: 0.8847, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0856, Loss: 0.3199 Train: 0.8774, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 0857, Loss: 0.3230 Train: 0.8709, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0858, Loss: 0.3257 Train: 0.8734, Val: 0.7727, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0859, Loss: 0.3030 Train: 0.8782, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0860, Loss: 0.3165 Train: 0.8782, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0861, Loss: 0.3196 Train: 0.8766, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0862, Loss: 0.3258 Train: 0.8799, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0863, Loss: 0.3132 Train: 0.8815, Val: 0.7652, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0864, Loss: 0.3018 Train: 0.8831, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0865, Loss: 0.3191 Train: 0.8864, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0866, Loss: 0.3039 Train: 0.8872, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0867, Loss: 0.3021 Train: 0.8864, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0868, Loss: 0.3043 Train: 0.8864, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0869, Loss: 0.3191 Train: 0.8839, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0870, Loss: 0.3113 Train: 0.8856, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0871, Loss: 0.3229 Train: 0.8839, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0872, Loss: 0.3189 Train: 0.8807, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0873, Loss: 0.3091 Train: 0.8774, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0874, Loss: 0.3429 Train: 0.8782, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0875, Loss: 0.3240 Train: 0.8880, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0876, Loss: 0.3202 Train: 0.8799, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0877, Loss: 0.3211 Train: 0.8718, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0878, Loss: 0.3139 Train: 0.8726, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0879, Loss: 0.3071 Train: 0.8677, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0880, Loss: 0.3264 Train: 0.8661, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0881, Loss: 0.3365 Train: 0.8750, Val: 0.7576, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0882, Loss: 0.3148 Train: 0.8726, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0883, Loss: 0.3126 Train: 0.8750, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0884, Loss: 0.3087 Train: 0.8726, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0885, Loss: 0.3409 Train: 0.8799, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0886, Loss: 0.3137 Train: 0.8791, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0887, Loss: 0.3109 Train: 0.8782, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0888, Loss: 0.3200 Train: 0.8750, Val: 0.7652, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0889, Loss: 0.3042 Train: 0.8742, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0890, Loss: 0.3090 Train: 0.8766, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0891, Loss: 0.3125 Train: 0.8823, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0892, Loss: 0.3113 Train: 0.8758, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0893, Loss: 0.3249 Train: 0.8831, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0894, Loss: 0.3148 Train: 0.8904, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0895, Loss: 0.2978 Train: 0.8864, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0896, Loss: 0.3136 Train: 0.8872, Val: 0.7727, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0897, Loss: 0.3058 Train: 0.8815, Val: 0.7652, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0898, Loss: 0.2942 Train: 0.8807, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0899, Loss: 0.3067 Train: 0.8774, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0900, Loss: 0.3016 Train: 0.8709, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0901, Loss: 0.3109 Train: 0.8823, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0902, Loss: 0.3022 Train: 0.8774, Val: 0.7500, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 0903, Loss: 0.3191 Train: 0.8718, Val: 0.7462, Test: 0.7462, Final Test: 0.8182\n",
            "Epoch: 0904, Loss: 0.3345 Train: 0.8823, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0905, Loss: 0.3141 Train: 0.8750, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0906, Loss: 0.3089 Train: 0.8734, Val: 0.7727, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0907, Loss: 0.3229 Train: 0.8839, Val: 0.7727, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0908, Loss: 0.2907 Train: 0.8791, Val: 0.7689, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0909, Loss: 0.3055 Train: 0.8693, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0910, Loss: 0.3303 Train: 0.8750, Val: 0.7765, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0911, Loss: 0.3105 Train: 0.8758, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0912, Loss: 0.3007 Train: 0.8677, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0913, Loss: 0.3135 Train: 0.8742, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0914, Loss: 0.3154 Train: 0.8718, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0915, Loss: 0.3272 Train: 0.8596, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0916, Loss: 0.3187 Train: 0.8677, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0917, Loss: 0.3133 Train: 0.8677, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0918, Loss: 0.3313 Train: 0.8701, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0919, Loss: 0.3348 Train: 0.8709, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 0920, Loss: 0.3213 Train: 0.8766, Val: 0.7727, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0921, Loss: 0.3085 Train: 0.8807, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0922, Loss: 0.3057 Train: 0.8750, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0923, Loss: 0.3235 Train: 0.8782, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0924, Loss: 0.3094 Train: 0.8807, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0925, Loss: 0.3002 Train: 0.8791, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0926, Loss: 0.3068 Train: 0.8750, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0927, Loss: 0.2943 Train: 0.8758, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0928, Loss: 0.3065 Train: 0.8742, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0929, Loss: 0.3209 Train: 0.8799, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0930, Loss: 0.3208 Train: 0.8774, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0931, Loss: 0.3097 Train: 0.8669, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0932, Loss: 0.3111 Train: 0.8750, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0933, Loss: 0.3088 Train: 0.8799, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0934, Loss: 0.2952 Train: 0.8791, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0935, Loss: 0.2957 Train: 0.8774, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0936, Loss: 0.3139 Train: 0.8847, Val: 0.7424, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 0937, Loss: 0.2909 Train: 0.8831, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0938, Loss: 0.3067 Train: 0.8839, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0939, Loss: 0.2981 Train: 0.8815, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0940, Loss: 0.2928 Train: 0.8782, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0941, Loss: 0.3001 Train: 0.8823, Val: 0.7538, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0942, Loss: 0.3038 Train: 0.8815, Val: 0.7462, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 0943, Loss: 0.2864 Train: 0.8831, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0944, Loss: 0.2843 Train: 0.8847, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0945, Loss: 0.3017 Train: 0.8815, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0946, Loss: 0.2943 Train: 0.8750, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0947, Loss: 0.2863 Train: 0.8758, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0948, Loss: 0.2917 Train: 0.8799, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0949, Loss: 0.3083 Train: 0.8872, Val: 0.7727, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0950, Loss: 0.2925 Train: 0.8888, Val: 0.7538, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0951, Loss: 0.2772 Train: 0.8904, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0952, Loss: 0.2965 Train: 0.8920, Val: 0.7538, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0953, Loss: 0.2773 Train: 0.8929, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0954, Loss: 0.2945 Train: 0.8766, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0955, Loss: 0.2876 Train: 0.8766, Val: 0.7462, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0956, Loss: 0.3147 Train: 0.8815, Val: 0.7727, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0957, Loss: 0.2797 Train: 0.8856, Val: 0.7614, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 0958, Loss: 0.3159 Train: 0.8912, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0959, Loss: 0.3203 Train: 0.8904, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0960, Loss: 0.2871 Train: 0.8856, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0961, Loss: 0.3047 Train: 0.8847, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0962, Loss: 0.3050 Train: 0.8856, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0963, Loss: 0.2942 Train: 0.8815, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0964, Loss: 0.2992 Train: 0.8742, Val: 0.7462, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0965, Loss: 0.2862 Train: 0.8750, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0966, Loss: 0.2877 Train: 0.8823, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0967, Loss: 0.2848 Train: 0.8815, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0968, Loss: 0.2758 Train: 0.8864, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0969, Loss: 0.2740 Train: 0.8929, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0970, Loss: 0.2903 Train: 0.8839, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0971, Loss: 0.2945 Train: 0.8823, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0972, Loss: 0.3088 Train: 0.8847, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0973, Loss: 0.3055 Train: 0.8912, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0974, Loss: 0.2939 Train: 0.8880, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0975, Loss: 0.3196 Train: 0.8839, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0976, Loss: 0.3036 Train: 0.8839, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0977, Loss: 0.2974 Train: 0.8888, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0978, Loss: 0.2826 Train: 0.8856, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0979, Loss: 0.3065 Train: 0.8799, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0980, Loss: 0.3045 Train: 0.8799, Val: 0.7424, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0981, Loss: 0.2826 Train: 0.8896, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0982, Loss: 0.2919 Train: 0.8872, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 0983, Loss: 0.2926 Train: 0.8864, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 0984, Loss: 0.3231 Train: 0.8872, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0985, Loss: 0.2918 Train: 0.8839, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0986, Loss: 0.2802 Train: 0.8774, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0987, Loss: 0.3036 Train: 0.8856, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 0988, Loss: 0.2844 Train: 0.8823, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0989, Loss: 0.2886 Train: 0.8880, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 0990, Loss: 0.2832 Train: 0.8880, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 0991, Loss: 0.3119 Train: 0.8872, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0992, Loss: 0.2844 Train: 0.8839, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 0993, Loss: 0.2825 Train: 0.8791, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 0994, Loss: 0.2919 Train: 0.8929, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 0995, Loss: 0.2846 Train: 0.8937, Val: 0.7538, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 0996, Loss: 0.2691 Train: 0.8945, Val: 0.7576, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 0997, Loss: 0.3083 Train: 0.8904, Val: 0.7538, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0998, Loss: 0.3058 Train: 0.8904, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 0999, Loss: 0.2902 Train: 0.8888, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1000, Loss: 0.3026 Train: 0.8945, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1001, Loss: 0.2729 Train: 0.8888, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1002, Loss: 0.2824 Train: 0.8872, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1003, Loss: 0.2931 Train: 0.8782, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1004, Loss: 0.2919 Train: 0.8823, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1005, Loss: 0.2987 Train: 0.8750, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1006, Loss: 0.2818 Train: 0.8742, Val: 0.7386, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1007, Loss: 0.3016 Train: 0.8847, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1008, Loss: 0.2908 Train: 0.8807, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1009, Loss: 0.3048 Train: 0.8880, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1010, Loss: 0.2958 Train: 0.8815, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1011, Loss: 0.3040 Train: 0.8880, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1012, Loss: 0.3001 Train: 0.8856, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1013, Loss: 0.2931 Train: 0.8782, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1014, Loss: 0.3018 Train: 0.8864, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1015, Loss: 0.2998 Train: 0.8937, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1016, Loss: 0.2779 Train: 0.8823, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1017, Loss: 0.3092 Train: 0.8831, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1018, Loss: 0.3005 Train: 0.8888, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1019, Loss: 0.2966 Train: 0.8799, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1020, Loss: 0.2991 Train: 0.8782, Val: 0.7576, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1021, Loss: 0.3068 Train: 0.8831, Val: 0.7652, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1022, Loss: 0.2783 Train: 0.8831, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1023, Loss: 0.3021 Train: 0.8856, Val: 0.7803, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1024, Loss: 0.2789 Train: 0.8937, Val: 0.7765, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1025, Loss: 0.2957 Train: 0.8888, Val: 0.7652, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1026, Loss: 0.2671 Train: 0.8856, Val: 0.7576, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1027, Loss: 0.2929 Train: 0.8904, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1028, Loss: 0.2755 Train: 0.8839, Val: 0.7273, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1029, Loss: 0.2968 Train: 0.8847, Val: 0.7273, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1030, Loss: 0.3103 Train: 0.8937, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1031, Loss: 0.2932 Train: 0.8953, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1032, Loss: 0.2945 Train: 0.8904, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1033, Loss: 0.2869 Train: 0.8839, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1034, Loss: 0.2850 Train: 0.8815, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1035, Loss: 0.2823 Train: 0.8872, Val: 0.7424, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1036, Loss: 0.3110 Train: 0.8994, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1037, Loss: 0.2684 Train: 0.8953, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1038, Loss: 0.2965 Train: 0.8937, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1039, Loss: 0.2805 Train: 0.8961, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1040, Loss: 0.2850 Train: 0.8872, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1041, Loss: 0.2833 Train: 0.8856, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1042, Loss: 0.2867 Train: 0.8880, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1043, Loss: 0.3005 Train: 0.8920, Val: 0.7538, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1044, Loss: 0.2889 Train: 0.8961, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1045, Loss: 0.2752 Train: 0.8953, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1046, Loss: 0.2788 Train: 0.8904, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1047, Loss: 0.2753 Train: 0.8929, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1048, Loss: 0.2870 Train: 0.8896, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1049, Loss: 0.2917 Train: 0.8888, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1050, Loss: 0.2850 Train: 0.8888, Val: 0.7727, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1051, Loss: 0.2683 Train: 0.8912, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1052, Loss: 0.2750 Train: 0.8807, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1053, Loss: 0.2695 Train: 0.8799, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1054, Loss: 0.2946 Train: 0.8815, Val: 0.7652, Test: 0.7462, Final Test: 0.8182\n",
            "Epoch: 1055, Loss: 0.2724 Train: 0.8904, Val: 0.7614, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1056, Loss: 0.2719 Train: 0.9010, Val: 0.7652, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1057, Loss: 0.2830 Train: 0.8977, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1058, Loss: 0.2850 Train: 0.8961, Val: 0.7462, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1059, Loss: 0.2734 Train: 0.8953, Val: 0.7386, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1060, Loss: 0.2786 Train: 0.8847, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1061, Loss: 0.2846 Train: 0.8831, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1062, Loss: 0.2902 Train: 0.8864, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1063, Loss: 0.2553 Train: 0.8872, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1064, Loss: 0.2710 Train: 0.8872, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1065, Loss: 0.2806 Train: 0.8961, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1066, Loss: 0.2931 Train: 0.8977, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1067, Loss: 0.2684 Train: 0.8953, Val: 0.7576, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1068, Loss: 0.2668 Train: 0.8929, Val: 0.7500, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1069, Loss: 0.2703 Train: 0.8961, Val: 0.7538, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1070, Loss: 0.2939 Train: 0.8945, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1071, Loss: 0.2626 Train: 0.8969, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1072, Loss: 0.2834 Train: 0.9010, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1073, Loss: 0.2747 Train: 0.8985, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1074, Loss: 0.2894 Train: 0.9002, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1075, Loss: 0.2717 Train: 0.8872, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1076, Loss: 0.2643 Train: 0.8831, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1077, Loss: 0.2800 Train: 0.8904, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1078, Loss: 0.2762 Train: 0.8985, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1079, Loss: 0.2765 Train: 0.9002, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1080, Loss: 0.2703 Train: 0.9018, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1081, Loss: 0.2777 Train: 0.8994, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1082, Loss: 0.2776 Train: 0.8920, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1083, Loss: 0.2843 Train: 0.8888, Val: 0.7197, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1084, Loss: 0.2943 Train: 0.8904, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1085, Loss: 0.2810 Train: 0.8912, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1086, Loss: 0.2565 Train: 0.8888, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1087, Loss: 0.3124 Train: 0.8920, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1088, Loss: 0.2626 Train: 0.8969, Val: 0.7652, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1089, Loss: 0.2889 Train: 0.8985, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1090, Loss: 0.2766 Train: 0.9002, Val: 0.7500, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1091, Loss: 0.2776 Train: 0.8977, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1092, Loss: 0.2624 Train: 0.8985, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1093, Loss: 0.2853 Train: 0.8985, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1094, Loss: 0.2939 Train: 0.8920, Val: 0.7538, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1095, Loss: 0.2925 Train: 0.8831, Val: 0.7576, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1096, Loss: 0.2969 Train: 0.8912, Val: 0.7538, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1097, Loss: 0.2752 Train: 0.8953, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1098, Loss: 0.3084 Train: 0.8945, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1099, Loss: 0.3078 Train: 0.8912, Val: 0.7386, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1100, Loss: 0.2892 Train: 0.8912, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1101, Loss: 0.2686 Train: 0.8945, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1102, Loss: 0.2759 Train: 0.8888, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1103, Loss: 0.2873 Train: 0.8937, Val: 0.7652, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1104, Loss: 0.2707 Train: 0.8920, Val: 0.7538, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1105, Loss: 0.2641 Train: 0.8912, Val: 0.7424, Test: 0.7462, Final Test: 0.8182\n",
            "Epoch: 1106, Loss: 0.2777 Train: 0.8945, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1107, Loss: 0.2900 Train: 0.8904, Val: 0.7348, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1108, Loss: 0.2902 Train: 0.8823, Val: 0.7424, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1109, Loss: 0.2958 Train: 0.8807, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1110, Loss: 0.3110 Train: 0.8799, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1111, Loss: 0.2888 Train: 0.8888, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1112, Loss: 0.2906 Train: 0.8839, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1113, Loss: 0.2840 Train: 0.8856, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1114, Loss: 0.3049 Train: 0.8872, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1115, Loss: 0.2819 Train: 0.8904, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1116, Loss: 0.2786 Train: 0.8985, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1117, Loss: 0.2749 Train: 0.8961, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1118, Loss: 0.2687 Train: 0.8977, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1119, Loss: 0.2669 Train: 0.8977, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1120, Loss: 0.2787 Train: 0.8969, Val: 0.7424, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1121, Loss: 0.2942 Train: 0.8953, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1122, Loss: 0.2641 Train: 0.8985, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1123, Loss: 0.2771 Train: 0.9010, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1124, Loss: 0.2811 Train: 0.9058, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1125, Loss: 0.2810 Train: 0.9050, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1126, Loss: 0.2532 Train: 0.9058, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1127, Loss: 0.2693 Train: 0.9026, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1128, Loss: 0.2727 Train: 0.8920, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1129, Loss: 0.2644 Train: 0.8864, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1130, Loss: 0.2695 Train: 0.8904, Val: 0.7424, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1131, Loss: 0.2652 Train: 0.8912, Val: 0.7462, Test: 0.7462, Final Test: 0.8182\n",
            "Epoch: 1132, Loss: 0.2742 Train: 0.8985, Val: 0.7462, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1133, Loss: 0.2678 Train: 0.9042, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1134, Loss: 0.2666 Train: 0.9026, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1135, Loss: 0.2666 Train: 0.9034, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1136, Loss: 0.2762 Train: 0.8977, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1137, Loss: 0.2822 Train: 0.8994, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1138, Loss: 0.2591 Train: 0.9002, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1139, Loss: 0.2482 Train: 0.9091, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1140, Loss: 0.2578 Train: 0.9058, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1141, Loss: 0.2632 Train: 0.9067, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1142, Loss: 0.2915 Train: 0.9026, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1143, Loss: 0.2576 Train: 0.8977, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1144, Loss: 0.2652 Train: 0.8961, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1145, Loss: 0.2717 Train: 0.8929, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1146, Loss: 0.2580 Train: 0.8888, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1147, Loss: 0.2650 Train: 0.9002, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1148, Loss: 0.2685 Train: 0.9034, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1149, Loss: 0.2770 Train: 0.9058, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1150, Loss: 0.2814 Train: 0.9010, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1151, Loss: 0.2705 Train: 0.8969, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1152, Loss: 0.2677 Train: 0.8994, Val: 0.7197, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1153, Loss: 0.2757 Train: 0.9050, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1154, Loss: 0.2716 Train: 0.9050, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1155, Loss: 0.2867 Train: 0.9075, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1156, Loss: 0.2659 Train: 0.8953, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1157, Loss: 0.2798 Train: 0.8872, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1158, Loss: 0.2399 Train: 0.8782, Val: 0.7311, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1159, Loss: 0.2849 Train: 0.8782, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1160, Loss: 0.2829 Train: 0.8831, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1161, Loss: 0.2586 Train: 0.8823, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1162, Loss: 0.2680 Train: 0.8912, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1163, Loss: 0.2735 Train: 0.8969, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1164, Loss: 0.2551 Train: 0.8969, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1165, Loss: 0.2656 Train: 0.8929, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1166, Loss: 0.2745 Train: 0.8896, Val: 0.7311, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1167, Loss: 0.2709 Train: 0.8937, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1168, Loss: 0.2561 Train: 0.8920, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1169, Loss: 0.2857 Train: 0.9018, Val: 0.7500, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1170, Loss: 0.2742 Train: 0.9042, Val: 0.7500, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1171, Loss: 0.2654 Train: 0.9075, Val: 0.7424, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1172, Loss: 0.2538 Train: 0.9002, Val: 0.7424, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1173, Loss: 0.2662 Train: 0.9034, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1174, Loss: 0.2477 Train: 0.9010, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1175, Loss: 0.2720 Train: 0.9002, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1176, Loss: 0.2738 Train: 0.9026, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1177, Loss: 0.2640 Train: 0.9018, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1178, Loss: 0.2702 Train: 0.8969, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1179, Loss: 0.2587 Train: 0.8953, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1180, Loss: 0.2577 Train: 0.8977, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1181, Loss: 0.2694 Train: 0.8937, Val: 0.7197, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1182, Loss: 0.2762 Train: 0.8929, Val: 0.7197, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1183, Loss: 0.2553 Train: 0.9018, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1184, Loss: 0.2461 Train: 0.9050, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1185, Loss: 0.2622 Train: 0.9034, Val: 0.7424, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1186, Loss: 0.2693 Train: 0.9050, Val: 0.7500, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1187, Loss: 0.2806 Train: 0.9099, Val: 0.7424, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1188, Loss: 0.2506 Train: 0.9058, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1189, Loss: 0.2506 Train: 0.9010, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1190, Loss: 0.2571 Train: 0.9075, Val: 0.7424, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1191, Loss: 0.2614 Train: 0.9107, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1192, Loss: 0.2476 Train: 0.9018, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1193, Loss: 0.2513 Train: 0.9002, Val: 0.7576, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1194, Loss: 0.2606 Train: 0.8961, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1195, Loss: 0.2649 Train: 0.8945, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1196, Loss: 0.2553 Train: 0.8969, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1197, Loss: 0.2577 Train: 0.9034, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1198, Loss: 0.2651 Train: 0.9042, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1199, Loss: 0.2500 Train: 0.9050, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1200, Loss: 0.2494 Train: 0.8985, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1201, Loss: 0.2400 Train: 0.8929, Val: 0.7462, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1202, Loss: 0.2470 Train: 0.8953, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1203, Loss: 0.2462 Train: 0.8985, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1204, Loss: 0.2470 Train: 0.9067, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1205, Loss: 0.2657 Train: 0.9083, Val: 0.7424, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1206, Loss: 0.2363 Train: 0.9050, Val: 0.7386, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1207, Loss: 0.2513 Train: 0.9050, Val: 0.7159, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1208, Loss: 0.2353 Train: 0.9018, Val: 0.7235, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1209, Loss: 0.2529 Train: 0.9058, Val: 0.7462, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1210, Loss: 0.2386 Train: 0.9034, Val: 0.7462, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1211, Loss: 0.2642 Train: 0.8994, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1212, Loss: 0.2535 Train: 0.8961, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1213, Loss: 0.2605 Train: 0.8961, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1214, Loss: 0.2651 Train: 0.9002, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1215, Loss: 0.2637 Train: 0.9026, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1216, Loss: 0.2724 Train: 0.9050, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1217, Loss: 0.2674 Train: 0.9042, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1218, Loss: 0.2758 Train: 0.9026, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1219, Loss: 0.2660 Train: 0.8961, Val: 0.7424, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1220, Loss: 0.2599 Train: 0.8945, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1221, Loss: 0.2611 Train: 0.8896, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1222, Loss: 0.2842 Train: 0.8977, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1223, Loss: 0.2779 Train: 0.9010, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1224, Loss: 0.2699 Train: 0.9050, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1225, Loss: 0.2449 Train: 0.9107, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1226, Loss: 0.2353 Train: 0.9050, Val: 0.7311, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1227, Loss: 0.2372 Train: 0.9018, Val: 0.7121, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1228, Loss: 0.2430 Train: 0.8994, Val: 0.6970, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1229, Loss: 0.2545 Train: 0.8904, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1230, Loss: 0.2639 Train: 0.9018, Val: 0.7311, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1231, Loss: 0.2797 Train: 0.9026, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1232, Loss: 0.2669 Train: 0.9026, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1233, Loss: 0.2507 Train: 0.9010, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1234, Loss: 0.2654 Train: 0.9002, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1235, Loss: 0.2717 Train: 0.8969, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1236, Loss: 0.2634 Train: 0.9018, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1237, Loss: 0.2703 Train: 0.9018, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1238, Loss: 0.2449 Train: 0.9026, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1239, Loss: 0.2394 Train: 0.9026, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1240, Loss: 0.2523 Train: 0.9034, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1241, Loss: 0.3037 Train: 0.8977, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1242, Loss: 0.2434 Train: 0.8945, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1243, Loss: 0.2401 Train: 0.8945, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1244, Loss: 0.2611 Train: 0.8920, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1245, Loss: 0.2492 Train: 0.8896, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1246, Loss: 0.2616 Train: 0.8937, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1247, Loss: 0.2700 Train: 0.8977, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1248, Loss: 0.2596 Train: 0.8969, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1249, Loss: 0.2467 Train: 0.9010, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1250, Loss: 0.2600 Train: 0.9050, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1251, Loss: 0.2567 Train: 0.9075, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1252, Loss: 0.2492 Train: 0.9010, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1253, Loss: 0.2796 Train: 0.9002, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1254, Loss: 0.2264 Train: 0.8912, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1255, Loss: 0.2514 Train: 0.8920, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1256, Loss: 0.2491 Train: 0.9002, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1257, Loss: 0.2412 Train: 0.9026, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1258, Loss: 0.2433 Train: 0.9018, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1259, Loss: 0.2396 Train: 0.8994, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1260, Loss: 0.2742 Train: 0.9002, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1261, Loss: 0.2395 Train: 0.9010, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1262, Loss: 0.2651 Train: 0.9050, Val: 0.7348, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1263, Loss: 0.2455 Train: 0.9083, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1264, Loss: 0.2390 Train: 0.9115, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1265, Loss: 0.2692 Train: 0.8969, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1266, Loss: 0.2614 Train: 0.8872, Val: 0.7273, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1267, Loss: 0.2791 Train: 0.8904, Val: 0.7273, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1268, Loss: 0.2734 Train: 0.8977, Val: 0.7197, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1269, Loss: 0.2589 Train: 0.9018, Val: 0.7311, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1270, Loss: 0.2647 Train: 0.9018, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1271, Loss: 0.2658 Train: 0.8937, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1272, Loss: 0.2817 Train: 0.8880, Val: 0.7235, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1273, Loss: 0.2657 Train: 0.8912, Val: 0.7235, Test: 0.7462, Final Test: 0.8182\n",
            "Epoch: 1274, Loss: 0.2850 Train: 0.8937, Val: 0.7311, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1275, Loss: 0.2733 Train: 0.8929, Val: 0.7273, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1276, Loss: 0.2496 Train: 0.8977, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1277, Loss: 0.2780 Train: 0.9042, Val: 0.7462, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1278, Loss: 0.3032 Train: 0.9050, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1279, Loss: 0.2698 Train: 0.9010, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1280, Loss: 0.2561 Train: 0.8937, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1281, Loss: 0.2751 Train: 0.8945, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1282, Loss: 0.2557 Train: 0.8985, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1283, Loss: 0.2675 Train: 0.9026, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1284, Loss: 0.2777 Train: 0.8937, Val: 0.7386, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1285, Loss: 0.2630 Train: 0.8912, Val: 0.7197, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1286, Loss: 0.2653 Train: 0.8945, Val: 0.7159, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1287, Loss: 0.2647 Train: 0.8929, Val: 0.7576, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1288, Loss: 0.2578 Train: 0.8945, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1289, Loss: 0.2710 Train: 0.9034, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1290, Loss: 0.2498 Train: 0.9067, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1291, Loss: 0.2556 Train: 0.9002, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1292, Loss: 0.2721 Train: 0.8961, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1293, Loss: 0.2554 Train: 0.9034, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1294, Loss: 0.2330 Train: 0.9034, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1295, Loss: 0.2491 Train: 0.8969, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1296, Loss: 0.2652 Train: 0.8977, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1297, Loss: 0.2344 Train: 0.9026, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1298, Loss: 0.2330 Train: 0.9083, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1299, Loss: 0.2348 Train: 0.9123, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1300, Loss: 0.2557 Train: 0.9148, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1301, Loss: 0.2489 Train: 0.9172, Val: 0.7386, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1302, Loss: 0.2334 Train: 0.9123, Val: 0.7462, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1303, Loss: 0.2402 Train: 0.9140, Val: 0.7576, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1304, Loss: 0.2447 Train: 0.9042, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1305, Loss: 0.2374 Train: 0.8920, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1306, Loss: 0.2564 Train: 0.8969, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1307, Loss: 0.2698 Train: 0.8985, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1308, Loss: 0.2448 Train: 0.8969, Val: 0.7273, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1309, Loss: 0.2812 Train: 0.9026, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1310, Loss: 0.2591 Train: 0.8920, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1311, Loss: 0.2615 Train: 0.8896, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1312, Loss: 0.3040 Train: 0.8961, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1313, Loss: 0.2602 Train: 0.8961, Val: 0.7538, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1314, Loss: 0.2894 Train: 0.8961, Val: 0.7386, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1315, Loss: 0.2757 Train: 0.9018, Val: 0.7424, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1316, Loss: 0.2744 Train: 0.9002, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1317, Loss: 0.2466 Train: 0.8969, Val: 0.7386, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1318, Loss: 0.2551 Train: 0.8904, Val: 0.7348, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1319, Loss: 0.2665 Train: 0.8961, Val: 0.7538, Test: 0.7424, Final Test: 0.8182\n",
            "Epoch: 1320, Loss: 0.2552 Train: 0.9042, Val: 0.7462, Test: 0.7424, Final Test: 0.8182\n",
            "Epoch: 1321, Loss: 0.2512 Train: 0.8985, Val: 0.7386, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1322, Loss: 0.2620 Train: 0.8994, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1323, Loss: 0.2727 Train: 0.8994, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1324, Loss: 0.2469 Train: 0.9002, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1325, Loss: 0.2662 Train: 0.8969, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1326, Loss: 0.2356 Train: 0.9018, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1327, Loss: 0.2448 Train: 0.9083, Val: 0.7424, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1328, Loss: 0.2396 Train: 0.9067, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1329, Loss: 0.2511 Train: 0.8985, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1330, Loss: 0.2386 Train: 0.8953, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1331, Loss: 0.2567 Train: 0.9026, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1332, Loss: 0.2554 Train: 0.9050, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1333, Loss: 0.2522 Train: 0.9091, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1334, Loss: 0.2538 Train: 0.9091, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1335, Loss: 0.2451 Train: 0.9026, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1336, Loss: 0.2591 Train: 0.9002, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1337, Loss: 0.2537 Train: 0.9026, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1338, Loss: 0.2484 Train: 0.9067, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1339, Loss: 0.2740 Train: 0.9067, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1340, Loss: 0.2498 Train: 0.9067, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1341, Loss: 0.2683 Train: 0.9010, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1342, Loss: 0.2465 Train: 0.8985, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1343, Loss: 0.2428 Train: 0.8994, Val: 0.7235, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1344, Loss: 0.2602 Train: 0.8969, Val: 0.7273, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1345, Loss: 0.2651 Train: 0.9042, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1346, Loss: 0.2405 Train: 0.9083, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1347, Loss: 0.2708 Train: 0.9115, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1348, Loss: 0.2564 Train: 0.9107, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1349, Loss: 0.2456 Train: 0.9107, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1350, Loss: 0.2675 Train: 0.9131, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1351, Loss: 0.2408 Train: 0.9091, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1352, Loss: 0.2482 Train: 0.9083, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1353, Loss: 0.2397 Train: 0.9002, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1354, Loss: 0.2606 Train: 0.9010, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1355, Loss: 0.2475 Train: 0.9026, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1356, Loss: 0.2449 Train: 0.9067, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1357, Loss: 0.2438 Train: 0.9050, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1358, Loss: 0.2780 Train: 0.9034, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1359, Loss: 0.2673 Train: 0.9034, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1360, Loss: 0.2433 Train: 0.9067, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1361, Loss: 0.2664 Train: 0.9075, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1362, Loss: 0.2596 Train: 0.9058, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1363, Loss: 0.2494 Train: 0.9018, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1364, Loss: 0.2691 Train: 0.9067, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1365, Loss: 0.2512 Train: 0.9034, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1366, Loss: 0.2666 Train: 0.9034, Val: 0.7538, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1367, Loss: 0.2620 Train: 0.8969, Val: 0.7462, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1368, Loss: 0.2564 Train: 0.9002, Val: 0.7462, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1369, Loss: 0.2583 Train: 0.9050, Val: 0.7500, Test: 0.7424, Final Test: 0.8182\n",
            "Epoch: 1370, Loss: 0.2473 Train: 0.9067, Val: 0.7311, Test: 0.7348, Final Test: 0.8182\n",
            "Epoch: 1371, Loss: 0.2772 Train: 0.9018, Val: 0.7159, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1372, Loss: 0.2458 Train: 0.9010, Val: 0.7121, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1373, Loss: 0.2528 Train: 0.8994, Val: 0.7159, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1374, Loss: 0.2555 Train: 0.9026, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1375, Loss: 0.2623 Train: 0.8985, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1376, Loss: 0.2500 Train: 0.9034, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1377, Loss: 0.2609 Train: 0.9018, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1378, Loss: 0.2374 Train: 0.9042, Val: 0.7614, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1379, Loss: 0.2676 Train: 0.9026, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1380, Loss: 0.2599 Train: 0.8985, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1381, Loss: 0.2531 Train: 0.9026, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1382, Loss: 0.2413 Train: 0.9034, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1383, Loss: 0.2546 Train: 0.9083, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1384, Loss: 0.2499 Train: 0.9140, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1385, Loss: 0.2678 Train: 0.9099, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1386, Loss: 0.2537 Train: 0.9067, Val: 0.7462, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1387, Loss: 0.2643 Train: 0.9018, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1388, Loss: 0.2413 Train: 0.9018, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1389, Loss: 0.2399 Train: 0.9067, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1390, Loss: 0.2462 Train: 0.9042, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1391, Loss: 0.2885 Train: 0.9050, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1392, Loss: 0.2616 Train: 0.8969, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1393, Loss: 0.2364 Train: 0.8994, Val: 0.7462, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1394, Loss: 0.2466 Train: 0.9091, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1395, Loss: 0.2456 Train: 0.9107, Val: 0.7652, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1396, Loss: 0.2598 Train: 0.9018, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1397, Loss: 0.2447 Train: 0.8994, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1398, Loss: 0.2340 Train: 0.8929, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1399, Loss: 0.2463 Train: 0.8896, Val: 0.7652, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1400, Loss: 0.2552 Train: 0.9002, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1401, Loss: 0.2751 Train: 0.9188, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1402, Loss: 0.2507 Train: 0.9026, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1403, Loss: 0.2500 Train: 0.9010, Val: 0.7197, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1404, Loss: 0.2365 Train: 0.9018, Val: 0.7311, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1405, Loss: 0.2594 Train: 0.9050, Val: 0.7386, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1406, Loss: 0.2516 Train: 0.9091, Val: 0.7500, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1407, Loss: 0.2508 Train: 0.9067, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1408, Loss: 0.2417 Train: 0.9123, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1409, Loss: 0.2462 Train: 0.9188, Val: 0.7538, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1410, Loss: 0.2392 Train: 0.9188, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1411, Loss: 0.2403 Train: 0.9091, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1412, Loss: 0.2477 Train: 0.9058, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1413, Loss: 0.2365 Train: 0.8969, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1414, Loss: 0.2391 Train: 0.9058, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1415, Loss: 0.2442 Train: 0.9083, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1416, Loss: 0.2382 Train: 0.9115, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1417, Loss: 0.2408 Train: 0.9123, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1418, Loss: 0.2335 Train: 0.9164, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1419, Loss: 0.2445 Train: 0.9156, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1420, Loss: 0.2401 Train: 0.9148, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1421, Loss: 0.2180 Train: 0.9099, Val: 0.7652, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1422, Loss: 0.2304 Train: 0.9067, Val: 0.7727, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1423, Loss: 0.2741 Train: 0.9018, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1424, Loss: 0.2330 Train: 0.9018, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1425, Loss: 0.2619 Train: 0.9131, Val: 0.7689, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1426, Loss: 0.2431 Train: 0.9091, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1427, Loss: 0.2400 Train: 0.9123, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1428, Loss: 0.2352 Train: 0.9164, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1429, Loss: 0.2579 Train: 0.9107, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1430, Loss: 0.2516 Train: 0.9131, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1431, Loss: 0.2369 Train: 0.9164, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1432, Loss: 0.2505 Train: 0.9115, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1433, Loss: 0.2360 Train: 0.9115, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1434, Loss: 0.2489 Train: 0.9164, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1435, Loss: 0.2331 Train: 0.9131, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1436, Loss: 0.2645 Train: 0.9083, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1437, Loss: 0.2613 Train: 0.9083, Val: 0.7576, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1438, Loss: 0.2616 Train: 0.9067, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1439, Loss: 0.2598 Train: 0.9107, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1440, Loss: 0.2555 Train: 0.9148, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1441, Loss: 0.2416 Train: 0.9180, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1442, Loss: 0.2411 Train: 0.9205, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1443, Loss: 0.2481 Train: 0.9164, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1444, Loss: 0.2378 Train: 0.9140, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1445, Loss: 0.2327 Train: 0.9075, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1446, Loss: 0.2402 Train: 0.9075, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1447, Loss: 0.2348 Train: 0.9075, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1448, Loss: 0.2304 Train: 0.9075, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1449, Loss: 0.2457 Train: 0.9083, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1450, Loss: 0.2280 Train: 0.9115, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1451, Loss: 0.2524 Train: 0.9099, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1452, Loss: 0.2325 Train: 0.9083, Val: 0.7462, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1453, Loss: 0.2382 Train: 0.9075, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1454, Loss: 0.2235 Train: 0.9140, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1455, Loss: 0.2344 Train: 0.9229, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1456, Loss: 0.2280 Train: 0.9205, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1457, Loss: 0.2412 Train: 0.9205, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1458, Loss: 0.2223 Train: 0.9156, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1459, Loss: 0.2191 Train: 0.9091, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1460, Loss: 0.2388 Train: 0.9099, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1461, Loss: 0.2206 Train: 0.9180, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1462, Loss: 0.2183 Train: 0.9148, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1463, Loss: 0.2215 Train: 0.9099, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1464, Loss: 0.2287 Train: 0.9107, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1465, Loss: 0.2222 Train: 0.9107, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1466, Loss: 0.2164 Train: 0.9115, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1467, Loss: 0.2365 Train: 0.9156, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1468, Loss: 0.2425 Train: 0.9172, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1469, Loss: 0.2129 Train: 0.9140, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1470, Loss: 0.2255 Train: 0.9140, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1471, Loss: 0.2429 Train: 0.9180, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1472, Loss: 0.2426 Train: 0.9188, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1473, Loss: 0.2372 Train: 0.9156, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1474, Loss: 0.2411 Train: 0.9156, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1475, Loss: 0.2398 Train: 0.9091, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1476, Loss: 0.2328 Train: 0.9002, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1477, Loss: 0.2208 Train: 0.9018, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1478, Loss: 0.2528 Train: 0.9091, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1479, Loss: 0.2224 Train: 0.9172, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1480, Loss: 0.2284 Train: 0.9253, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1481, Loss: 0.2267 Train: 0.9205, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1482, Loss: 0.2282 Train: 0.9140, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1483, Loss: 0.2207 Train: 0.9026, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1484, Loss: 0.2250 Train: 0.9058, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1485, Loss: 0.2167 Train: 0.9123, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1486, Loss: 0.2245 Train: 0.9188, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1487, Loss: 0.2213 Train: 0.9148, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1488, Loss: 0.2290 Train: 0.9091, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1489, Loss: 0.2090 Train: 0.9042, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1490, Loss: 0.2151 Train: 0.9010, Val: 0.7197, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1491, Loss: 0.2278 Train: 0.9099, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1492, Loss: 0.2314 Train: 0.9156, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1493, Loss: 0.2551 Train: 0.9131, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1494, Loss: 0.2321 Train: 0.9180, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1495, Loss: 0.2178 Train: 0.9156, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1496, Loss: 0.2374 Train: 0.9058, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1497, Loss: 0.2460 Train: 0.9058, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1498, Loss: 0.2175 Train: 0.9075, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1499, Loss: 0.2488 Train: 0.9123, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1500, Loss: 0.2312 Train: 0.9172, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1501, Loss: 0.2246 Train: 0.9172, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1502, Loss: 0.2206 Train: 0.9221, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1503, Loss: 0.2243 Train: 0.9237, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1504, Loss: 0.2434 Train: 0.9196, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1505, Loss: 0.2289 Train: 0.9164, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1506, Loss: 0.2350 Train: 0.9180, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1507, Loss: 0.2483 Train: 0.9196, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1508, Loss: 0.2219 Train: 0.9156, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1509, Loss: 0.2361 Train: 0.9156, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1510, Loss: 0.2359 Train: 0.9099, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1511, Loss: 0.2371 Train: 0.9067, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1512, Loss: 0.2375 Train: 0.9083, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1513, Loss: 0.2260 Train: 0.9164, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1514, Loss: 0.2360 Train: 0.9188, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1515, Loss: 0.2221 Train: 0.9180, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1516, Loss: 0.2394 Train: 0.9172, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1517, Loss: 0.2350 Train: 0.9180, Val: 0.7235, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1518, Loss: 0.2093 Train: 0.9164, Val: 0.7008, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1519, Loss: 0.2533 Train: 0.9196, Val: 0.7083, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1520, Loss: 0.2182 Train: 0.9172, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1521, Loss: 0.2251 Train: 0.9083, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1522, Loss: 0.2302 Train: 0.9058, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1523, Loss: 0.2386 Train: 0.9042, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1524, Loss: 0.2570 Train: 0.9148, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1525, Loss: 0.2268 Train: 0.9221, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1526, Loss: 0.2293 Train: 0.9164, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1527, Loss: 0.2137 Train: 0.9172, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1528, Loss: 0.2212 Train: 0.9148, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1529, Loss: 0.2366 Train: 0.9172, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 1530, Loss: 0.2359 Train: 0.9164, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1531, Loss: 0.2274 Train: 0.9140, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1532, Loss: 0.2266 Train: 0.9083, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1533, Loss: 0.2406 Train: 0.9075, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1534, Loss: 0.2303 Train: 0.9140, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1535, Loss: 0.2085 Train: 0.9164, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1536, Loss: 0.2192 Train: 0.9188, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1537, Loss: 0.2127 Train: 0.9253, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1538, Loss: 0.3019 Train: 0.9140, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1539, Loss: 0.2585 Train: 0.8920, Val: 0.7045, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1540, Loss: 0.2714 Train: 0.8880, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1541, Loss: 0.3130 Train: 0.8937, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1542, Loss: 0.2936 Train: 0.8920, Val: 0.7500, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 1543, Loss: 0.2816 Train: 0.8831, Val: 0.7386, Test: 0.7424, Final Test: 0.8182\n",
            "Epoch: 1544, Loss: 0.3010 Train: 0.8774, Val: 0.7500, Test: 0.7462, Final Test: 0.8182\n",
            "Epoch: 1545, Loss: 0.3046 Train: 0.8839, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1546, Loss: 0.3162 Train: 0.8977, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1547, Loss: 0.2531 Train: 0.8994, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1548, Loss: 0.2758 Train: 0.9002, Val: 0.7386, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1549, Loss: 0.2967 Train: 0.9018, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1550, Loss: 0.2721 Train: 0.9002, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1551, Loss: 0.2952 Train: 0.8985, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1552, Loss: 0.3003 Train: 0.9026, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1553, Loss: 0.2945 Train: 0.9026, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1554, Loss: 0.2535 Train: 0.8985, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1555, Loss: 0.2808 Train: 0.9002, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1556, Loss: 0.2562 Train: 0.9010, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1557, Loss: 0.2707 Train: 0.8961, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1558, Loss: 0.2583 Train: 0.9010, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1559, Loss: 0.2356 Train: 0.9002, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1560, Loss: 0.2485 Train: 0.9067, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1561, Loss: 0.2583 Train: 0.9083, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1562, Loss: 0.2566 Train: 0.9164, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1563, Loss: 0.2561 Train: 0.9140, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1564, Loss: 0.2597 Train: 0.9148, Val: 0.7652, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1565, Loss: 0.2424 Train: 0.9115, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1566, Loss: 0.2472 Train: 0.9042, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1567, Loss: 0.2536 Train: 0.9058, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1568, Loss: 0.2363 Train: 0.9050, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1569, Loss: 0.2472 Train: 0.9115, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1570, Loss: 0.2539 Train: 0.9075, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1571, Loss: 0.2667 Train: 0.9010, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1572, Loss: 0.2376 Train: 0.8953, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1573, Loss: 0.2533 Train: 0.9026, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1574, Loss: 0.2577 Train: 0.9083, Val: 0.7652, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1575, Loss: 0.2342 Train: 0.9042, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1576, Loss: 0.2379 Train: 0.9050, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1577, Loss: 0.2516 Train: 0.9002, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1578, Loss: 0.2554 Train: 0.8945, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1579, Loss: 0.2340 Train: 0.8945, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1580, Loss: 0.2444 Train: 0.8904, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1581, Loss: 0.2387 Train: 0.9058, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1582, Loss: 0.2656 Train: 0.9188, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1583, Loss: 0.2516 Train: 0.9213, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1584, Loss: 0.2621 Train: 0.9245, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1585, Loss: 0.2236 Train: 0.9221, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1586, Loss: 0.2278 Train: 0.9148, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1587, Loss: 0.2617 Train: 0.9083, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1588, Loss: 0.2455 Train: 0.9067, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1589, Loss: 0.2269 Train: 0.9237, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1590, Loss: 0.2351 Train: 0.9221, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1591, Loss: 0.2224 Train: 0.9196, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1592, Loss: 0.2358 Train: 0.9229, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1593, Loss: 0.2282 Train: 0.9205, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1594, Loss: 0.2235 Train: 0.9156, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1595, Loss: 0.2130 Train: 0.9131, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1596, Loss: 0.2266 Train: 0.9156, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1597, Loss: 0.2308 Train: 0.9205, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1598, Loss: 0.2281 Train: 0.9269, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1599, Loss: 0.2290 Train: 0.9229, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1600, Loss: 0.2400 Train: 0.9278, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1601, Loss: 0.2134 Train: 0.9213, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1602, Loss: 0.2344 Train: 0.9221, Val: 0.7500, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1603, Loss: 0.2196 Train: 0.9205, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1604, Loss: 0.2192 Train: 0.9164, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1605, Loss: 0.2244 Train: 0.9172, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1606, Loss: 0.2131 Train: 0.9253, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1607, Loss: 0.1874 Train: 0.9205, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1608, Loss: 0.2173 Train: 0.9229, Val: 0.7689, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1609, Loss: 0.2135 Train: 0.9245, Val: 0.7652, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1610, Loss: 0.2012 Train: 0.9221, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1611, Loss: 0.2210 Train: 0.9172, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1612, Loss: 0.2081 Train: 0.9180, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1613, Loss: 0.2260 Train: 0.9123, Val: 0.7538, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1614, Loss: 0.2294 Train: 0.9115, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1615, Loss: 0.2212 Train: 0.9172, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1616, Loss: 0.2316 Train: 0.9188, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1617, Loss: 0.2458 Train: 0.9140, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1618, Loss: 0.2393 Train: 0.9115, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1619, Loss: 0.2217 Train: 0.9123, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1620, Loss: 0.2327 Train: 0.9131, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1621, Loss: 0.2325 Train: 0.9188, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1622, Loss: 0.2082 Train: 0.9269, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1623, Loss: 0.2415 Train: 0.9278, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1624, Loss: 0.2127 Train: 0.9286, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1625, Loss: 0.2202 Train: 0.9188, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1626, Loss: 0.2363 Train: 0.9261, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1627, Loss: 0.2106 Train: 0.9261, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1628, Loss: 0.2423 Train: 0.9245, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1629, Loss: 0.2231 Train: 0.9156, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1630, Loss: 0.2363 Train: 0.9042, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1631, Loss: 0.2566 Train: 0.9067, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1632, Loss: 0.2523 Train: 0.9091, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1633, Loss: 0.2547 Train: 0.9156, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1634, Loss: 0.2195 Train: 0.9205, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1635, Loss: 0.2269 Train: 0.9221, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1636, Loss: 0.2377 Train: 0.9221, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1637, Loss: 0.2198 Train: 0.9205, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1638, Loss: 0.2348 Train: 0.9164, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1639, Loss: 0.2273 Train: 0.9196, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1640, Loss: 0.2027 Train: 0.9213, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1641, Loss: 0.2394 Train: 0.9196, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1642, Loss: 0.2135 Train: 0.9140, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1643, Loss: 0.2359 Train: 0.9131, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1644, Loss: 0.2106 Train: 0.9140, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1645, Loss: 0.2351 Train: 0.9237, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1646, Loss: 0.2314 Train: 0.9180, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1647, Loss: 0.2269 Train: 0.9229, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1648, Loss: 0.2280 Train: 0.9205, Val: 0.7652, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1649, Loss: 0.2214 Train: 0.9205, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1650, Loss: 0.2230 Train: 0.9115, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1651, Loss: 0.2340 Train: 0.9172, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1652, Loss: 0.2232 Train: 0.9213, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1653, Loss: 0.2314 Train: 0.9261, Val: 0.7462, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1654, Loss: 0.2182 Train: 0.9237, Val: 0.7424, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1655, Loss: 0.2360 Train: 0.9172, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1656, Loss: 0.2605 Train: 0.9196, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1657, Loss: 0.2245 Train: 0.9261, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1658, Loss: 0.2345 Train: 0.9294, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1659, Loss: 0.2103 Train: 0.9245, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1660, Loss: 0.2281 Train: 0.9205, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1661, Loss: 0.2088 Train: 0.9156, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1662, Loss: 0.2163 Train: 0.9156, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1663, Loss: 0.2303 Train: 0.9140, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1664, Loss: 0.2224 Train: 0.9213, Val: 0.7652, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1665, Loss: 0.2180 Train: 0.9253, Val: 0.7614, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1666, Loss: 0.2061 Train: 0.9269, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1667, Loss: 0.2149 Train: 0.9302, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1668, Loss: 0.2171 Train: 0.9253, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1669, Loss: 0.2365 Train: 0.9180, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1670, Loss: 0.2237 Train: 0.9123, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1671, Loss: 0.2227 Train: 0.9164, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1672, Loss: 0.2097 Train: 0.9188, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1673, Loss: 0.2163 Train: 0.9196, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1674, Loss: 0.2027 Train: 0.9278, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1675, Loss: 0.2322 Train: 0.9294, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1676, Loss: 0.2092 Train: 0.9286, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1677, Loss: 0.2008 Train: 0.9188, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1678, Loss: 0.2118 Train: 0.9196, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1679, Loss: 0.2056 Train: 0.9172, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1680, Loss: 0.2150 Train: 0.9261, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1681, Loss: 0.2044 Train: 0.9261, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1682, Loss: 0.2192 Train: 0.9278, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1683, Loss: 0.2155 Train: 0.9294, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1684, Loss: 0.2189 Train: 0.9253, Val: 0.7424, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1685, Loss: 0.2052 Train: 0.9140, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1686, Loss: 0.2168 Train: 0.9067, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1687, Loss: 0.2282 Train: 0.9115, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1688, Loss: 0.2255 Train: 0.9253, Val: 0.7689, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1689, Loss: 0.2067 Train: 0.9294, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1690, Loss: 0.2355 Train: 0.9278, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1691, Loss: 0.2300 Train: 0.9294, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1692, Loss: 0.2219 Train: 0.9278, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1693, Loss: 0.2151 Train: 0.9229, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1694, Loss: 0.2153 Train: 0.9205, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1695, Loss: 0.1971 Train: 0.9156, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1696, Loss: 0.2378 Train: 0.9245, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1697, Loss: 0.2015 Train: 0.9302, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1698, Loss: 0.2027 Train: 0.9286, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1699, Loss: 0.2126 Train: 0.9269, Val: 0.7614, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1700, Loss: 0.2212 Train: 0.9294, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1701, Loss: 0.2391 Train: 0.9294, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1702, Loss: 0.2236 Train: 0.9261, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1703, Loss: 0.2165 Train: 0.9131, Val: 0.7273, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1704, Loss: 0.2212 Train: 0.9164, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1705, Loss: 0.2532 Train: 0.9245, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1706, Loss: 0.2113 Train: 0.9269, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1707, Loss: 0.2154 Train: 0.9237, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1708, Loss: 0.2395 Train: 0.9245, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1709, Loss: 0.2300 Train: 0.9164, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1710, Loss: 0.2208 Train: 0.9083, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1711, Loss: 0.2176 Train: 0.9099, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1712, Loss: 0.2142 Train: 0.9196, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1713, Loss: 0.2103 Train: 0.9278, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1714, Loss: 0.2292 Train: 0.9278, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1715, Loss: 0.2283 Train: 0.9294, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1716, Loss: 0.2098 Train: 0.9286, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1717, Loss: 0.2069 Train: 0.9196, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1718, Loss: 0.2081 Train: 0.9115, Val: 0.6970, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1719, Loss: 0.2081 Train: 0.9172, Val: 0.7121, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1720, Loss: 0.1963 Train: 0.9229, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1721, Loss: 0.2006 Train: 0.9245, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1722, Loss: 0.1965 Train: 0.9196, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1723, Loss: 0.1932 Train: 0.9229, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1724, Loss: 0.1920 Train: 0.9278, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1725, Loss: 0.1917 Train: 0.9221, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1726, Loss: 0.2052 Train: 0.9196, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1727, Loss: 0.2090 Train: 0.9286, Val: 0.7121, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1728, Loss: 0.2002 Train: 0.9261, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1729, Loss: 0.1922 Train: 0.9261, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1730, Loss: 0.1988 Train: 0.9278, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1731, Loss: 0.2091 Train: 0.9343, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1732, Loss: 0.2092 Train: 0.9237, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1733, Loss: 0.2025 Train: 0.9229, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1734, Loss: 0.2064 Train: 0.9213, Val: 0.7083, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1735, Loss: 0.1890 Train: 0.9180, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1736, Loss: 0.1971 Train: 0.9237, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1737, Loss: 0.1961 Train: 0.9310, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1738, Loss: 0.2047 Train: 0.9318, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1739, Loss: 0.2080 Train: 0.9278, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1740, Loss: 0.2142 Train: 0.9302, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1741, Loss: 0.2038 Train: 0.9269, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1742, Loss: 0.2054 Train: 0.9245, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1743, Loss: 0.2002 Train: 0.9278, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1744, Loss: 0.2272 Train: 0.9261, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1745, Loss: 0.2327 Train: 0.9188, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1746, Loss: 0.2088 Train: 0.9156, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1747, Loss: 0.2145 Train: 0.9180, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1748, Loss: 0.2302 Train: 0.9156, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1749, Loss: 0.2468 Train: 0.9123, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1750, Loss: 0.2319 Train: 0.9156, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1751, Loss: 0.2268 Train: 0.9172, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1752, Loss: 0.2125 Train: 0.9213, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1753, Loss: 0.2212 Train: 0.9172, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1754, Loss: 0.2165 Train: 0.9221, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1755, Loss: 0.2226 Train: 0.9294, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1756, Loss: 0.2204 Train: 0.9213, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1757, Loss: 0.2232 Train: 0.9221, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1758, Loss: 0.2322 Train: 0.9221, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1759, Loss: 0.2460 Train: 0.9156, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1760, Loss: 0.2154 Train: 0.9196, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1761, Loss: 0.2206 Train: 0.9164, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1762, Loss: 0.2253 Train: 0.9140, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1763, Loss: 0.2143 Train: 0.9091, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1764, Loss: 0.2246 Train: 0.9131, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1765, Loss: 0.2037 Train: 0.9172, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1766, Loss: 0.2261 Train: 0.9237, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1767, Loss: 0.2027 Train: 0.9278, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1768, Loss: 0.2225 Train: 0.9294, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1769, Loss: 0.2146 Train: 0.9310, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1770, Loss: 0.2175 Train: 0.9278, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1771, Loss: 0.2141 Train: 0.9229, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1772, Loss: 0.1999 Train: 0.9196, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1773, Loss: 0.2124 Train: 0.9205, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1774, Loss: 0.1968 Train: 0.9245, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1775, Loss: 0.2202 Train: 0.9229, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1776, Loss: 0.2150 Train: 0.9172, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1777, Loss: 0.2263 Train: 0.9229, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1778, Loss: 0.1973 Train: 0.9237, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1779, Loss: 0.2166 Train: 0.9245, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1780, Loss: 0.2019 Train: 0.9326, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1781, Loss: 0.2136 Train: 0.9343, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1782, Loss: 0.1832 Train: 0.9245, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1783, Loss: 0.1926 Train: 0.9278, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1784, Loss: 0.2208 Train: 0.9294, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1785, Loss: 0.1988 Train: 0.9278, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1786, Loss: 0.2211 Train: 0.9294, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1787, Loss: 0.2258 Train: 0.9278, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1788, Loss: 0.2177 Train: 0.9180, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1789, Loss: 0.2209 Train: 0.9172, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1790, Loss: 0.2343 Train: 0.9083, Val: 0.7121, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1791, Loss: 0.2057 Train: 0.9075, Val: 0.7121, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1792, Loss: 0.2176 Train: 0.9172, Val: 0.7045, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1793, Loss: 0.2308 Train: 0.9286, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1794, Loss: 0.2058 Train: 0.9164, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1795, Loss: 0.2162 Train: 0.9140, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1796, Loss: 0.2340 Train: 0.9099, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1797, Loss: 0.2241 Train: 0.9156, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1798, Loss: 0.2272 Train: 0.9148, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1799, Loss: 0.2326 Train: 0.9253, Val: 0.7121, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1800, Loss: 0.2109 Train: 0.9253, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1801, Loss: 0.2201 Train: 0.9278, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1802, Loss: 0.2133 Train: 0.9253, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1803, Loss: 0.2107 Train: 0.9237, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1804, Loss: 0.2151 Train: 0.9213, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1805, Loss: 0.2237 Train: 0.9164, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1806, Loss: 0.1987 Train: 0.9083, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1807, Loss: 0.2245 Train: 0.9172, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1808, Loss: 0.2121 Train: 0.9237, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1809, Loss: 0.1838 Train: 0.9213, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1810, Loss: 0.2326 Train: 0.9261, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1811, Loss: 0.2221 Train: 0.9253, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1812, Loss: 0.2147 Train: 0.9213, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1813, Loss: 0.1990 Train: 0.9188, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1814, Loss: 0.2046 Train: 0.9188, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1815, Loss: 0.1915 Train: 0.9229, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1816, Loss: 0.1995 Train: 0.9261, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1817, Loss: 0.1940 Train: 0.9302, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1818, Loss: 0.2158 Train: 0.9261, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1819, Loss: 0.1916 Train: 0.9245, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1820, Loss: 0.2101 Train: 0.9221, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1821, Loss: 0.1985 Train: 0.9229, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1822, Loss: 0.1886 Train: 0.9245, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1823, Loss: 0.2216 Train: 0.9286, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1824, Loss: 0.2363 Train: 0.9286, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1825, Loss: 0.2153 Train: 0.9294, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1826, Loss: 0.2187 Train: 0.9261, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1827, Loss: 0.2016 Train: 0.9172, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1828, Loss: 0.1991 Train: 0.9131, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1829, Loss: 0.2156 Train: 0.9269, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1830, Loss: 0.2133 Train: 0.9294, Val: 0.7538, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1831, Loss: 0.1880 Train: 0.9343, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1832, Loss: 0.2161 Train: 0.9334, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1833, Loss: 0.2032 Train: 0.9343, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1834, Loss: 0.2053 Train: 0.9286, Val: 0.7235, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1835, Loss: 0.2056 Train: 0.9278, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1836, Loss: 0.2113 Train: 0.9237, Val: 0.7083, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1837, Loss: 0.1937 Train: 0.9237, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1838, Loss: 0.2089 Train: 0.9245, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1839, Loss: 0.2164 Train: 0.9318, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1840, Loss: 0.1847 Train: 0.9359, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1841, Loss: 0.1884 Train: 0.9343, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1842, Loss: 0.1947 Train: 0.9359, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1843, Loss: 0.2080 Train: 0.9375, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1844, Loss: 0.1949 Train: 0.9302, Val: 0.7121, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1845, Loss: 0.1969 Train: 0.9245, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1846, Loss: 0.1868 Train: 0.9269, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1847, Loss: 0.1746 Train: 0.9237, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1848, Loss: 0.2068 Train: 0.9269, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1849, Loss: 0.1854 Train: 0.9351, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1850, Loss: 0.1991 Train: 0.9367, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1851, Loss: 0.1911 Train: 0.9367, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1852, Loss: 0.1897 Train: 0.9351, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1853, Loss: 0.2036 Train: 0.9351, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1854, Loss: 0.1874 Train: 0.9359, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 1855, Loss: 0.1920 Train: 0.9399, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1856, Loss: 0.1858 Train: 0.9399, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1857, Loss: 0.1823 Train: 0.9383, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1858, Loss: 0.2010 Train: 0.9294, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1859, Loss: 0.1956 Train: 0.9269, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1860, Loss: 0.2079 Train: 0.9294, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 1861, Loss: 0.1986 Train: 0.9334, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1862, Loss: 0.1786 Train: 0.9326, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 1863, Loss: 0.1933 Train: 0.9326, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 1864, Loss: 0.1979 Train: 0.9294, Val: 0.7348, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 1865, Loss: 0.1747 Train: 0.9269, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1866, Loss: 0.1858 Train: 0.9278, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1867, Loss: 0.2043 Train: 0.9310, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1868, Loss: 0.1785 Train: 0.9351, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1869, Loss: 0.1772 Train: 0.9367, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1870, Loss: 0.1957 Train: 0.9310, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1871, Loss: 0.1919 Train: 0.9229, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1872, Loss: 0.1916 Train: 0.9213, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1873, Loss: 0.1837 Train: 0.9229, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1874, Loss: 0.1930 Train: 0.9237, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1875, Loss: 0.1862 Train: 0.9286, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1876, Loss: 0.1912 Train: 0.9334, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1877, Loss: 0.1936 Train: 0.9343, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1878, Loss: 0.1938 Train: 0.9351, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1879, Loss: 0.1924 Train: 0.9326, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1880, Loss: 0.1958 Train: 0.9294, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1881, Loss: 0.1779 Train: 0.9221, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1882, Loss: 0.1988 Train: 0.9180, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1883, Loss: 0.1973 Train: 0.9156, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1884, Loss: 0.1853 Train: 0.9213, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1885, Loss: 0.1779 Train: 0.9164, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1886, Loss: 0.2175 Train: 0.9253, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1887, Loss: 0.1883 Train: 0.9253, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1888, Loss: 0.1916 Train: 0.9294, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1889, Loss: 0.2390 Train: 0.9131, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1890, Loss: 0.2046 Train: 0.9083, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1891, Loss: 0.2005 Train: 0.9180, Val: 0.7348, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 1892, Loss: 0.2158 Train: 0.9188, Val: 0.7083, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1893, Loss: 0.1922 Train: 0.9156, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1894, Loss: 0.2223 Train: 0.9156, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1895, Loss: 0.2082 Train: 0.9115, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1896, Loss: 0.2164 Train: 0.9099, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1897, Loss: 0.2342 Train: 0.9075, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1898, Loss: 0.2216 Train: 0.9140, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1899, Loss: 0.2184 Train: 0.9221, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1900, Loss: 0.1955 Train: 0.9343, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1901, Loss: 0.2033 Train: 0.9351, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1902, Loss: 0.2288 Train: 0.9351, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1903, Loss: 0.2132 Train: 0.9326, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1904, Loss: 0.2053 Train: 0.9237, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1905, Loss: 0.2109 Train: 0.9099, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1906, Loss: 0.2091 Train: 0.9123, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1907, Loss: 0.2100 Train: 0.9245, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1908, Loss: 0.1959 Train: 0.9334, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1909, Loss: 0.2139 Train: 0.9326, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1910, Loss: 0.2064 Train: 0.9294, Val: 0.7614, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1911, Loss: 0.2140 Train: 0.9261, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1912, Loss: 0.1972 Train: 0.9123, Val: 0.7273, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1913, Loss: 0.2296 Train: 0.9156, Val: 0.7235, Test: 0.7500, Final Test: 0.8182\n",
            "Epoch: 1914, Loss: 0.2101 Train: 0.9196, Val: 0.7197, Test: 0.7424, Final Test: 0.8182\n",
            "Epoch: 1915, Loss: 0.1916 Train: 0.9278, Val: 0.7159, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 1916, Loss: 0.2066 Train: 0.9294, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1917, Loss: 0.2189 Train: 0.9318, Val: 0.7424, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1918, Loss: 0.2194 Train: 0.9245, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1919, Loss: 0.2175 Train: 0.9188, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1920, Loss: 0.2096 Train: 0.9131, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1921, Loss: 0.2278 Train: 0.9140, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1922, Loss: 0.2132 Train: 0.9221, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1923, Loss: 0.2019 Train: 0.9180, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1924, Loss: 0.2172 Train: 0.9229, Val: 0.7197, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1925, Loss: 0.1963 Train: 0.9269, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1926, Loss: 0.1872 Train: 0.9229, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 1927, Loss: 0.2095 Train: 0.9310, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1928, Loss: 0.1795 Train: 0.9294, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1929, Loss: 0.1937 Train: 0.9237, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1930, Loss: 0.2032 Train: 0.9180, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1931, Loss: 0.2078 Train: 0.9180, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1932, Loss: 0.1948 Train: 0.9221, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1933, Loss: 0.1861 Train: 0.9253, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1934, Loss: 0.2057 Train: 0.9245, Val: 0.7500, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 1935, Loss: 0.1844 Train: 0.9261, Val: 0.7614, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 1936, Loss: 0.2001 Train: 0.9326, Val: 0.7576, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 1937, Loss: 0.2175 Train: 0.9367, Val: 0.7652, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1938, Loss: 0.2136 Train: 0.9269, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 1939, Loss: 0.1986 Train: 0.9229, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1940, Loss: 0.2001 Train: 0.9294, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1941, Loss: 0.1766 Train: 0.9278, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1942, Loss: 0.2048 Train: 0.9302, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1943, Loss: 0.1817 Train: 0.9334, Val: 0.7045, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1944, Loss: 0.2142 Train: 0.9367, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1945, Loss: 0.1808 Train: 0.9326, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1946, Loss: 0.1672 Train: 0.9278, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1947, Loss: 0.1824 Train: 0.9302, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1948, Loss: 0.1972 Train: 0.9334, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1949, Loss: 0.2020 Train: 0.9407, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1950, Loss: 0.2052 Train: 0.9464, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1951, Loss: 0.1872 Train: 0.9440, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1952, Loss: 0.1845 Train: 0.9399, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1953, Loss: 0.2039 Train: 0.9334, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1954, Loss: 0.1920 Train: 0.9286, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1955, Loss: 0.1818 Train: 0.9196, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1956, Loss: 0.1870 Train: 0.9253, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1957, Loss: 0.1967 Train: 0.9375, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1958, Loss: 0.1733 Train: 0.9416, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1959, Loss: 0.1793 Train: 0.9440, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1960, Loss: 0.1778 Train: 0.9424, Val: 0.7424, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1961, Loss: 0.1816 Train: 0.9424, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1962, Loss: 0.1967 Train: 0.9351, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1963, Loss: 0.1736 Train: 0.9278, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1964, Loss: 0.1832 Train: 0.9253, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1965, Loss: 0.1887 Train: 0.9318, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 1966, Loss: 0.1776 Train: 0.9375, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1967, Loss: 0.1850 Train: 0.9391, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1968, Loss: 0.1831 Train: 0.9326, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1969, Loss: 0.2042 Train: 0.9261, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1970, Loss: 0.1945 Train: 0.9221, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 1971, Loss: 0.1843 Train: 0.9180, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1972, Loss: 0.2006 Train: 0.9221, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1973, Loss: 0.1875 Train: 0.9269, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1974, Loss: 0.1754 Train: 0.9326, Val: 0.7614, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1975, Loss: 0.2064 Train: 0.9334, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1976, Loss: 0.2027 Train: 0.9310, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1977, Loss: 0.1881 Train: 0.9326, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 1978, Loss: 0.1962 Train: 0.9343, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 1979, Loss: 0.2002 Train: 0.9334, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1980, Loss: 0.1852 Train: 0.9302, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1981, Loss: 0.2031 Train: 0.9318, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 1982, Loss: 0.1822 Train: 0.9302, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1983, Loss: 0.1794 Train: 0.9278, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1984, Loss: 0.1832 Train: 0.9269, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1985, Loss: 0.2008 Train: 0.9261, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 1986, Loss: 0.1873 Train: 0.9286, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1987, Loss: 0.1840 Train: 0.9278, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1988, Loss: 0.1675 Train: 0.9310, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1989, Loss: 0.1922 Train: 0.9351, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1990, Loss: 0.1867 Train: 0.9367, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1991, Loss: 0.1899 Train: 0.9278, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 1992, Loss: 0.1920 Train: 0.9261, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 1993, Loss: 0.1834 Train: 0.9326, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1994, Loss: 0.1832 Train: 0.9318, Val: 0.7614, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 1995, Loss: 0.1625 Train: 0.9310, Val: 0.7652, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 1996, Loss: 0.1941 Train: 0.9318, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1997, Loss: 0.1848 Train: 0.9343, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 1998, Loss: 0.1783 Train: 0.9383, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 1999, Loss: 0.1742 Train: 0.9351, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2000, Loss: 0.1817 Train: 0.9334, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2001, Loss: 0.1790 Train: 0.9302, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2002, Loss: 0.1813 Train: 0.9286, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2003, Loss: 0.1920 Train: 0.9278, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2004, Loss: 0.2012 Train: 0.9351, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2005, Loss: 0.1785 Train: 0.9383, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2006, Loss: 0.1817 Train: 0.9391, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2007, Loss: 0.2092 Train: 0.9375, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2008, Loss: 0.1765 Train: 0.9334, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2009, Loss: 0.2007 Train: 0.9375, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2010, Loss: 0.1988 Train: 0.9432, Val: 0.7538, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2011, Loss: 0.1853 Train: 0.9416, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2012, Loss: 0.1856 Train: 0.9326, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2013, Loss: 0.2221 Train: 0.9253, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2014, Loss: 0.1634 Train: 0.9172, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2015, Loss: 0.1890 Train: 0.9156, Val: 0.7121, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 2016, Loss: 0.1960 Train: 0.9229, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2017, Loss: 0.1722 Train: 0.9253, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2018, Loss: 0.1776 Train: 0.9326, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2019, Loss: 0.1936 Train: 0.9318, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2020, Loss: 0.1772 Train: 0.9318, Val: 0.7652, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2021, Loss: 0.1823 Train: 0.9221, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2022, Loss: 0.2087 Train: 0.9188, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2023, Loss: 0.1968 Train: 0.9172, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2024, Loss: 0.2367 Train: 0.9075, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2025, Loss: 0.2468 Train: 0.9164, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2026, Loss: 0.2037 Train: 0.9278, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2027, Loss: 0.2285 Train: 0.9253, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2028, Loss: 0.2324 Train: 0.9213, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2029, Loss: 0.2352 Train: 0.9205, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2030, Loss: 0.2114 Train: 0.9107, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2031, Loss: 0.2016 Train: 0.9156, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2032, Loss: 0.2117 Train: 0.9196, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2033, Loss: 0.2252 Train: 0.9213, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2034, Loss: 0.2205 Train: 0.9278, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2035, Loss: 0.2478 Train: 0.9334, Val: 0.7197, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 2036, Loss: 0.2156 Train: 0.9367, Val: 0.7159, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2037, Loss: 0.2201 Train: 0.9359, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2038, Loss: 0.2199 Train: 0.9310, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2039, Loss: 0.2097 Train: 0.9310, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2040, Loss: 0.2209 Train: 0.9326, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2041, Loss: 0.1967 Train: 0.9407, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2042, Loss: 0.2106 Train: 0.9383, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2043, Loss: 0.2118 Train: 0.9383, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2044, Loss: 0.2283 Train: 0.9399, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2045, Loss: 0.2109 Train: 0.9375, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2046, Loss: 0.2125 Train: 0.9310, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2047, Loss: 0.2104 Train: 0.9253, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2048, Loss: 0.2079 Train: 0.9261, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2049, Loss: 0.1917 Train: 0.9294, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2050, Loss: 0.2446 Train: 0.9351, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2051, Loss: 0.1948 Train: 0.9343, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2052, Loss: 0.1940 Train: 0.9310, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2053, Loss: 0.2217 Train: 0.9286, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2054, Loss: 0.1925 Train: 0.9245, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2055, Loss: 0.2184 Train: 0.9148, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2056, Loss: 0.2052 Train: 0.9172, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2057, Loss: 0.2281 Train: 0.9302, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2058, Loss: 0.1815 Train: 0.9343, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2059, Loss: 0.1954 Train: 0.9383, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2060, Loss: 0.1998 Train: 0.9383, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2061, Loss: 0.1878 Train: 0.9375, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2062, Loss: 0.2092 Train: 0.9334, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2063, Loss: 0.1972 Train: 0.9310, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2064, Loss: 0.2021 Train: 0.9286, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2065, Loss: 0.2027 Train: 0.9294, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2066, Loss: 0.1910 Train: 0.9326, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2067, Loss: 0.1839 Train: 0.9343, Val: 0.7083, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2068, Loss: 0.1829 Train: 0.9343, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2069, Loss: 0.1951 Train: 0.9391, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2070, Loss: 0.1939 Train: 0.9383, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2071, Loss: 0.1834 Train: 0.9286, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2072, Loss: 0.1822 Train: 0.9196, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2073, Loss: 0.1821 Train: 0.9221, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2074, Loss: 0.2088 Train: 0.9269, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2075, Loss: 0.1894 Train: 0.9286, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2076, Loss: 0.1873 Train: 0.9245, Val: 0.7083, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2077, Loss: 0.1868 Train: 0.9229, Val: 0.7083, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2078, Loss: 0.1948 Train: 0.9261, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2079, Loss: 0.1906 Train: 0.9278, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2080, Loss: 0.1937 Train: 0.9302, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2081, Loss: 0.1795 Train: 0.9351, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2082, Loss: 0.1668 Train: 0.9391, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2083, Loss: 0.1930 Train: 0.9383, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2084, Loss: 0.1870 Train: 0.9359, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2085, Loss: 0.1815 Train: 0.9399, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2086, Loss: 0.1807 Train: 0.9416, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2087, Loss: 0.1648 Train: 0.9310, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2088, Loss: 0.1887 Train: 0.9269, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2089, Loss: 0.1740 Train: 0.9253, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2090, Loss: 0.1763 Train: 0.9245, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2091, Loss: 0.1773 Train: 0.9310, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2092, Loss: 0.2018 Train: 0.9383, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2093, Loss: 0.1825 Train: 0.9407, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2094, Loss: 0.1863 Train: 0.9399, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2095, Loss: 0.1688 Train: 0.9359, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2096, Loss: 0.1804 Train: 0.9343, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2097, Loss: 0.1888 Train: 0.9343, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2098, Loss: 0.1878 Train: 0.9269, Val: 0.7197, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 2099, Loss: 0.1788 Train: 0.9221, Val: 0.7008, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2100, Loss: 0.1889 Train: 0.9286, Val: 0.6970, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2101, Loss: 0.1926 Train: 0.9326, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2102, Loss: 0.1791 Train: 0.9286, Val: 0.7045, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2103, Loss: 0.1903 Train: 0.9278, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2104, Loss: 0.1645 Train: 0.9229, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2105, Loss: 0.1776 Train: 0.9229, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2106, Loss: 0.1799 Train: 0.9245, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2107, Loss: 0.1837 Train: 0.9237, Val: 0.7121, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2108, Loss: 0.2013 Train: 0.9245, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2109, Loss: 0.1910 Train: 0.9351, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2110, Loss: 0.1831 Train: 0.9351, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2111, Loss: 0.1905 Train: 0.9367, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2112, Loss: 0.1842 Train: 0.9269, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2113, Loss: 0.1813 Train: 0.9156, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2114, Loss: 0.1990 Train: 0.9205, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2115, Loss: 0.2088 Train: 0.9261, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2116, Loss: 0.1974 Train: 0.9367, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2117, Loss: 0.1835 Train: 0.9302, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2118, Loss: 0.2005 Train: 0.9237, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2119, Loss: 0.1936 Train: 0.9286, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2120, Loss: 0.1965 Train: 0.9221, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2121, Loss: 0.2019 Train: 0.9253, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2122, Loss: 0.1990 Train: 0.9343, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2123, Loss: 0.1712 Train: 0.9334, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2124, Loss: 0.1805 Train: 0.9310, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2125, Loss: 0.1943 Train: 0.9213, Val: 0.6970, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2126, Loss: 0.2152 Train: 0.9180, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2127, Loss: 0.1945 Train: 0.9164, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2128, Loss: 0.1996 Train: 0.9196, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2129, Loss: 0.2125 Train: 0.9351, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2130, Loss: 0.1713 Train: 0.9448, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2131, Loss: 0.2144 Train: 0.9407, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2132, Loss: 0.2060 Train: 0.9375, Val: 0.7008, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2133, Loss: 0.1939 Train: 0.9237, Val: 0.6894, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2134, Loss: 0.1764 Train: 0.9172, Val: 0.6932, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2135, Loss: 0.2000 Train: 0.9131, Val: 0.6970, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2136, Loss: 0.1926 Train: 0.9026, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2137, Loss: 0.1980 Train: 0.9148, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2138, Loss: 0.1966 Train: 0.9375, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2139, Loss: 0.2010 Train: 0.9391, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2140, Loss: 0.1986 Train: 0.9310, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2141, Loss: 0.2114 Train: 0.9302, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2142, Loss: 0.1889 Train: 0.9326, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2143, Loss: 0.2129 Train: 0.9278, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2144, Loss: 0.2149 Train: 0.9253, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2145, Loss: 0.2175 Train: 0.9351, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2146, Loss: 0.1812 Train: 0.9367, Val: 0.7803, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2147, Loss: 0.1912 Train: 0.9351, Val: 0.7689, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2148, Loss: 0.2099 Train: 0.9359, Val: 0.7689, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2149, Loss: 0.1868 Train: 0.9334, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2150, Loss: 0.1824 Train: 0.9245, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2151, Loss: 0.1728 Train: 0.9278, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2152, Loss: 0.1888 Train: 0.9334, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2153, Loss: 0.1780 Train: 0.9326, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2154, Loss: 0.1785 Train: 0.9310, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2155, Loss: 0.1959 Train: 0.9407, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2156, Loss: 0.1750 Train: 0.9432, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2157, Loss: 0.1863 Train: 0.9367, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2158, Loss: 0.1718 Train: 0.9334, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2159, Loss: 0.1998 Train: 0.9334, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2160, Loss: 0.1821 Train: 0.9359, Val: 0.7083, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2161, Loss: 0.1778 Train: 0.9326, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2162, Loss: 0.2082 Train: 0.9334, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2163, Loss: 0.1858 Train: 0.9286, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2164, Loss: 0.1802 Train: 0.9278, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2165, Loss: 0.1855 Train: 0.9261, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2166, Loss: 0.1893 Train: 0.9294, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2167, Loss: 0.1841 Train: 0.9286, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2168, Loss: 0.1810 Train: 0.9310, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2169, Loss: 0.1849 Train: 0.9269, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2170, Loss: 0.1857 Train: 0.9294, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2171, Loss: 0.1777 Train: 0.9351, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2172, Loss: 0.1894 Train: 0.9326, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2173, Loss: 0.1835 Train: 0.9237, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2174, Loss: 0.2056 Train: 0.9302, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2175, Loss: 0.1734 Train: 0.9229, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2176, Loss: 0.1935 Train: 0.9188, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2177, Loss: 0.2027 Train: 0.9286, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2178, Loss: 0.1932 Train: 0.9326, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2179, Loss: 0.2123 Train: 0.9294, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2180, Loss: 0.1898 Train: 0.9269, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2181, Loss: 0.2016 Train: 0.9286, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2182, Loss: 0.1878 Train: 0.9237, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2183, Loss: 0.1851 Train: 0.9205, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2184, Loss: 0.1804 Train: 0.9253, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2185, Loss: 0.1979 Train: 0.9351, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2186, Loss: 0.1871 Train: 0.9359, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2187, Loss: 0.1695 Train: 0.9318, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2188, Loss: 0.1816 Train: 0.9318, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2189, Loss: 0.1854 Train: 0.9286, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2190, Loss: 0.1891 Train: 0.9294, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2191, Loss: 0.1931 Train: 0.9326, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2192, Loss: 0.1872 Train: 0.9294, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2193, Loss: 0.1839 Train: 0.9359, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2194, Loss: 0.1804 Train: 0.9391, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2195, Loss: 0.1699 Train: 0.9432, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2196, Loss: 0.1710 Train: 0.9391, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2197, Loss: 0.1724 Train: 0.9407, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2198, Loss: 0.1970 Train: 0.9310, Val: 0.7273, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2199, Loss: 0.1937 Train: 0.9351, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2200, Loss: 0.1984 Train: 0.9351, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2201, Loss: 0.1930 Train: 0.9334, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2202, Loss: 0.1973 Train: 0.9391, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2203, Loss: 0.1894 Train: 0.9407, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2204, Loss: 0.1717 Train: 0.9334, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2205, Loss: 0.1991 Train: 0.9334, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2206, Loss: 0.1732 Train: 0.9367, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2207, Loss: 0.1889 Train: 0.9318, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2208, Loss: 0.1658 Train: 0.9334, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2209, Loss: 0.1692 Train: 0.9383, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2210, Loss: 0.1561 Train: 0.9391, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2211, Loss: 0.1668 Train: 0.9424, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2212, Loss: 0.1630 Train: 0.9424, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2213, Loss: 0.2011 Train: 0.9391, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2214, Loss: 0.1761 Train: 0.9278, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2215, Loss: 0.1972 Train: 0.9188, Val: 0.7159, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2216, Loss: 0.1808 Train: 0.9213, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2217, Loss: 0.2011 Train: 0.9302, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2218, Loss: 0.1809 Train: 0.9416, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2219, Loss: 0.1704 Train: 0.9424, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2220, Loss: 0.1817 Train: 0.9383, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2221, Loss: 0.1575 Train: 0.9416, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2222, Loss: 0.1707 Train: 0.9391, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2223, Loss: 0.1671 Train: 0.9375, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2224, Loss: 0.1666 Train: 0.9359, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2225, Loss: 0.1793 Train: 0.9318, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2226, Loss: 0.1871 Train: 0.9351, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2227, Loss: 0.1750 Train: 0.9343, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2228, Loss: 0.1593 Train: 0.9351, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2229, Loss: 0.1703 Train: 0.9359, Val: 0.7083, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2230, Loss: 0.1762 Train: 0.9432, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2231, Loss: 0.1730 Train: 0.9448, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2232, Loss: 0.1749 Train: 0.9424, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2233, Loss: 0.1588 Train: 0.9424, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2234, Loss: 0.1741 Train: 0.9383, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2235, Loss: 0.1932 Train: 0.9334, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2236, Loss: 0.1876 Train: 0.9326, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2237, Loss: 0.1655 Train: 0.9326, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2238, Loss: 0.1611 Train: 0.9302, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2239, Loss: 0.1733 Train: 0.9318, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2240, Loss: 0.1764 Train: 0.9359, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2241, Loss: 0.1618 Train: 0.9334, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2242, Loss: 0.1609 Train: 0.9310, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2243, Loss: 0.1781 Train: 0.9343, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2244, Loss: 0.1694 Train: 0.9383, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2245, Loss: 0.1704 Train: 0.9416, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2246, Loss: 0.1573 Train: 0.9383, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2247, Loss: 0.1695 Train: 0.9383, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2248, Loss: 0.1571 Train: 0.9399, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2249, Loss: 0.1597 Train: 0.9391, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2250, Loss: 0.1576 Train: 0.9359, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2251, Loss: 0.1508 Train: 0.9391, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2252, Loss: 0.1587 Train: 0.9432, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2253, Loss: 0.1692 Train: 0.9440, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2254, Loss: 0.1666 Train: 0.9391, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2255, Loss: 0.1720 Train: 0.9432, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2256, Loss: 0.1873 Train: 0.9456, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2257, Loss: 0.1835 Train: 0.9440, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2258, Loss: 0.1838 Train: 0.9407, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2259, Loss: 0.1736 Train: 0.9416, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2260, Loss: 0.2062 Train: 0.9310, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2261, Loss: 0.1751 Train: 0.9310, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2262, Loss: 0.1803 Train: 0.9359, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2263, Loss: 0.1697 Train: 0.9326, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2264, Loss: 0.1818 Train: 0.9343, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2265, Loss: 0.1867 Train: 0.9351, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2266, Loss: 0.1904 Train: 0.9383, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2267, Loss: 0.1718 Train: 0.9537, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2268, Loss: 0.1796 Train: 0.9594, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2269, Loss: 0.1683 Train: 0.9610, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2270, Loss: 0.1527 Train: 0.9456, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2271, Loss: 0.1670 Train: 0.9391, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2272, Loss: 0.1843 Train: 0.9416, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2273, Loss: 0.1721 Train: 0.9448, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2274, Loss: 0.1847 Train: 0.9456, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2275, Loss: 0.1679 Train: 0.9464, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2276, Loss: 0.1692 Train: 0.9432, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2277, Loss: 0.1791 Train: 0.9416, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2278, Loss: 0.1647 Train: 0.9416, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2279, Loss: 0.1817 Train: 0.9407, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2280, Loss: 0.1625 Train: 0.9391, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2281, Loss: 0.1724 Train: 0.9383, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2282, Loss: 0.1740 Train: 0.9391, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2283, Loss: 0.1691 Train: 0.9391, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2284, Loss: 0.1653 Train: 0.9407, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2285, Loss: 0.1606 Train: 0.9448, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2286, Loss: 0.1559 Train: 0.9464, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2287, Loss: 0.1576 Train: 0.9489, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2288, Loss: 0.1555 Train: 0.9456, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2289, Loss: 0.1552 Train: 0.9432, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2290, Loss: 0.2129 Train: 0.9375, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2291, Loss: 0.1737 Train: 0.9456, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2292, Loss: 0.1622 Train: 0.9472, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2293, Loss: 0.1561 Train: 0.9497, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2294, Loss: 0.1859 Train: 0.9497, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2295, Loss: 0.1675 Train: 0.9505, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2296, Loss: 0.1698 Train: 0.9383, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2297, Loss: 0.1663 Train: 0.9359, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2298, Loss: 0.1898 Train: 0.9359, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2299, Loss: 0.1813 Train: 0.9359, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2300, Loss: 0.1652 Train: 0.9383, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2301, Loss: 0.1891 Train: 0.9416, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2302, Loss: 0.1681 Train: 0.9424, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2303, Loss: 0.1815 Train: 0.9286, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2304, Loss: 0.1686 Train: 0.9343, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2305, Loss: 0.1625 Train: 0.9310, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2306, Loss: 0.1826 Train: 0.9286, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2307, Loss: 0.1892 Train: 0.9383, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2308, Loss: 0.1714 Train: 0.9416, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2309, Loss: 0.1700 Train: 0.9391, Val: 0.7273, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2310, Loss: 0.1871 Train: 0.9424, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2311, Loss: 0.1811 Train: 0.9407, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2312, Loss: 0.1886 Train: 0.9375, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2313, Loss: 0.1987 Train: 0.9351, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2314, Loss: 0.2784 Train: 0.9343, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2315, Loss: 0.1901 Train: 0.9367, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2316, Loss: 0.1922 Train: 0.9351, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2317, Loss: 0.1841 Train: 0.9334, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2318, Loss: 0.1845 Train: 0.9375, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2319, Loss: 0.1646 Train: 0.9343, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2320, Loss: 0.1693 Train: 0.9391, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2321, Loss: 0.1742 Train: 0.9367, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2322, Loss: 0.1800 Train: 0.9375, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2323, Loss: 0.1822 Train: 0.9391, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2324, Loss: 0.1787 Train: 0.9432, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2325, Loss: 0.1615 Train: 0.9464, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2326, Loss: 0.1573 Train: 0.9424, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2327, Loss: 0.2021 Train: 0.9164, Val: 0.6970, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2328, Loss: 0.3143 Train: 0.9180, Val: 0.7121, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2329, Loss: 0.2676 Train: 0.9164, Val: 0.7045, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2330, Loss: 0.1839 Train: 0.8945, Val: 0.6894, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 2331, Loss: 0.2505 Train: 0.8896, Val: 0.6856, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2332, Loss: 0.2475 Train: 0.9075, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2333, Loss: 0.2713 Train: 0.9164, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2334, Loss: 0.2254 Train: 0.9196, Val: 0.7197, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 2335, Loss: 0.2242 Train: 0.9205, Val: 0.7008, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2336, Loss: 0.2288 Train: 0.9229, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2337, Loss: 0.1851 Train: 0.9115, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2338, Loss: 0.2138 Train: 0.9058, Val: 0.7045, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2339, Loss: 0.2325 Train: 0.9099, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2340, Loss: 0.2106 Train: 0.9164, Val: 0.7045, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2341, Loss: 0.2163 Train: 0.9205, Val: 0.7121, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2342, Loss: 0.2157 Train: 0.9188, Val: 0.7045, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 2343, Loss: 0.2264 Train: 0.9164, Val: 0.7008, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2344, Loss: 0.2132 Train: 0.9221, Val: 0.7121, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2345, Loss: 0.1979 Train: 0.9302, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2346, Loss: 0.1881 Train: 0.9310, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2347, Loss: 0.1867 Train: 0.9302, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2348, Loss: 0.1866 Train: 0.9334, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2349, Loss: 0.1759 Train: 0.9334, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2350, Loss: 0.1832 Train: 0.9351, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2351, Loss: 0.1734 Train: 0.9302, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2352, Loss: 0.1885 Train: 0.9294, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2353, Loss: 0.1783 Train: 0.9245, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2354, Loss: 0.1846 Train: 0.9229, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2355, Loss: 0.1803 Train: 0.9318, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2356, Loss: 0.1891 Train: 0.9351, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2357, Loss: 0.1771 Train: 0.9432, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2358, Loss: 0.1891 Train: 0.9424, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2359, Loss: 0.1895 Train: 0.9383, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2360, Loss: 0.1574 Train: 0.9302, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2361, Loss: 0.1740 Train: 0.9310, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2362, Loss: 0.1747 Train: 0.9294, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2363, Loss: 0.1924 Train: 0.9359, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2364, Loss: 0.1817 Train: 0.9424, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2365, Loss: 0.1730 Train: 0.9375, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2366, Loss: 0.1804 Train: 0.9351, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2367, Loss: 0.1809 Train: 0.9383, Val: 0.7197, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 2368, Loss: 0.1762 Train: 0.9294, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2369, Loss: 0.1775 Train: 0.9278, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2370, Loss: 0.1883 Train: 0.9302, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2371, Loss: 0.1812 Train: 0.9310, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2372, Loss: 0.1693 Train: 0.9334, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2373, Loss: 0.1734 Train: 0.9278, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2374, Loss: 0.1754 Train: 0.9343, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2375, Loss: 0.1625 Train: 0.9464, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2376, Loss: 0.1563 Train: 0.9416, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2377, Loss: 0.1882 Train: 0.9416, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2378, Loss: 0.1814 Train: 0.9407, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2379, Loss: 0.1491 Train: 0.9359, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2380, Loss: 0.1588 Train: 0.9343, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2381, Loss: 0.1928 Train: 0.9310, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2382, Loss: 0.1667 Train: 0.9351, Val: 0.7197, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2383, Loss: 0.1611 Train: 0.9318, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2384, Loss: 0.1709 Train: 0.9375, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2385, Loss: 0.1770 Train: 0.9440, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2386, Loss: 0.1818 Train: 0.9424, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2387, Loss: 0.1565 Train: 0.9407, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2388, Loss: 0.1618 Train: 0.9399, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2389, Loss: 0.1650 Train: 0.9391, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2390, Loss: 0.1807 Train: 0.9375, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2391, Loss: 0.1598 Train: 0.9359, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2392, Loss: 0.1503 Train: 0.9343, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2393, Loss: 0.1635 Train: 0.9383, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2394, Loss: 0.1781 Train: 0.9375, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2395, Loss: 0.1632 Train: 0.9359, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2396, Loss: 0.1646 Train: 0.9326, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2397, Loss: 0.1591 Train: 0.9318, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2398, Loss: 0.2093 Train: 0.9391, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2399, Loss: 0.1723 Train: 0.9432, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2400, Loss: 0.1630 Train: 0.9432, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2401, Loss: 0.1679 Train: 0.9432, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2402, Loss: 0.1646 Train: 0.9383, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2403, Loss: 0.1805 Train: 0.9375, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2404, Loss: 0.1789 Train: 0.9351, Val: 0.7121, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2405, Loss: 0.1632 Train: 0.9294, Val: 0.7235, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 2406, Loss: 0.1927 Train: 0.9196, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2407, Loss: 0.1891 Train: 0.9245, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2408, Loss: 0.1697 Train: 0.9310, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2409, Loss: 0.1839 Train: 0.9343, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2410, Loss: 0.1846 Train: 0.9351, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2411, Loss: 0.1879 Train: 0.9359, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2412, Loss: 0.1806 Train: 0.9326, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2413, Loss: 0.1671 Train: 0.9343, Val: 0.7083, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2414, Loss: 0.1745 Train: 0.9310, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2415, Loss: 0.1677 Train: 0.9326, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2416, Loss: 0.1707 Train: 0.9359, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2417, Loss: 0.1854 Train: 0.9351, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2418, Loss: 0.1884 Train: 0.9367, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2419, Loss: 0.1752 Train: 0.9383, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2420, Loss: 0.1795 Train: 0.9391, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2421, Loss: 0.1714 Train: 0.9383, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2422, Loss: 0.1866 Train: 0.9286, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2423, Loss: 0.1863 Train: 0.9253, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2424, Loss: 0.1939 Train: 0.9205, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2425, Loss: 0.1721 Train: 0.9205, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2426, Loss: 0.1762 Train: 0.9164, Val: 0.7008, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2427, Loss: 0.2022 Train: 0.9131, Val: 0.6894, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2428, Loss: 0.1941 Train: 0.9156, Val: 0.6818, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2429, Loss: 0.1953 Train: 0.9107, Val: 0.6894, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2430, Loss: 0.1805 Train: 0.9221, Val: 0.6932, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2431, Loss: 0.1877 Train: 0.9294, Val: 0.7008, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2432, Loss: 0.1685 Train: 0.9253, Val: 0.7197, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 2433, Loss: 0.1821 Train: 0.9334, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2434, Loss: 0.1648 Train: 0.9351, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2435, Loss: 0.1741 Train: 0.9383, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2436, Loss: 0.1597 Train: 0.9367, Val: 0.7083, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2437, Loss: 0.1840 Train: 0.9326, Val: 0.7045, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2438, Loss: 0.2012 Train: 0.9334, Val: 0.7045, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2439, Loss: 0.1689 Train: 0.9359, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2440, Loss: 0.1603 Train: 0.9318, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2441, Loss: 0.1840 Train: 0.9343, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2442, Loss: 0.1631 Train: 0.9334, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2443, Loss: 0.1724 Train: 0.9334, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2444, Loss: 0.1838 Train: 0.9334, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2445, Loss: 0.1667 Train: 0.9375, Val: 0.6894, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2446, Loss: 0.1802 Train: 0.9375, Val: 0.6894, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2447, Loss: 0.1585 Train: 0.9399, Val: 0.7045, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2448, Loss: 0.1605 Train: 0.9399, Val: 0.7083, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2449, Loss: 0.1876 Train: 0.9440, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2450, Loss: 0.1612 Train: 0.9383, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2451, Loss: 0.1539 Train: 0.9302, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2452, Loss: 0.1762 Train: 0.9261, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2453, Loss: 0.1806 Train: 0.9326, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2454, Loss: 0.1639 Train: 0.9383, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2455, Loss: 0.1680 Train: 0.9432, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2456, Loss: 0.1475 Train: 0.9416, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2457, Loss: 0.1547 Train: 0.9416, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2458, Loss: 0.1641 Train: 0.9456, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2459, Loss: 0.1738 Train: 0.9432, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2460, Loss: 0.1825 Train: 0.9416, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2461, Loss: 0.1670 Train: 0.9359, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2462, Loss: 0.1595 Train: 0.9343, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2463, Loss: 0.1637 Train: 0.9334, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2464, Loss: 0.1491 Train: 0.9407, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2465, Loss: 0.1644 Train: 0.9472, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2466, Loss: 0.1538 Train: 0.9489, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2467, Loss: 0.1587 Train: 0.9481, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2468, Loss: 0.1933 Train: 0.9489, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2469, Loss: 0.1661 Train: 0.9497, Val: 0.7462, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 2470, Loss: 0.1579 Train: 0.9472, Val: 0.7576, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 2471, Loss: 0.1715 Train: 0.9416, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2472, Loss: 0.1726 Train: 0.9310, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2473, Loss: 0.1736 Train: 0.9253, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2474, Loss: 0.1885 Train: 0.9375, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2475, Loss: 0.1671 Train: 0.9407, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2476, Loss: 0.1540 Train: 0.9448, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2477, Loss: 0.1950 Train: 0.9464, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2478, Loss: 0.1628 Train: 0.9440, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2479, Loss: 0.1790 Train: 0.9407, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2480, Loss: 0.1754 Train: 0.9367, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2481, Loss: 0.2060 Train: 0.9278, Val: 0.7045, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2482, Loss: 0.1724 Train: 0.9334, Val: 0.6970, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2483, Loss: 0.1864 Train: 0.9286, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2484, Loss: 0.1747 Train: 0.9375, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2485, Loss: 0.1814 Train: 0.9407, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2486, Loss: 0.1843 Train: 0.9448, Val: 0.7083, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2487, Loss: 0.1772 Train: 0.9448, Val: 0.6932, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2488, Loss: 0.1735 Train: 0.9432, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2489, Loss: 0.1715 Train: 0.9424, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2490, Loss: 0.1612 Train: 0.9440, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2491, Loss: 0.1558 Train: 0.9399, Val: 0.7159, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2492, Loss: 0.1603 Train: 0.9383, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2493, Loss: 0.1735 Train: 0.9375, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2494, Loss: 0.1712 Train: 0.9424, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2495, Loss: 0.1682 Train: 0.9399, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2496, Loss: 0.1764 Train: 0.9432, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2497, Loss: 0.1519 Train: 0.9464, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2498, Loss: 0.1530 Train: 0.9497, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2499, Loss: 0.1455 Train: 0.9489, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2500, Loss: 0.1702 Train: 0.9513, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2501, Loss: 0.1648 Train: 0.9513, Val: 0.7652, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2502, Loss: 0.1596 Train: 0.9529, Val: 0.7614, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2503, Loss: 0.1800 Train: 0.9497, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2504, Loss: 0.1653 Train: 0.9489, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2505, Loss: 0.1613 Train: 0.9432, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2506, Loss: 0.1706 Train: 0.9416, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2507, Loss: 0.1800 Train: 0.9489, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2508, Loss: 0.1664 Train: 0.9464, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2509, Loss: 0.1523 Train: 0.9416, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2510, Loss: 0.1572 Train: 0.9375, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2511, Loss: 0.1637 Train: 0.9448, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2512, Loss: 0.1645 Train: 0.9505, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2513, Loss: 0.1672 Train: 0.9472, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2514, Loss: 0.1569 Train: 0.9424, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2515, Loss: 0.1547 Train: 0.9367, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2516, Loss: 0.1738 Train: 0.9375, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2517, Loss: 0.1909 Train: 0.9383, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2518, Loss: 0.1476 Train: 0.9424, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2519, Loss: 0.1721 Train: 0.9497, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2520, Loss: 0.1540 Train: 0.9505, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2521, Loss: 0.1758 Train: 0.9472, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2522, Loss: 0.1945 Train: 0.9448, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2523, Loss: 0.1641 Train: 0.9440, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2524, Loss: 0.1536 Train: 0.9391, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2525, Loss: 0.1561 Train: 0.9391, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2526, Loss: 0.1818 Train: 0.9456, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2527, Loss: 0.1563 Train: 0.9440, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2528, Loss: 0.1640 Train: 0.9472, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2529, Loss: 0.1576 Train: 0.9472, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2530, Loss: 0.1718 Train: 0.9481, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2531, Loss: 0.1579 Train: 0.9472, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2532, Loss: 0.1490 Train: 0.9448, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2533, Loss: 0.1799 Train: 0.9432, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2534, Loss: 0.1608 Train: 0.9416, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2535, Loss: 0.1660 Train: 0.9391, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2536, Loss: 0.1612 Train: 0.9432, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2537, Loss: 0.1750 Train: 0.9489, Val: 0.7386, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2538, Loss: 0.1586 Train: 0.9424, Val: 0.7273, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2539, Loss: 0.1636 Train: 0.9407, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2540, Loss: 0.1796 Train: 0.9448, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2541, Loss: 0.1460 Train: 0.9464, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2542, Loss: 0.1678 Train: 0.9497, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2543, Loss: 0.1548 Train: 0.9464, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2544, Loss: 0.1728 Train: 0.9497, Val: 0.7538, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2545, Loss: 0.1596 Train: 0.9472, Val: 0.7576, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 2546, Loss: 0.1600 Train: 0.9416, Val: 0.7424, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2547, Loss: 0.1692 Train: 0.9391, Val: 0.7538, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2548, Loss: 0.1684 Train: 0.9367, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2549, Loss: 0.1479 Train: 0.9391, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2550, Loss: 0.1594 Train: 0.9432, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2551, Loss: 0.1763 Train: 0.9521, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2552, Loss: 0.1691 Train: 0.9513, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2553, Loss: 0.1537 Train: 0.9505, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2554, Loss: 0.1620 Train: 0.9464, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2555, Loss: 0.1822 Train: 0.9432, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2556, Loss: 0.1546 Train: 0.9472, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2557, Loss: 0.1898 Train: 0.9481, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2558, Loss: 0.1861 Train: 0.9489, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2559, Loss: 0.1781 Train: 0.9472, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2560, Loss: 0.1630 Train: 0.9383, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2561, Loss: 0.1549 Train: 0.9407, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2562, Loss: 0.1589 Train: 0.9383, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2563, Loss: 0.1702 Train: 0.9407, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2564, Loss: 0.2003 Train: 0.9399, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2565, Loss: 0.1634 Train: 0.9399, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2566, Loss: 0.1619 Train: 0.9399, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2567, Loss: 0.1607 Train: 0.9391, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2568, Loss: 0.1483 Train: 0.9407, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2569, Loss: 0.1588 Train: 0.9432, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2570, Loss: 0.1650 Train: 0.9513, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2571, Loss: 0.1489 Train: 0.9472, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2572, Loss: 0.1634 Train: 0.9481, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2573, Loss: 0.1525 Train: 0.9529, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2574, Loss: 0.1542 Train: 0.9497, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2575, Loss: 0.1632 Train: 0.9456, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2576, Loss: 0.1371 Train: 0.9497, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2577, Loss: 0.1525 Train: 0.9489, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2578, Loss: 0.1598 Train: 0.9497, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2579, Loss: 0.1539 Train: 0.9432, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2580, Loss: 0.1572 Train: 0.9367, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2581, Loss: 0.1673 Train: 0.9416, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2582, Loss: 0.1646 Train: 0.9497, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2583, Loss: 0.1345 Train: 0.9594, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2584, Loss: 0.1679 Train: 0.9554, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2585, Loss: 0.1552 Train: 0.9537, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2586, Loss: 0.1441 Train: 0.9464, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2587, Loss: 0.1610 Train: 0.9448, Val: 0.7273, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2588, Loss: 0.1614 Train: 0.9391, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2589, Loss: 0.1698 Train: 0.9481, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2590, Loss: 0.1633 Train: 0.9537, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2591, Loss: 0.1730 Train: 0.9505, Val: 0.7727, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2592, Loss: 0.1559 Train: 0.9464, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2593, Loss: 0.1505 Train: 0.9497, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2594, Loss: 0.1596 Train: 0.9464, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2595, Loss: 0.1472 Train: 0.9375, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2596, Loss: 0.1591 Train: 0.9359, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2597, Loss: 0.1553 Train: 0.9399, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2598, Loss: 0.1692 Train: 0.9432, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2599, Loss: 0.1662 Train: 0.9489, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2600, Loss: 0.1594 Train: 0.9497, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2601, Loss: 0.1509 Train: 0.9481, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2602, Loss: 0.1612 Train: 0.9456, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2603, Loss: 0.1668 Train: 0.9472, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2604, Loss: 0.1472 Train: 0.9505, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2605, Loss: 0.1569 Train: 0.9537, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2606, Loss: 0.1649 Train: 0.9521, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2607, Loss: 0.2066 Train: 0.9529, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2608, Loss: 0.1541 Train: 0.9464, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2609, Loss: 0.1721 Train: 0.9432, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2610, Loss: 0.1659 Train: 0.9424, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2611, Loss: 0.1799 Train: 0.9456, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2612, Loss: 0.1744 Train: 0.9554, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2613, Loss: 0.1448 Train: 0.9489, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2614, Loss: 0.1511 Train: 0.9440, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2615, Loss: 0.1597 Train: 0.9472, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2616, Loss: 0.1583 Train: 0.9472, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2617, Loss: 0.1338 Train: 0.9456, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2618, Loss: 0.1651 Train: 0.9472, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2619, Loss: 0.1546 Train: 0.9481, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2620, Loss: 0.1673 Train: 0.9481, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2621, Loss: 0.1571 Train: 0.9554, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2622, Loss: 0.1546 Train: 0.9521, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2623, Loss: 0.1600 Train: 0.9489, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2624, Loss: 0.1698 Train: 0.9481, Val: 0.7500, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 2625, Loss: 0.1648 Train: 0.9440, Val: 0.7500, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2626, Loss: 0.1711 Train: 0.9440, Val: 0.7538, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2627, Loss: 0.1748 Train: 0.9472, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2628, Loss: 0.1582 Train: 0.9472, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2629, Loss: 0.1482 Train: 0.9440, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2630, Loss: 0.1679 Train: 0.9432, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2631, Loss: 0.1486 Train: 0.9448, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2632, Loss: 0.1573 Train: 0.9489, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2633, Loss: 0.1634 Train: 0.9529, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2634, Loss: 0.1688 Train: 0.9521, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2635, Loss: 0.1665 Train: 0.9464, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2636, Loss: 0.1670 Train: 0.9489, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2637, Loss: 0.1498 Train: 0.9464, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2638, Loss: 0.1659 Train: 0.9472, Val: 0.7121, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2639, Loss: 0.1771 Train: 0.9416, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2640, Loss: 0.1769 Train: 0.9383, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2641, Loss: 0.1628 Train: 0.9351, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2642, Loss: 0.1704 Train: 0.9505, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2643, Loss: 0.1551 Train: 0.9497, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2644, Loss: 0.1543 Train: 0.9489, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2645, Loss: 0.1601 Train: 0.9513, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2646, Loss: 0.1475 Train: 0.9570, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2647, Loss: 0.1886 Train: 0.9570, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2648, Loss: 0.1507 Train: 0.9554, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2649, Loss: 0.1596 Train: 0.9472, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2650, Loss: 0.1715 Train: 0.9448, Val: 0.7045, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2651, Loss: 0.1881 Train: 0.9375, Val: 0.6970, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2652, Loss: 0.1631 Train: 0.9359, Val: 0.7045, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2653, Loss: 0.1828 Train: 0.9456, Val: 0.6970, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2654, Loss: 0.1680 Train: 0.9529, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2655, Loss: 0.1570 Train: 0.9529, Val: 0.7273, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2656, Loss: 0.1623 Train: 0.9562, Val: 0.7197, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2657, Loss: 0.1748 Train: 0.9570, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2658, Loss: 0.1579 Train: 0.9505, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2659, Loss: 0.1672 Train: 0.9489, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2660, Loss: 0.1404 Train: 0.9407, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2661, Loss: 0.1422 Train: 0.9448, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2662, Loss: 0.1587 Train: 0.9456, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2663, Loss: 0.1507 Train: 0.9464, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2664, Loss: 0.1526 Train: 0.9489, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2665, Loss: 0.1457 Train: 0.9407, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2666, Loss: 0.1553 Train: 0.9440, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2667, Loss: 0.1676 Train: 0.9456, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2668, Loss: 0.1555 Train: 0.9489, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2669, Loss: 0.1522 Train: 0.9464, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2670, Loss: 0.1603 Train: 0.9472, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2671, Loss: 0.1547 Train: 0.9497, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2672, Loss: 0.1652 Train: 0.9497, Val: 0.6970, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2673, Loss: 0.1657 Train: 0.9489, Val: 0.7045, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2674, Loss: 0.1548 Train: 0.9481, Val: 0.7083, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2675, Loss: 0.1488 Train: 0.9513, Val: 0.6970, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2676, Loss: 0.1456 Train: 0.9545, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2677, Loss: 0.1683 Train: 0.9537, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2678, Loss: 0.1482 Train: 0.9537, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2679, Loss: 0.1471 Train: 0.9505, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2680, Loss: 0.1472 Train: 0.9513, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2681, Loss: 0.1637 Train: 0.9545, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2682, Loss: 0.1762 Train: 0.9545, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2683, Loss: 0.1373 Train: 0.9545, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2684, Loss: 0.1470 Train: 0.9562, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2685, Loss: 0.1510 Train: 0.9570, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2686, Loss: 0.1430 Train: 0.9562, Val: 0.7386, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2687, Loss: 0.1363 Train: 0.9513, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2688, Loss: 0.1318 Train: 0.9497, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2689, Loss: 0.1913 Train: 0.9481, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2690, Loss: 0.1440 Train: 0.9529, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2691, Loss: 0.1549 Train: 0.9545, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2692, Loss: 0.1623 Train: 0.9505, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2693, Loss: 0.1711 Train: 0.9489, Val: 0.7083, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2694, Loss: 0.1429 Train: 0.9448, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2695, Loss: 0.1495 Train: 0.9432, Val: 0.7008, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2696, Loss: 0.1371 Train: 0.9343, Val: 0.6932, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2697, Loss: 0.1495 Train: 0.9383, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2698, Loss: 0.1643 Train: 0.9481, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2699, Loss: 0.1504 Train: 0.9521, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2700, Loss: 0.1745 Train: 0.9513, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2701, Loss: 0.1556 Train: 0.9521, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2702, Loss: 0.1376 Train: 0.9399, Val: 0.7045, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2703, Loss: 0.1414 Train: 0.9343, Val: 0.6894, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2704, Loss: 0.1532 Train: 0.9391, Val: 0.6856, Test: 0.7462, Final Test: 0.8182\n",
            "Epoch: 2705, Loss: 0.1747 Train: 0.9399, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2706, Loss: 0.1505 Train: 0.9407, Val: 0.7159, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2707, Loss: 0.1703 Train: 0.9448, Val: 0.7197, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2708, Loss: 0.1637 Train: 0.9521, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2709, Loss: 0.1533 Train: 0.9489, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2710, Loss: 0.1602 Train: 0.9440, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2711, Loss: 0.1691 Train: 0.9456, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2712, Loss: 0.1865 Train: 0.9570, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2713, Loss: 0.1543 Train: 0.9456, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2714, Loss: 0.1888 Train: 0.9432, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2715, Loss: 0.1767 Train: 0.9440, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2716, Loss: 0.1667 Train: 0.9440, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2717, Loss: 0.1728 Train: 0.9416, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2718, Loss: 0.1629 Train: 0.9359, Val: 0.7083, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2719, Loss: 0.1732 Train: 0.9375, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2720, Loss: 0.1685 Train: 0.9448, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2721, Loss: 0.1844 Train: 0.9440, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2722, Loss: 0.1582 Train: 0.9456, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2723, Loss: 0.1864 Train: 0.9456, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2724, Loss: 0.1647 Train: 0.9416, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2725, Loss: 0.1607 Train: 0.9399, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2726, Loss: 0.1759 Train: 0.9416, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2727, Loss: 0.1569 Train: 0.9481, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2728, Loss: 0.1671 Train: 0.9481, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2729, Loss: 0.1633 Train: 0.9472, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2730, Loss: 0.1821 Train: 0.9497, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2731, Loss: 0.1703 Train: 0.9505, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2732, Loss: 0.1562 Train: 0.9497, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2733, Loss: 0.1670 Train: 0.9537, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2734, Loss: 0.1617 Train: 0.9529, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2735, Loss: 0.1395 Train: 0.9440, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2736, Loss: 0.1634 Train: 0.9481, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2737, Loss: 0.1836 Train: 0.9521, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2738, Loss: 0.1545 Train: 0.9489, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2739, Loss: 0.1713 Train: 0.9456, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2740, Loss: 0.1693 Train: 0.9448, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2741, Loss: 0.1576 Train: 0.9440, Val: 0.7159, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2742, Loss: 0.1803 Train: 0.9416, Val: 0.7045, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2743, Loss: 0.1648 Train: 0.9464, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2744, Loss: 0.1650 Train: 0.9481, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2745, Loss: 0.1535 Train: 0.9489, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2746, Loss: 0.1896 Train: 0.9448, Val: 0.7311, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 2747, Loss: 0.1536 Train: 0.9416, Val: 0.7273, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2748, Loss: 0.1707 Train: 0.9391, Val: 0.7045, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2749, Loss: 0.1573 Train: 0.9448, Val: 0.7008, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2750, Loss: 0.1480 Train: 0.9416, Val: 0.6970, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2751, Loss: 0.1570 Train: 0.9367, Val: 0.6856, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2752, Loss: 0.1718 Train: 0.9399, Val: 0.6856, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2753, Loss: 0.1331 Train: 0.9416, Val: 0.6856, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2754, Loss: 0.1478 Train: 0.9456, Val: 0.6894, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2755, Loss: 0.1616 Train: 0.9489, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2756, Loss: 0.1486 Train: 0.9505, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2757, Loss: 0.1343 Train: 0.9481, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2758, Loss: 0.1768 Train: 0.9448, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2759, Loss: 0.1509 Train: 0.9497, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2760, Loss: 0.1518 Train: 0.9513, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2761, Loss: 0.1359 Train: 0.9456, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2762, Loss: 0.1596 Train: 0.9464, Val: 0.7083, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2763, Loss: 0.1580 Train: 0.9481, Val: 0.7008, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2764, Loss: 0.1633 Train: 0.9497, Val: 0.7008, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2765, Loss: 0.1529 Train: 0.9472, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2766, Loss: 0.1638 Train: 0.9448, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2767, Loss: 0.1433 Train: 0.9416, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2768, Loss: 0.1477 Train: 0.9416, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2769, Loss: 0.1759 Train: 0.9456, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2770, Loss: 0.1493 Train: 0.9456, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2771, Loss: 0.1589 Train: 0.9432, Val: 0.7083, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2772, Loss: 0.1702 Train: 0.9472, Val: 0.7008, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2773, Loss: 0.1866 Train: 0.9481, Val: 0.7083, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2774, Loss: 0.1506 Train: 0.9489, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2775, Loss: 0.1521 Train: 0.9513, Val: 0.7045, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2776, Loss: 0.1438 Train: 0.9472, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2777, Loss: 0.1553 Train: 0.9432, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2778, Loss: 0.1515 Train: 0.9440, Val: 0.6932, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2779, Loss: 0.1707 Train: 0.9464, Val: 0.6932, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2780, Loss: 0.1575 Train: 0.9521, Val: 0.7008, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2781, Loss: 0.1451 Train: 0.9554, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2782, Loss: 0.1443 Train: 0.9554, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2783, Loss: 0.1562 Train: 0.9554, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2784, Loss: 0.1524 Train: 0.9578, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2785, Loss: 0.1508 Train: 0.9594, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2786, Loss: 0.1483 Train: 0.9610, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2787, Loss: 0.1436 Train: 0.9594, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2788, Loss: 0.1496 Train: 0.9586, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2789, Loss: 0.1415 Train: 0.9562, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2790, Loss: 0.1545 Train: 0.9513, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2791, Loss: 0.1599 Train: 0.9464, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2792, Loss: 0.1642 Train: 0.9497, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2793, Loss: 0.1607 Train: 0.9529, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2794, Loss: 0.1577 Train: 0.9545, Val: 0.7159, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2795, Loss: 0.1526 Train: 0.9586, Val: 0.7311, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 2796, Loss: 0.1648 Train: 0.9554, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2797, Loss: 0.1488 Train: 0.9529, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2798, Loss: 0.1501 Train: 0.9472, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2799, Loss: 0.1637 Train: 0.9424, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2800, Loss: 0.1656 Train: 0.9448, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2801, Loss: 0.1526 Train: 0.9407, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2802, Loss: 0.1572 Train: 0.9489, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2803, Loss: 0.1579 Train: 0.9529, Val: 0.7045, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2804, Loss: 0.1516 Train: 0.9586, Val: 0.6970, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2805, Loss: 0.1701 Train: 0.9529, Val: 0.7083, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2806, Loss: 0.1639 Train: 0.9513, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2807, Loss: 0.1756 Train: 0.9513, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2808, Loss: 0.2283 Train: 0.9545, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2809, Loss: 0.1378 Train: 0.9489, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2810, Loss: 0.1652 Train: 0.9456, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2811, Loss: 0.1625 Train: 0.9472, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2812, Loss: 0.1588 Train: 0.9489, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2813, Loss: 0.1646 Train: 0.9472, Val: 0.7386, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2814, Loss: 0.1797 Train: 0.9432, Val: 0.7121, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2815, Loss: 0.1621 Train: 0.9424, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2816, Loss: 0.1676 Train: 0.9424, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2817, Loss: 0.1705 Train: 0.9448, Val: 0.7197, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2818, Loss: 0.1668 Train: 0.9529, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2819, Loss: 0.1437 Train: 0.9594, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2820, Loss: 0.1807 Train: 0.9570, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2821, Loss: 0.1626 Train: 0.9537, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2822, Loss: 0.1527 Train: 0.9472, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2823, Loss: 0.1381 Train: 0.9432, Val: 0.7083, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2824, Loss: 0.1446 Train: 0.9424, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2825, Loss: 0.1541 Train: 0.9448, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2826, Loss: 0.1558 Train: 0.9521, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2827, Loss: 0.1472 Train: 0.9554, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2828, Loss: 0.1610 Train: 0.9562, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2829, Loss: 0.1340 Train: 0.9537, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2830, Loss: 0.1469 Train: 0.9554, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2831, Loss: 0.1592 Train: 0.9537, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2832, Loss: 0.1519 Train: 0.9537, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2833, Loss: 0.1747 Train: 0.9554, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2834, Loss: 0.1294 Train: 0.9570, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2835, Loss: 0.1649 Train: 0.9578, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2836, Loss: 0.1732 Train: 0.9594, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2837, Loss: 0.1576 Train: 0.9554, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2838, Loss: 0.1631 Train: 0.9407, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2839, Loss: 0.1527 Train: 0.9424, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2840, Loss: 0.1434 Train: 0.9440, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2841, Loss: 0.1592 Train: 0.9464, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2842, Loss: 0.1537 Train: 0.9513, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2843, Loss: 0.1366 Train: 0.9554, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2844, Loss: 0.1452 Train: 0.9594, Val: 0.7008, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2845, Loss: 0.1525 Train: 0.9586, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2846, Loss: 0.1515 Train: 0.9529, Val: 0.7121, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2847, Loss: 0.1534 Train: 0.9521, Val: 0.7083, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2848, Loss: 0.1581 Train: 0.9521, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2849, Loss: 0.1576 Train: 0.9521, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2850, Loss: 0.1514 Train: 0.9497, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2851, Loss: 0.1699 Train: 0.9521, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2852, Loss: 0.1383 Train: 0.9529, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2853, Loss: 0.1516 Train: 0.9570, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2854, Loss: 0.1494 Train: 0.9545, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2855, Loss: 0.1611 Train: 0.9489, Val: 0.6856, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2856, Loss: 0.1516 Train: 0.9456, Val: 0.6856, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2857, Loss: 0.1768 Train: 0.9505, Val: 0.6970, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2858, Loss: 0.1487 Train: 0.9472, Val: 0.6970, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2859, Loss: 0.1453 Train: 0.9440, Val: 0.6894, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2860, Loss: 0.1581 Train: 0.9464, Val: 0.7008, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2861, Loss: 0.1816 Train: 0.9529, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2862, Loss: 0.1547 Train: 0.9578, Val: 0.7235, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 2863, Loss: 0.1531 Train: 0.9562, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2864, Loss: 0.1538 Train: 0.9578, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2865, Loss: 0.1566 Train: 0.9594, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2866, Loss: 0.1659 Train: 0.9554, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2867, Loss: 0.1377 Train: 0.9505, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2868, Loss: 0.1473 Train: 0.9464, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2869, Loss: 0.1470 Train: 0.9464, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2870, Loss: 0.1433 Train: 0.9448, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2871, Loss: 0.1508 Train: 0.9464, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2872, Loss: 0.1365 Train: 0.9497, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2873, Loss: 0.1222 Train: 0.9489, Val: 0.7576, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2874, Loss: 0.1530 Train: 0.9497, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2875, Loss: 0.1592 Train: 0.9489, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2876, Loss: 0.1495 Train: 0.9505, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2877, Loss: 0.1526 Train: 0.9529, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2878, Loss: 0.1310 Train: 0.9497, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2879, Loss: 0.1503 Train: 0.9481, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2880, Loss: 0.1341 Train: 0.9448, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2881, Loss: 0.1432 Train: 0.9456, Val: 0.7045, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2882, Loss: 0.1476 Train: 0.9416, Val: 0.7083, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2883, Loss: 0.1488 Train: 0.9489, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2884, Loss: 0.1392 Train: 0.9513, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2885, Loss: 0.1394 Train: 0.9513, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2886, Loss: 0.1598 Train: 0.9602, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2887, Loss: 0.1585 Train: 0.9586, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2888, Loss: 0.1540 Train: 0.9545, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2889, Loss: 0.1390 Train: 0.9497, Val: 0.7121, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2890, Loss: 0.1601 Train: 0.9497, Val: 0.7008, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2891, Loss: 0.1453 Train: 0.9497, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2892, Loss: 0.1497 Train: 0.9529, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2893, Loss: 0.1355 Train: 0.9545, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2894, Loss: 0.1410 Train: 0.9529, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2895, Loss: 0.1582 Train: 0.9497, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2896, Loss: 0.1366 Train: 0.9497, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2897, Loss: 0.1411 Train: 0.9513, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2898, Loss: 0.1317 Train: 0.9545, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2899, Loss: 0.1537 Train: 0.9529, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2900, Loss: 0.1185 Train: 0.9562, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2901, Loss: 0.1457 Train: 0.9562, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2902, Loss: 0.1586 Train: 0.9513, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2903, Loss: 0.1481 Train: 0.9505, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2904, Loss: 0.1450 Train: 0.9537, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2905, Loss: 0.1438 Train: 0.9586, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2906, Loss: 0.1342 Train: 0.9578, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2907, Loss: 0.1475 Train: 0.9578, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2908, Loss: 0.1416 Train: 0.9545, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2909, Loss: 0.1572 Train: 0.9505, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2910, Loss: 0.1372 Train: 0.9602, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2911, Loss: 0.1340 Train: 0.9619, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2912, Loss: 0.1471 Train: 0.9586, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2913, Loss: 0.1390 Train: 0.9594, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2914, Loss: 0.1373 Train: 0.9554, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2915, Loss: 0.1296 Train: 0.9497, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2916, Loss: 0.1225 Train: 0.9481, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2917, Loss: 0.1673 Train: 0.9456, Val: 0.7197, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2918, Loss: 0.1322 Train: 0.9448, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2919, Loss: 0.1597 Train: 0.9472, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2920, Loss: 0.1280 Train: 0.9586, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2921, Loss: 0.1399 Train: 0.9537, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2922, Loss: 0.1438 Train: 0.9529, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2923, Loss: 0.1546 Train: 0.9537, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2924, Loss: 0.1411 Train: 0.9505, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2925, Loss: 0.1267 Train: 0.9472, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2926, Loss: 0.1628 Train: 0.9529, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2927, Loss: 0.1319 Train: 0.9610, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2928, Loss: 0.1430 Train: 0.9627, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2929, Loss: 0.1515 Train: 0.9554, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2930, Loss: 0.1285 Train: 0.9424, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2931, Loss: 0.1380 Train: 0.9440, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2932, Loss: 0.1514 Train: 0.9537, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2933, Loss: 0.1379 Train: 0.9562, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2934, Loss: 0.1319 Train: 0.9586, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2935, Loss: 0.1381 Train: 0.9578, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2936, Loss: 0.1437 Train: 0.9594, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2937, Loss: 0.1541 Train: 0.9627, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2938, Loss: 0.1414 Train: 0.9602, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2939, Loss: 0.1316 Train: 0.9554, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2940, Loss: 0.1562 Train: 0.9489, Val: 0.7273, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2941, Loss: 0.1251 Train: 0.9424, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2942, Loss: 0.1262 Train: 0.9464, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2943, Loss: 0.1322 Train: 0.9521, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2944, Loss: 0.1690 Train: 0.9554, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2945, Loss: 0.1747 Train: 0.9545, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2946, Loss: 0.1579 Train: 0.9489, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2947, Loss: 0.1542 Train: 0.9505, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2948, Loss: 0.1788 Train: 0.9456, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2949, Loss: 0.1472 Train: 0.9432, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2950, Loss: 0.1654 Train: 0.9489, Val: 0.6932, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2951, Loss: 0.1653 Train: 0.9529, Val: 0.7159, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2952, Loss: 0.1525 Train: 0.9521, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2953, Loss: 0.1548 Train: 0.9537, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2954, Loss: 0.1645 Train: 0.9521, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2955, Loss: 0.1754 Train: 0.9472, Val: 0.7045, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2956, Loss: 0.1806 Train: 0.9497, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2957, Loss: 0.1538 Train: 0.9497, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2958, Loss: 0.1634 Train: 0.9505, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2959, Loss: 0.1553 Train: 0.9513, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2960, Loss: 0.1618 Train: 0.9521, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2961, Loss: 0.1481 Train: 0.9554, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2962, Loss: 0.1573 Train: 0.9513, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2963, Loss: 0.1657 Train: 0.9562, Val: 0.7386, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2964, Loss: 0.1612 Train: 0.9570, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2965, Loss: 0.1683 Train: 0.9521, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2966, Loss: 0.1762 Train: 0.9456, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 2967, Loss: 0.1489 Train: 0.9497, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 2968, Loss: 0.1402 Train: 0.9489, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2969, Loss: 0.1447 Train: 0.9489, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2970, Loss: 0.1537 Train: 0.9497, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2971, Loss: 0.1678 Train: 0.9489, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2972, Loss: 0.1629 Train: 0.9432, Val: 0.6970, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2973, Loss: 0.1624 Train: 0.9481, Val: 0.6970, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2974, Loss: 0.1325 Train: 0.9513, Val: 0.7121, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2975, Loss: 0.1394 Train: 0.9545, Val: 0.7197, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 2976, Loss: 0.1491 Train: 0.9562, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 2977, Loss: 0.1547 Train: 0.9562, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2978, Loss: 0.1492 Train: 0.9424, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2979, Loss: 0.1731 Train: 0.9391, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2980, Loss: 0.1852 Train: 0.9456, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 2981, Loss: 0.1803 Train: 0.9472, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 2982, Loss: 0.1650 Train: 0.9513, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2983, Loss: 0.1421 Train: 0.9545, Val: 0.7045, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 2984, Loss: 0.1666 Train: 0.9545, Val: 0.7045, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2985, Loss: 0.1617 Train: 0.9497, Val: 0.7008, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2986, Loss: 0.1786 Train: 0.9489, Val: 0.7235, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2987, Loss: 0.1555 Train: 0.9432, Val: 0.7121, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 2988, Loss: 0.1463 Train: 0.9424, Val: 0.7159, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 2989, Loss: 0.1741 Train: 0.9351, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2990, Loss: 0.1763 Train: 0.9416, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2991, Loss: 0.1443 Train: 0.9489, Val: 0.7614, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2992, Loss: 0.1408 Train: 0.9554, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 2993, Loss: 0.1505 Train: 0.9521, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 2994, Loss: 0.1572 Train: 0.9481, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 2995, Loss: 0.1455 Train: 0.9432, Val: 0.7235, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 2996, Loss: 0.1769 Train: 0.9424, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 2997, Loss: 0.1658 Train: 0.9464, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 2998, Loss: 0.1583 Train: 0.9464, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 2999, Loss: 0.1564 Train: 0.9432, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3000, Loss: 0.1519 Train: 0.9375, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3001, Loss: 0.1492 Train: 0.9399, Val: 0.7045, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3002, Loss: 0.1617 Train: 0.9464, Val: 0.7083, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3003, Loss: 0.1531 Train: 0.9513, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3004, Loss: 0.1770 Train: 0.9521, Val: 0.6932, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3005, Loss: 0.1674 Train: 0.9529, Val: 0.6970, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3006, Loss: 0.1801 Train: 0.9545, Val: 0.7008, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3007, Loss: 0.1724 Train: 0.9537, Val: 0.7008, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3008, Loss: 0.1853 Train: 0.9537, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3009, Loss: 0.1562 Train: 0.9472, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3010, Loss: 0.1758 Train: 0.9464, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3011, Loss: 0.1989 Train: 0.9497, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3012, Loss: 0.1583 Train: 0.9562, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3013, Loss: 0.1559 Train: 0.9529, Val: 0.7424, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3014, Loss: 0.1549 Train: 0.9497, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3015, Loss: 0.1535 Train: 0.9497, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3016, Loss: 0.1600 Train: 0.9489, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3017, Loss: 0.1617 Train: 0.9472, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3018, Loss: 0.1638 Train: 0.9481, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3019, Loss: 0.1682 Train: 0.9472, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3020, Loss: 0.1412 Train: 0.9545, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3021, Loss: 0.1367 Train: 0.9505, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3022, Loss: 0.1600 Train: 0.9562, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3023, Loss: 0.1419 Train: 0.9602, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3024, Loss: 0.1595 Train: 0.9570, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3025, Loss: 0.1461 Train: 0.9537, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3026, Loss: 0.1268 Train: 0.9489, Val: 0.7197, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 3027, Loss: 0.1691 Train: 0.9513, Val: 0.7235, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 3028, Loss: 0.1321 Train: 0.9497, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3029, Loss: 0.1684 Train: 0.9521, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3030, Loss: 0.1418 Train: 0.9513, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3031, Loss: 0.1470 Train: 0.9513, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3032, Loss: 0.1493 Train: 0.9554, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3033, Loss: 0.1656 Train: 0.9554, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3034, Loss: 0.1361 Train: 0.9529, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3035, Loss: 0.1484 Train: 0.9456, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3036, Loss: 0.1517 Train: 0.9456, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3037, Loss: 0.1648 Train: 0.9497, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3038, Loss: 0.1579 Train: 0.9570, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3039, Loss: 0.1494 Train: 0.9635, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3040, Loss: 0.1347 Train: 0.9594, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3041, Loss: 0.1606 Train: 0.9586, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3042, Loss: 0.1637 Train: 0.9570, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3043, Loss: 0.1435 Train: 0.9594, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3044, Loss: 0.1614 Train: 0.9521, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3045, Loss: 0.1444 Train: 0.9545, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3046, Loss: 0.1206 Train: 0.9529, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3047, Loss: 0.1442 Train: 0.9497, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3048, Loss: 0.1336 Train: 0.9554, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3049, Loss: 0.1386 Train: 0.9578, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3050, Loss: 0.1570 Train: 0.9570, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3051, Loss: 0.1276 Train: 0.9562, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3052, Loss: 0.1459 Train: 0.9448, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3053, Loss: 0.1624 Train: 0.9432, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3054, Loss: 0.1710 Train: 0.9521, Val: 0.7311, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3055, Loss: 0.1452 Train: 0.9562, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3056, Loss: 0.1571 Train: 0.9521, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3057, Loss: 0.1355 Train: 0.9464, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3058, Loss: 0.1473 Train: 0.9497, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3059, Loss: 0.1550 Train: 0.9562, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3060, Loss: 0.1472 Train: 0.9554, Val: 0.7273, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3061, Loss: 0.1380 Train: 0.9529, Val: 0.7424, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3062, Loss: 0.1363 Train: 0.9472, Val: 0.7311, Test: 0.8295, Final Test: 0.8182\n",
            "Epoch: 3063, Loss: 0.1424 Train: 0.9464, Val: 0.7311, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3064, Loss: 0.1460 Train: 0.9529, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3065, Loss: 0.1557 Train: 0.9602, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3066, Loss: 0.1451 Train: 0.9562, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3067, Loss: 0.1424 Train: 0.9513, Val: 0.7008, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3068, Loss: 0.1641 Train: 0.9505, Val: 0.6970, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3069, Loss: 0.1307 Train: 0.9521, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3070, Loss: 0.1507 Train: 0.9562, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3071, Loss: 0.1493 Train: 0.9562, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3072, Loss: 0.1431 Train: 0.9570, Val: 0.7424, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3073, Loss: 0.1405 Train: 0.9554, Val: 0.7386, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3074, Loss: 0.1619 Train: 0.9586, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3075, Loss: 0.1454 Train: 0.9570, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3076, Loss: 0.1533 Train: 0.9570, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3077, Loss: 0.1335 Train: 0.9529, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3078, Loss: 0.1425 Train: 0.9472, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3079, Loss: 0.1370 Train: 0.9432, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3080, Loss: 0.1616 Train: 0.9456, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3081, Loss: 0.1518 Train: 0.9432, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3082, Loss: 0.1488 Train: 0.9513, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3083, Loss: 0.1276 Train: 0.9481, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3084, Loss: 0.1508 Train: 0.9440, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3085, Loss: 0.1524 Train: 0.9456, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3086, Loss: 0.1581 Train: 0.9521, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3087, Loss: 0.1614 Train: 0.9529, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3088, Loss: 0.1640 Train: 0.9505, Val: 0.7121, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3089, Loss: 0.1698 Train: 0.9464, Val: 0.7121, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3090, Loss: 0.1587 Train: 0.9432, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3091, Loss: 0.1317 Train: 0.9424, Val: 0.7083, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3092, Loss: 0.1543 Train: 0.9448, Val: 0.7083, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3093, Loss: 0.1484 Train: 0.9513, Val: 0.7121, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3094, Loss: 0.1522 Train: 0.9464, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3095, Loss: 0.1615 Train: 0.9456, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3096, Loss: 0.1394 Train: 0.9472, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3097, Loss: 0.1386 Train: 0.9521, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3098, Loss: 0.1557 Train: 0.9554, Val: 0.7652, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3099, Loss: 0.1272 Train: 0.9521, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3100, Loss: 0.1298 Train: 0.9505, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3101, Loss: 0.1485 Train: 0.9464, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3102, Loss: 0.1363 Train: 0.9481, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3103, Loss: 0.1313 Train: 0.9513, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3104, Loss: 0.1458 Train: 0.9505, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3105, Loss: 0.1517 Train: 0.9505, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3106, Loss: 0.1750 Train: 0.9505, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3107, Loss: 0.1430 Train: 0.9513, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3108, Loss: 0.1587 Train: 0.9537, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3109, Loss: 0.1667 Train: 0.9562, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3110, Loss: 0.1446 Train: 0.9521, Val: 0.7008, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3111, Loss: 0.1407 Train: 0.9472, Val: 0.7008, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3112, Loss: 0.1479 Train: 0.9472, Val: 0.7008, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3113, Loss: 0.1560 Train: 0.9505, Val: 0.7008, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3114, Loss: 0.1744 Train: 0.9497, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3115, Loss: 0.1475 Train: 0.9416, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3116, Loss: 0.1586 Train: 0.9440, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3117, Loss: 0.1445 Train: 0.9464, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3118, Loss: 0.1452 Train: 0.9497, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3119, Loss: 0.1737 Train: 0.9537, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3120, Loss: 0.1807 Train: 0.9554, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3121, Loss: 0.1502 Train: 0.9570, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3122, Loss: 0.1552 Train: 0.9570, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3123, Loss: 0.1426 Train: 0.9554, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3124, Loss: 0.1360 Train: 0.9545, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3125, Loss: 0.1390 Train: 0.9545, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3126, Loss: 0.1480 Train: 0.9521, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3127, Loss: 0.1342 Train: 0.9521, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3128, Loss: 0.1444 Train: 0.9578, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3129, Loss: 0.1502 Train: 0.9489, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3130, Loss: 0.1471 Train: 0.9424, Val: 0.7045, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3131, Loss: 0.1887 Train: 0.9399, Val: 0.6894, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3132, Loss: 0.1844 Train: 0.9391, Val: 0.6894, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3133, Loss: 0.2300 Train: 0.9407, Val: 0.6894, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3134, Loss: 0.2292 Train: 0.9481, Val: 0.7045, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3135, Loss: 0.1636 Train: 0.9391, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3136, Loss: 0.2609 Train: 0.9188, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3137, Loss: 0.2371 Train: 0.9205, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3138, Loss: 0.2752 Train: 0.9221, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3139, Loss: 0.2544 Train: 0.9221, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3140, Loss: 0.2421 Train: 0.9286, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3141, Loss: 0.2295 Train: 0.9261, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3142, Loss: 0.2411 Train: 0.9180, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3143, Loss: 0.2332 Train: 0.9083, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3144, Loss: 0.2515 Train: 0.9083, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3145, Loss: 0.1963 Train: 0.9050, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3146, Loss: 0.2160 Train: 0.9140, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3147, Loss: 0.1947 Train: 0.9237, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3148, Loss: 0.2064 Train: 0.9213, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3149, Loss: 0.1855 Train: 0.9245, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3150, Loss: 0.2003 Train: 0.9278, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3151, Loss: 0.1846 Train: 0.9286, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3152, Loss: 0.1905 Train: 0.9278, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3153, Loss: 0.1936 Train: 0.9302, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3154, Loss: 0.1923 Train: 0.9326, Val: 0.7235, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3155, Loss: 0.1837 Train: 0.9334, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3156, Loss: 0.1575 Train: 0.9367, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3157, Loss: 0.1783 Train: 0.9318, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3158, Loss: 0.1878 Train: 0.9351, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3159, Loss: 0.1675 Train: 0.9367, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3160, Loss: 0.1831 Train: 0.9391, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3161, Loss: 0.1602 Train: 0.9351, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3162, Loss: 0.1619 Train: 0.9326, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3163, Loss: 0.1854 Train: 0.9375, Val: 0.7424, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3164, Loss: 0.1803 Train: 0.9416, Val: 0.7462, Test: 0.8333, Final Test: 0.8182\n",
            "Epoch: 3165, Loss: 0.1574 Train: 0.9448, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3166, Loss: 0.1643 Train: 0.9481, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3167, Loss: 0.1555 Train: 0.9440, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3168, Loss: 0.1695 Train: 0.9456, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3169, Loss: 0.1497 Train: 0.9489, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3170, Loss: 0.1639 Train: 0.9497, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3171, Loss: 0.1643 Train: 0.9464, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3172, Loss: 0.1706 Train: 0.9521, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3173, Loss: 0.1687 Train: 0.9521, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3174, Loss: 0.1654 Train: 0.9529, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3175, Loss: 0.1315 Train: 0.9464, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3176, Loss: 0.1820 Train: 0.9399, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3177, Loss: 0.2437 Train: 0.9367, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3178, Loss: 0.1807 Train: 0.9326, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3179, Loss: 0.1783 Train: 0.9343, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3180, Loss: 0.1771 Train: 0.9343, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3181, Loss: 0.1753 Train: 0.9407, Val: 0.7045, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3182, Loss: 0.1682 Train: 0.9472, Val: 0.7045, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3183, Loss: 0.1627 Train: 0.9497, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3184, Loss: 0.1822 Train: 0.9497, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3185, Loss: 0.1557 Train: 0.9481, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3186, Loss: 0.1669 Train: 0.9456, Val: 0.7008, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3187, Loss: 0.1721 Train: 0.9383, Val: 0.7045, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3188, Loss: 0.1697 Train: 0.9391, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3189, Loss: 0.1822 Train: 0.9416, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3190, Loss: 0.1577 Train: 0.9367, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3191, Loss: 0.1484 Train: 0.9343, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3192, Loss: 0.1721 Train: 0.9351, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3193, Loss: 0.1620 Train: 0.9440, Val: 0.7159, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3194, Loss: 0.1597 Train: 0.9456, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3195, Loss: 0.1713 Train: 0.9424, Val: 0.7197, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3196, Loss: 0.1541 Train: 0.9432, Val: 0.7348, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3197, Loss: 0.1777 Train: 0.9440, Val: 0.7462, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3198, Loss: 0.1441 Train: 0.9424, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3199, Loss: 0.1531 Train: 0.9464, Val: 0.7311, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3200, Loss: 0.1614 Train: 0.9497, Val: 0.7235, Test: 0.8333, Final Test: 0.8182\n",
            "Epoch: 3201, Loss: 0.1575 Train: 0.9554, Val: 0.7500, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3202, Loss: 0.1592 Train: 0.9562, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3203, Loss: 0.1559 Train: 0.9570, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3204, Loss: 0.1500 Train: 0.9545, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3205, Loss: 0.1458 Train: 0.9537, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3206, Loss: 0.1644 Train: 0.9464, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3207, Loss: 0.1546 Train: 0.9497, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3208, Loss: 0.1490 Train: 0.9521, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3209, Loss: 0.1650 Train: 0.9521, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3210, Loss: 0.1434 Train: 0.9554, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3211, Loss: 0.1855 Train: 0.9505, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3212, Loss: 0.1698 Train: 0.9505, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3213, Loss: 0.1648 Train: 0.9521, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3214, Loss: 0.1501 Train: 0.9456, Val: 0.7576, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3215, Loss: 0.1683 Train: 0.9424, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3216, Loss: 0.1658 Train: 0.9391, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3217, Loss: 0.1556 Train: 0.9440, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3218, Loss: 0.1531 Train: 0.9448, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3219, Loss: 0.1413 Train: 0.9432, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3220, Loss: 0.1403 Train: 0.9464, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3221, Loss: 0.1717 Train: 0.9481, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3222, Loss: 0.1580 Train: 0.9432, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3223, Loss: 0.1589 Train: 0.9440, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3224, Loss: 0.1435 Train: 0.9481, Val: 0.7235, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3225, Loss: 0.1516 Train: 0.9489, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3226, Loss: 0.1630 Train: 0.9521, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3227, Loss: 0.1502 Train: 0.9521, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3228, Loss: 0.1376 Train: 0.9562, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3229, Loss: 0.1495 Train: 0.9586, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3230, Loss: 0.1371 Train: 0.9521, Val: 0.7311, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3231, Loss: 0.1487 Train: 0.9489, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3232, Loss: 0.1489 Train: 0.9529, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3233, Loss: 0.1424 Train: 0.9578, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3234, Loss: 0.1339 Train: 0.9513, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3235, Loss: 0.1460 Train: 0.9529, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3236, Loss: 0.1307 Train: 0.9610, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3237, Loss: 0.1341 Train: 0.9594, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3238, Loss: 0.1432 Train: 0.9602, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3239, Loss: 0.1518 Train: 0.9619, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3240, Loss: 0.1482 Train: 0.9659, Val: 0.7462, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3241, Loss: 0.1300 Train: 0.9619, Val: 0.7462, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3242, Loss: 0.1343 Train: 0.9554, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3243, Loss: 0.1440 Train: 0.9489, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3244, Loss: 0.1474 Train: 0.9521, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3245, Loss: 0.1479 Train: 0.9529, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3246, Loss: 0.1328 Train: 0.9554, Val: 0.7311, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3247, Loss: 0.1308 Train: 0.9570, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3248, Loss: 0.1504 Train: 0.9602, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3249, Loss: 0.1374 Train: 0.9602, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3250, Loss: 0.1526 Train: 0.9610, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3251, Loss: 0.1487 Train: 0.9529, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3252, Loss: 0.1435 Train: 0.9545, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3253, Loss: 0.1669 Train: 0.9521, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3254, Loss: 0.1379 Train: 0.9537, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3255, Loss: 0.1457 Train: 0.9562, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3256, Loss: 0.1371 Train: 0.9594, Val: 0.7083, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3257, Loss: 0.1539 Train: 0.9627, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3258, Loss: 0.1402 Train: 0.9610, Val: 0.7348, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3259, Loss: 0.1459 Train: 0.9586, Val: 0.7500, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3260, Loss: 0.1421 Train: 0.9570, Val: 0.7462, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3261, Loss: 0.1434 Train: 0.9472, Val: 0.7386, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3262, Loss: 0.1275 Train: 0.9432, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3263, Loss: 0.1516 Train: 0.9489, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3264, Loss: 0.1316 Train: 0.9529, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3265, Loss: 0.1290 Train: 0.9554, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3266, Loss: 0.1339 Train: 0.9619, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3267, Loss: 0.1245 Train: 0.9627, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3268, Loss: 0.1366 Train: 0.9610, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3269, Loss: 0.1424 Train: 0.9521, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3270, Loss: 0.1306 Train: 0.9545, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3271, Loss: 0.1320 Train: 0.9586, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3272, Loss: 0.1353 Train: 0.9586, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3273, Loss: 0.1435 Train: 0.9570, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3274, Loss: 0.1222 Train: 0.9513, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3275, Loss: 0.1248 Train: 0.9521, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3276, Loss: 0.1313 Train: 0.9521, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3277, Loss: 0.1497 Train: 0.9578, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3278, Loss: 0.1735 Train: 0.9554, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3279, Loss: 0.1455 Train: 0.9594, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3280, Loss: 0.1518 Train: 0.9562, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3281, Loss: 0.1563 Train: 0.9456, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3282, Loss: 0.1446 Train: 0.9432, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3283, Loss: 0.1331 Train: 0.9424, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3284, Loss: 0.1289 Train: 0.9424, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3285, Loss: 0.1453 Train: 0.9432, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3286, Loss: 0.1547 Train: 0.9472, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3287, Loss: 0.1586 Train: 0.9505, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3288, Loss: 0.1463 Train: 0.9505, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3289, Loss: 0.1380 Train: 0.9521, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3290, Loss: 0.1437 Train: 0.9545, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3291, Loss: 0.1415 Train: 0.9562, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3292, Loss: 0.1403 Train: 0.9610, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3293, Loss: 0.1325 Train: 0.9610, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3294, Loss: 0.1246 Train: 0.9635, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3295, Loss: 0.1287 Train: 0.9635, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3296, Loss: 0.1407 Train: 0.9635, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3297, Loss: 0.1277 Train: 0.9619, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3298, Loss: 0.1287 Train: 0.9619, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3299, Loss: 0.1391 Train: 0.9586, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3300, Loss: 0.1235 Train: 0.9594, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3301, Loss: 0.1568 Train: 0.9570, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3302, Loss: 0.1331 Train: 0.9521, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3303, Loss: 0.1386 Train: 0.9521, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3304, Loss: 0.1281 Train: 0.9578, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3305, Loss: 0.1406 Train: 0.9627, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3306, Loss: 0.1165 Train: 0.9643, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3307, Loss: 0.1470 Train: 0.9643, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3308, Loss: 0.1482 Train: 0.9602, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3309, Loss: 0.1250 Train: 0.9586, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 3310, Loss: 0.1481 Train: 0.9521, Val: 0.7348, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 3311, Loss: 0.1403 Train: 0.9497, Val: 0.7235, Test: 0.7538, Final Test: 0.8182\n",
            "Epoch: 3312, Loss: 0.1308 Train: 0.9505, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3313, Loss: 0.1403 Train: 0.9562, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3314, Loss: 0.1348 Train: 0.9602, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3315, Loss: 0.1246 Train: 0.9659, Val: 0.7500, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3316, Loss: 0.1221 Train: 0.9683, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3317, Loss: 0.1293 Train: 0.9683, Val: 0.7500, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3318, Loss: 0.1145 Train: 0.9651, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3319, Loss: 0.1424 Train: 0.9643, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3320, Loss: 0.1270 Train: 0.9651, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3321, Loss: 0.1217 Train: 0.9635, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3322, Loss: 0.1310 Train: 0.9627, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3323, Loss: 0.1202 Train: 0.9610, Val: 0.7689, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3324, Loss: 0.1105 Train: 0.9602, Val: 0.7727, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3325, Loss: 0.1488 Train: 0.9578, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3326, Loss: 0.1226 Train: 0.9554, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3327, Loss: 0.1336 Train: 0.9521, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3328, Loss: 0.1389 Train: 0.9513, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3329, Loss: 0.1413 Train: 0.9537, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3330, Loss: 0.1494 Train: 0.9472, Val: 0.6970, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3331, Loss: 0.1444 Train: 0.9505, Val: 0.7008, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3332, Loss: 0.1392 Train: 0.9578, Val: 0.7121, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3333, Loss: 0.1304 Train: 0.9562, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3334, Loss: 0.1586 Train: 0.9619, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3335, Loss: 0.1534 Train: 0.9602, Val: 0.7197, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3336, Loss: 0.1435 Train: 0.9529, Val: 0.7273, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3337, Loss: 0.1306 Train: 0.9529, Val: 0.7235, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3338, Loss: 0.1441 Train: 0.9513, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3339, Loss: 0.1284 Train: 0.9481, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3340, Loss: 0.1437 Train: 0.9489, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3341, Loss: 0.1447 Train: 0.9545, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3342, Loss: 0.1437 Train: 0.9521, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3343, Loss: 0.1288 Train: 0.9545, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3344, Loss: 0.1473 Train: 0.9570, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3345, Loss: 0.1240 Train: 0.9619, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3346, Loss: 0.1381 Train: 0.9619, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3347, Loss: 0.1383 Train: 0.9651, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3348, Loss: 0.1335 Train: 0.9651, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3349, Loss: 0.1420 Train: 0.9635, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3350, Loss: 0.1290 Train: 0.9554, Val: 0.7386, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3351, Loss: 0.1551 Train: 0.9562, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3352, Loss: 0.1668 Train: 0.9529, Val: 0.7652, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3353, Loss: 0.1412 Train: 0.9521, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3354, Loss: 0.1384 Train: 0.9521, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3355, Loss: 0.1392 Train: 0.9594, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3356, Loss: 0.1530 Train: 0.9627, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3357, Loss: 0.1248 Train: 0.9619, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3358, Loss: 0.1379 Train: 0.9667, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3359, Loss: 0.1422 Train: 0.9651, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3360, Loss: 0.1348 Train: 0.9627, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3361, Loss: 0.1330 Train: 0.9610, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3362, Loss: 0.1408 Train: 0.9610, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3363, Loss: 0.1263 Train: 0.9521, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3364, Loss: 0.1500 Train: 0.9448, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3365, Loss: 0.1627 Train: 0.9513, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3366, Loss: 0.1525 Train: 0.9505, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3367, Loss: 0.1792 Train: 0.9489, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3368, Loss: 0.1605 Train: 0.9586, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3369, Loss: 0.1600 Train: 0.9513, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3370, Loss: 0.1495 Train: 0.9351, Val: 0.7159, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 3371, Loss: 0.1693 Train: 0.9375, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3372, Loss: 0.1767 Train: 0.9570, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3373, Loss: 0.1408 Train: 0.9562, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3374, Loss: 0.1536 Train: 0.9529, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3375, Loss: 0.1374 Train: 0.9521, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3376, Loss: 0.1598 Train: 0.9529, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3377, Loss: 0.1456 Train: 0.9586, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3378, Loss: 0.1360 Train: 0.9562, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3379, Loss: 0.1287 Train: 0.9578, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3380, Loss: 0.1382 Train: 0.9627, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3381, Loss: 0.1588 Train: 0.9619, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3382, Loss: 0.1537 Train: 0.9627, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3383, Loss: 0.1205 Train: 0.9643, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3384, Loss: 0.1399 Train: 0.9635, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3385, Loss: 0.1453 Train: 0.9635, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3386, Loss: 0.1309 Train: 0.9651, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3387, Loss: 0.1180 Train: 0.9627, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3388, Loss: 0.1505 Train: 0.9627, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3389, Loss: 0.1451 Train: 0.9610, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3390, Loss: 0.1318 Train: 0.9594, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3391, Loss: 0.1421 Train: 0.9570, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3392, Loss: 0.1159 Train: 0.9545, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3393, Loss: 0.1354 Train: 0.9529, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3394, Loss: 0.1304 Train: 0.9537, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3395, Loss: 0.1399 Train: 0.9578, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3396, Loss: 0.1327 Train: 0.9537, Val: 0.7386, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3397, Loss: 0.1313 Train: 0.9472, Val: 0.7273, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3398, Loss: 0.1452 Train: 0.9481, Val: 0.7386, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3399, Loss: 0.1486 Train: 0.9578, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3400, Loss: 0.1313 Train: 0.9578, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3401, Loss: 0.1688 Train: 0.9578, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3402, Loss: 0.1630 Train: 0.9627, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3403, Loss: 0.1183 Train: 0.9610, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3404, Loss: 0.1189 Train: 0.9562, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3405, Loss: 0.1446 Train: 0.9537, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3406, Loss: 0.1289 Train: 0.9562, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3407, Loss: 0.1357 Train: 0.9562, Val: 0.7083, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3408, Loss: 0.1328 Train: 0.9578, Val: 0.7235, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3409, Loss: 0.1435 Train: 0.9594, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3410, Loss: 0.1334 Train: 0.9635, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3411, Loss: 0.1446 Train: 0.9554, Val: 0.7538, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3412, Loss: 0.1561 Train: 0.9545, Val: 0.7462, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3413, Loss: 0.1621 Train: 0.9505, Val: 0.7348, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3414, Loss: 0.1249 Train: 0.9464, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3415, Loss: 0.1293 Train: 0.9472, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3416, Loss: 0.1343 Train: 0.9505, Val: 0.7348, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3417, Loss: 0.1305 Train: 0.9554, Val: 0.7424, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3418, Loss: 0.1336 Train: 0.9513, Val: 0.7424, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3419, Loss: 0.1294 Train: 0.9513, Val: 0.7386, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3420, Loss: 0.1747 Train: 0.9537, Val: 0.7424, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3421, Loss: 0.1285 Train: 0.9513, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3422, Loss: 0.1286 Train: 0.9489, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3423, Loss: 0.1416 Train: 0.9529, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3424, Loss: 0.1549 Train: 0.9562, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3425, Loss: 0.1206 Train: 0.9513, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3426, Loss: 0.1329 Train: 0.9481, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3427, Loss: 0.1536 Train: 0.9594, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3428, Loss: 0.1583 Train: 0.9635, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3429, Loss: 0.1311 Train: 0.9578, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3430, Loss: 0.1537 Train: 0.9627, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3431, Loss: 0.1465 Train: 0.9635, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3432, Loss: 0.1445 Train: 0.9610, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3433, Loss: 0.1457 Train: 0.9505, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3434, Loss: 0.1406 Train: 0.9456, Val: 0.7235, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3435, Loss: 0.1408 Train: 0.9464, Val: 0.7235, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3436, Loss: 0.1594 Train: 0.9472, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3437, Loss: 0.1647 Train: 0.9578, Val: 0.7197, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3438, Loss: 0.1397 Train: 0.9594, Val: 0.7235, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3439, Loss: 0.1504 Train: 0.9586, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3440, Loss: 0.1268 Train: 0.9578, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3441, Loss: 0.1329 Train: 0.9578, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3442, Loss: 0.1513 Train: 0.9529, Val: 0.7348, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3443, Loss: 0.1306 Train: 0.9570, Val: 0.7386, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3444, Loss: 0.1298 Train: 0.9537, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3445, Loss: 0.1547 Train: 0.9497, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3446, Loss: 0.1429 Train: 0.9456, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3447, Loss: 0.1274 Train: 0.9513, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3448, Loss: 0.1349 Train: 0.9562, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3449, Loss: 0.1516 Train: 0.9602, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3450, Loss: 0.1469 Train: 0.9643, Val: 0.7424, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3451, Loss: 0.1376 Train: 0.9594, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3452, Loss: 0.1449 Train: 0.9570, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3453, Loss: 0.1335 Train: 0.9610, Val: 0.7500, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3454, Loss: 0.1365 Train: 0.9635, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3455, Loss: 0.1201 Train: 0.9619, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3456, Loss: 0.1472 Train: 0.9610, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3457, Loss: 0.1202 Train: 0.9554, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3458, Loss: 0.1306 Train: 0.9513, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3459, Loss: 0.1329 Train: 0.9545, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3460, Loss: 0.2110 Train: 0.9513, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3461, Loss: 0.1625 Train: 0.9537, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3462, Loss: 0.1589 Train: 0.9464, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3463, Loss: 0.1752 Train: 0.9529, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3464, Loss: 0.1647 Train: 0.9521, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3465, Loss: 0.1808 Train: 0.9407, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3466, Loss: 0.2156 Train: 0.9448, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3467, Loss: 0.1536 Train: 0.9529, Val: 0.7045, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3468, Loss: 0.1739 Train: 0.9456, Val: 0.6932, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3469, Loss: 0.1537 Train: 0.9464, Val: 0.6780, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3470, Loss: 0.1654 Train: 0.9545, Val: 0.6932, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3471, Loss: 0.1652 Train: 0.9570, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3472, Loss: 0.1738 Train: 0.9537, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3473, Loss: 0.1581 Train: 0.9505, Val: 0.7045, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3474, Loss: 0.1648 Train: 0.9562, Val: 0.7045, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3475, Loss: 0.1456 Train: 0.9521, Val: 0.7121, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3476, Loss: 0.1499 Train: 0.9489, Val: 0.7083, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3477, Loss: 0.1420 Train: 0.9489, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3478, Loss: 0.1605 Train: 0.9481, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3479, Loss: 0.1340 Train: 0.9513, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3480, Loss: 0.1505 Train: 0.9472, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3481, Loss: 0.1520 Train: 0.9521, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3482, Loss: 0.1559 Train: 0.9521, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3483, Loss: 0.1422 Train: 0.9513, Val: 0.7121, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3484, Loss: 0.1444 Train: 0.9497, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3485, Loss: 0.1508 Train: 0.9537, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3486, Loss: 0.1456 Train: 0.9513, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3487, Loss: 0.1446 Train: 0.9472, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3488, Loss: 0.1497 Train: 0.9464, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3489, Loss: 0.1502 Train: 0.9505, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3490, Loss: 0.1578 Train: 0.9594, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3491, Loss: 0.1434 Train: 0.9602, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3492, Loss: 0.1236 Train: 0.9570, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3493, Loss: 0.1383 Train: 0.9578, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3494, Loss: 0.1596 Train: 0.9578, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3495, Loss: 0.1698 Train: 0.9562, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3496, Loss: 0.1237 Train: 0.9529, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3497, Loss: 0.1341 Train: 0.9472, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3498, Loss: 0.1474 Train: 0.9513, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3499, Loss: 0.1282 Train: 0.9610, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3500, Loss: 0.1276 Train: 0.9643, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3501, Loss: 0.1339 Train: 0.9675, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3502, Loss: 0.1506 Train: 0.9675, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3503, Loss: 0.1283 Train: 0.9675, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3504, Loss: 0.1214 Train: 0.9667, Val: 0.7235, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3505, Loss: 0.1362 Train: 0.9643, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3506, Loss: 0.1208 Train: 0.9619, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3507, Loss: 0.1216 Train: 0.9635, Val: 0.7538, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3508, Loss: 0.1351 Train: 0.9643, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3509, Loss: 0.1387 Train: 0.9667, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3510, Loss: 0.1360 Train: 0.9651, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3511, Loss: 0.1330 Train: 0.9651, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3512, Loss: 0.1094 Train: 0.9619, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3513, Loss: 0.1378 Train: 0.9619, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3514, Loss: 0.1187 Train: 0.9586, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3515, Loss: 0.1343 Train: 0.9594, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3516, Loss: 0.1112 Train: 0.9610, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3517, Loss: 0.1246 Train: 0.9594, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3518, Loss: 0.1460 Train: 0.9619, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3519, Loss: 0.1257 Train: 0.9627, Val: 0.7008, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3520, Loss: 0.1095 Train: 0.9578, Val: 0.6970, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3521, Loss: 0.1187 Train: 0.9578, Val: 0.6970, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3522, Loss: 0.1526 Train: 0.9610, Val: 0.7121, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3523, Loss: 0.1270 Train: 0.9537, Val: 0.7045, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3524, Loss: 0.1462 Train: 0.9521, Val: 0.7008, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3525, Loss: 0.1412 Train: 0.9594, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3526, Loss: 0.1371 Train: 0.9619, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3527, Loss: 0.1260 Train: 0.9619, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3528, Loss: 0.1355 Train: 0.9594, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3529, Loss: 0.1410 Train: 0.9619, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3530, Loss: 0.1472 Train: 0.9635, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3531, Loss: 0.1343 Train: 0.9513, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3532, Loss: 0.1342 Train: 0.9554, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3533, Loss: 0.1603 Train: 0.9562, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3534, Loss: 0.1299 Train: 0.9537, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3535, Loss: 0.1256 Train: 0.9513, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3536, Loss: 0.1358 Train: 0.9481, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3537, Loss: 0.1394 Train: 0.9424, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3538, Loss: 0.1464 Train: 0.9505, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3539, Loss: 0.1412 Train: 0.9521, Val: 0.7500, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3540, Loss: 0.1801 Train: 0.9545, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3541, Loss: 0.1471 Train: 0.9545, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3542, Loss: 0.1567 Train: 0.9570, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3543, Loss: 0.1716 Train: 0.9619, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3544, Loss: 0.1381 Train: 0.9586, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3545, Loss: 0.1359 Train: 0.9570, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3546, Loss: 0.1329 Train: 0.9570, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3547, Loss: 0.1322 Train: 0.9578, Val: 0.7311, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 3548, Loss: 0.1415 Train: 0.9627, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 3549, Loss: 0.1362 Train: 0.9602, Val: 0.7424, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 3550, Loss: 0.1266 Train: 0.9578, Val: 0.7273, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 3551, Loss: 0.1380 Train: 0.9570, Val: 0.7273, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 3552, Loss: 0.1457 Train: 0.9627, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3553, Loss: 0.1236 Train: 0.9570, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3554, Loss: 0.1197 Train: 0.9578, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3555, Loss: 0.1288 Train: 0.9554, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3556, Loss: 0.1288 Train: 0.9529, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3557, Loss: 0.1328 Train: 0.9529, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3558, Loss: 0.1592 Train: 0.9545, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3559, Loss: 0.1587 Train: 0.9570, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3560, Loss: 0.1550 Train: 0.9529, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3561, Loss: 0.1535 Train: 0.9578, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3562, Loss: 0.1533 Train: 0.9505, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3563, Loss: 0.1419 Train: 0.9497, Val: 0.7083, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3564, Loss: 0.1473 Train: 0.9529, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3565, Loss: 0.1227 Train: 0.9497, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3566, Loss: 0.1446 Train: 0.9521, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3567, Loss: 0.1366 Train: 0.9545, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3568, Loss: 0.1530 Train: 0.9562, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3569, Loss: 0.1307 Train: 0.9619, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3570, Loss: 0.1254 Train: 0.9627, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3571, Loss: 0.1453 Train: 0.9627, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3572, Loss: 0.1394 Train: 0.9635, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3573, Loss: 0.1497 Train: 0.9635, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3574, Loss: 0.1489 Train: 0.9635, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3575, Loss: 0.1313 Train: 0.9643, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3576, Loss: 0.1312 Train: 0.9619, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3577, Loss: 0.1238 Train: 0.9545, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3578, Loss: 0.1224 Train: 0.9562, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3579, Loss: 0.1360 Train: 0.9578, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3580, Loss: 0.1328 Train: 0.9570, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3581, Loss: 0.1524 Train: 0.9570, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3582, Loss: 0.1305 Train: 0.9578, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3583, Loss: 0.1349 Train: 0.9586, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3584, Loss: 0.1264 Train: 0.9610, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3585, Loss: 0.1151 Train: 0.9505, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3586, Loss: 0.1357 Train: 0.9545, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3587, Loss: 0.1208 Train: 0.9497, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3588, Loss: 0.1227 Train: 0.9537, Val: 0.7462, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3589, Loss: 0.1189 Train: 0.9578, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3590, Loss: 0.1317 Train: 0.9554, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3591, Loss: 0.1273 Train: 0.9505, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3592, Loss: 0.2196 Train: 0.9513, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3593, Loss: 0.1486 Train: 0.9472, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3594, Loss: 0.1441 Train: 0.9448, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3595, Loss: 0.1495 Train: 0.9391, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3596, Loss: 0.1507 Train: 0.9294, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3597, Loss: 0.1876 Train: 0.9302, Val: 0.7008, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3598, Loss: 0.1929 Train: 0.9351, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3599, Loss: 0.1590 Train: 0.9334, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3600, Loss: 0.2313 Train: 0.9343, Val: 0.7045, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3601, Loss: 0.1688 Train: 0.9213, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3602, Loss: 0.1803 Train: 0.9343, Val: 0.7159, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3603, Loss: 0.1472 Train: 0.9318, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3604, Loss: 0.1588 Train: 0.9261, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3605, Loss: 0.1759 Train: 0.9351, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3606, Loss: 0.1757 Train: 0.9456, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3607, Loss: 0.1699 Train: 0.9481, Val: 0.7008, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3608, Loss: 0.1457 Train: 0.9489, Val: 0.6894, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3609, Loss: 0.1489 Train: 0.9399, Val: 0.7045, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3610, Loss: 0.1676 Train: 0.9407, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3611, Loss: 0.1695 Train: 0.9407, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3612, Loss: 0.1533 Train: 0.9399, Val: 0.7045, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3613, Loss: 0.1530 Train: 0.9472, Val: 0.7083, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3614, Loss: 0.1274 Train: 0.9481, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3615, Loss: 0.1485 Train: 0.9497, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3616, Loss: 0.1450 Train: 0.9545, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3617, Loss: 0.1456 Train: 0.9602, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3618, Loss: 0.1259 Train: 0.9602, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3619, Loss: 0.1454 Train: 0.9602, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3620, Loss: 0.1310 Train: 0.9627, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3621, Loss: 0.1301 Train: 0.9667, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3622, Loss: 0.1375 Train: 0.9683, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3623, Loss: 0.1257 Train: 0.9692, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3624, Loss: 0.1441 Train: 0.9586, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3625, Loss: 0.1434 Train: 0.9578, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3626, Loss: 0.1557 Train: 0.9635, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3627, Loss: 0.1503 Train: 0.9667, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3628, Loss: 0.1125 Train: 0.9635, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3629, Loss: 0.1272 Train: 0.9635, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3630, Loss: 0.1233 Train: 0.9610, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3631, Loss: 0.1380 Train: 0.9570, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3632, Loss: 0.1150 Train: 0.9602, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3633, Loss: 0.1234 Train: 0.9627, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3634, Loss: 0.1200 Train: 0.9659, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3635, Loss: 0.1159 Train: 0.9659, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3636, Loss: 0.1421 Train: 0.9651, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3637, Loss: 0.1267 Train: 0.9635, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3638, Loss: 0.1372 Train: 0.9627, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3639, Loss: 0.1234 Train: 0.9659, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3640, Loss: 0.1148 Train: 0.9667, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3641, Loss: 0.1196 Train: 0.9635, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3642, Loss: 0.1290 Train: 0.9659, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3643, Loss: 0.1239 Train: 0.9659, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3644, Loss: 0.1083 Train: 0.9659, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3645, Loss: 0.1391 Train: 0.9651, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3646, Loss: 0.1075 Train: 0.9667, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3647, Loss: 0.1298 Train: 0.9667, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3648, Loss: 0.1388 Train: 0.9675, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3649, Loss: 0.1405 Train: 0.9708, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3650, Loss: 0.1257 Train: 0.9700, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3651, Loss: 0.1227 Train: 0.9700, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3652, Loss: 0.1132 Train: 0.9683, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3653, Loss: 0.1204 Train: 0.9683, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3654, Loss: 0.1283 Train: 0.9675, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3655, Loss: 0.1189 Train: 0.9651, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3656, Loss: 0.1164 Train: 0.9659, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3657, Loss: 0.1328 Train: 0.9675, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3658, Loss: 0.1198 Train: 0.9692, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3659, Loss: 0.1378 Train: 0.9724, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3660, Loss: 0.1078 Train: 0.9716, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3661, Loss: 0.1397 Train: 0.9692, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3662, Loss: 0.1286 Train: 0.9708, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3663, Loss: 0.1395 Train: 0.9659, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3664, Loss: 0.1265 Train: 0.9643, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3665, Loss: 0.1518 Train: 0.9651, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3666, Loss: 0.1250 Train: 0.9667, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3667, Loss: 0.1354 Train: 0.9635, Val: 0.7197, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3668, Loss: 0.1493 Train: 0.9594, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3669, Loss: 0.1380 Train: 0.9570, Val: 0.7424, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3670, Loss: 0.1372 Train: 0.9562, Val: 0.7614, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3671, Loss: 0.1359 Train: 0.9545, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3672, Loss: 0.1444 Train: 0.9610, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3673, Loss: 0.1234 Train: 0.9610, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3674, Loss: 0.1308 Train: 0.9619, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3675, Loss: 0.1374 Train: 0.9643, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3676, Loss: 0.1223 Train: 0.9667, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3677, Loss: 0.1301 Train: 0.9659, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3678, Loss: 0.1296 Train: 0.9716, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3679, Loss: 0.1304 Train: 0.9716, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3680, Loss: 0.1291 Train: 0.9700, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3681, Loss: 0.1273 Train: 0.9683, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3682, Loss: 0.1163 Train: 0.9602, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3683, Loss: 0.1444 Train: 0.9554, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3684, Loss: 0.1607 Train: 0.9594, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3685, Loss: 0.1244 Train: 0.9578, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3686, Loss: 0.1426 Train: 0.9570, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3687, Loss: 0.1470 Train: 0.9570, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3688, Loss: 0.1492 Train: 0.9521, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3689, Loss: 0.1454 Train: 0.9456, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3690, Loss: 0.1391 Train: 0.9440, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3691, Loss: 0.1501 Train: 0.9513, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3692, Loss: 0.1391 Train: 0.9586, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3693, Loss: 0.1281 Train: 0.9651, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3694, Loss: 0.1490 Train: 0.9708, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3695, Loss: 0.1385 Train: 0.9708, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3696, Loss: 0.1176 Train: 0.9667, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3697, Loss: 0.1393 Train: 0.9610, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3698, Loss: 0.1351 Train: 0.9578, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3699, Loss: 0.1428 Train: 0.9562, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3700, Loss: 0.1497 Train: 0.9521, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3701, Loss: 0.1328 Train: 0.9440, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3702, Loss: 0.1396 Train: 0.9456, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3703, Loss: 0.1397 Train: 0.9464, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3704, Loss: 0.1662 Train: 0.9537, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3705, Loss: 0.1442 Train: 0.9562, Val: 0.7197, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3706, Loss: 0.1559 Train: 0.9521, Val: 0.7348, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3707, Loss: 0.1524 Train: 0.9594, Val: 0.7576, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3708, Loss: 0.1368 Train: 0.9651, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3709, Loss: 0.1150 Train: 0.9619, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3710, Loss: 0.1352 Train: 0.9529, Val: 0.7197, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3711, Loss: 0.1437 Train: 0.9586, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3712, Loss: 0.1365 Train: 0.9683, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3713, Loss: 0.1470 Train: 0.9619, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3714, Loss: 0.1097 Train: 0.9610, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3715, Loss: 0.1417 Train: 0.9627, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3716, Loss: 0.1411 Train: 0.9651, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3717, Loss: 0.1228 Train: 0.9667, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3718, Loss: 0.1499 Train: 0.9651, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3719, Loss: 0.1411 Train: 0.9619, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3720, Loss: 0.1253 Train: 0.9570, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3721, Loss: 0.1379 Train: 0.9578, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3722, Loss: 0.1451 Train: 0.9570, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3723, Loss: 0.1605 Train: 0.9610, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3724, Loss: 0.1293 Train: 0.9529, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3725, Loss: 0.1730 Train: 0.9554, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3726, Loss: 0.1492 Train: 0.9529, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3727, Loss: 0.1349 Train: 0.9513, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3728, Loss: 0.1408 Train: 0.9586, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3729, Loss: 0.1356 Train: 0.9602, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3730, Loss: 0.1294 Train: 0.9635, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3731, Loss: 0.1176 Train: 0.9602, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3732, Loss: 0.1358 Train: 0.9610, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3733, Loss: 0.1210 Train: 0.9594, Val: 0.7538, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3734, Loss: 0.1252 Train: 0.9602, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3735, Loss: 0.1208 Train: 0.9627, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3736, Loss: 0.1218 Train: 0.9610, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3737, Loss: 0.1464 Train: 0.9619, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3738, Loss: 0.1217 Train: 0.9602, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3739, Loss: 0.1203 Train: 0.9586, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3740, Loss: 0.1558 Train: 0.9554, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3741, Loss: 0.1337 Train: 0.9545, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3742, Loss: 0.1180 Train: 0.9586, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3743, Loss: 0.1258 Train: 0.9586, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3744, Loss: 0.1244 Train: 0.9529, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3745, Loss: 0.1376 Train: 0.9578, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3746, Loss: 0.1184 Train: 0.9586, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3747, Loss: 0.1728 Train: 0.9619, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3748, Loss: 0.1545 Train: 0.9594, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3749, Loss: 0.1324 Train: 0.9578, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3750, Loss: 0.1649 Train: 0.9545, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3751, Loss: 0.1613 Train: 0.9529, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3752, Loss: 0.1747 Train: 0.9505, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3753, Loss: 0.1384 Train: 0.9505, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3754, Loss: 0.1521 Train: 0.9513, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3755, Loss: 0.1548 Train: 0.9529, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3756, Loss: 0.1552 Train: 0.9521, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3757, Loss: 0.1905 Train: 0.9456, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3758, Loss: 0.1770 Train: 0.9416, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3759, Loss: 0.1503 Train: 0.9424, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3760, Loss: 0.1547 Train: 0.9375, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3761, Loss: 0.1446 Train: 0.9383, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3762, Loss: 0.1691 Train: 0.9391, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3763, Loss: 0.1693 Train: 0.9456, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3764, Loss: 0.1704 Train: 0.9529, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3765, Loss: 0.1284 Train: 0.9594, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3766, Loss: 0.1392 Train: 0.9627, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3767, Loss: 0.1350 Train: 0.9619, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3768, Loss: 0.1374 Train: 0.9562, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3769, Loss: 0.1450 Train: 0.9562, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3770, Loss: 0.1532 Train: 0.9545, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3771, Loss: 0.1542 Train: 0.9513, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3772, Loss: 0.1512 Train: 0.9497, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3773, Loss: 0.1495 Train: 0.9545, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3774, Loss: 0.1316 Train: 0.9562, Val: 0.7197, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3775, Loss: 0.1481 Train: 0.9545, Val: 0.7197, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 3776, Loss: 0.1545 Train: 0.9570, Val: 0.7197, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 3777, Loss: 0.1231 Train: 0.9610, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3778, Loss: 0.1426 Train: 0.9619, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3779, Loss: 0.1476 Train: 0.9619, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3780, Loss: 0.1318 Train: 0.9586, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3781, Loss: 0.1197 Train: 0.9578, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3782, Loss: 0.1172 Train: 0.9570, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3783, Loss: 0.1298 Train: 0.9529, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3784, Loss: 0.1214 Train: 0.9570, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3785, Loss: 0.1139 Train: 0.9529, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3786, Loss: 0.1169 Train: 0.9521, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3787, Loss: 0.1080 Train: 0.9472, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3788, Loss: 0.1208 Train: 0.9505, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3789, Loss: 0.1150 Train: 0.9578, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3790, Loss: 0.1311 Train: 0.9586, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3791, Loss: 0.1092 Train: 0.9578, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3792, Loss: 0.1342 Train: 0.9627, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3793, Loss: 0.1460 Train: 0.9619, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3794, Loss: 0.1353 Train: 0.9651, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3795, Loss: 0.1260 Train: 0.9635, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3796, Loss: 0.1408 Train: 0.9643, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3797, Loss: 0.1365 Train: 0.9602, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3798, Loss: 0.1378 Train: 0.9554, Val: 0.7197, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3799, Loss: 0.1385 Train: 0.9505, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3800, Loss: 0.1334 Train: 0.9545, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3801, Loss: 0.1198 Train: 0.9586, Val: 0.7462, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3802, Loss: 0.1351 Train: 0.9619, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3803, Loss: 0.1269 Train: 0.9627, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3804, Loss: 0.1352 Train: 0.9610, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3805, Loss: 0.1315 Train: 0.9635, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3806, Loss: 0.1189 Train: 0.9675, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3807, Loss: 0.1223 Train: 0.9627, Val: 0.7652, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3808, Loss: 0.1276 Train: 0.9692, Val: 0.7614, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3809, Loss: 0.1276 Train: 0.9683, Val: 0.7538, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3810, Loss: 0.1227 Train: 0.9651, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3811, Loss: 0.1428 Train: 0.9619, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3812, Loss: 0.1077 Train: 0.9610, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3813, Loss: 0.1438 Train: 0.9643, Val: 0.7462, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3814, Loss: 0.1093 Train: 0.9643, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3815, Loss: 0.1146 Train: 0.9578, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3816, Loss: 0.1397 Train: 0.9529, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3817, Loss: 0.1306 Train: 0.9513, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3818, Loss: 0.1275 Train: 0.9537, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3819, Loss: 0.1359 Train: 0.9537, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3820, Loss: 0.1317 Train: 0.9586, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3821, Loss: 0.1389 Train: 0.9659, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3822, Loss: 0.1135 Train: 0.9667, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3823, Loss: 0.1053 Train: 0.9651, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3824, Loss: 0.1376 Train: 0.9643, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3825, Loss: 0.1105 Train: 0.9659, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3826, Loss: 0.1350 Train: 0.9635, Val: 0.7083, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3827, Loss: 0.1231 Train: 0.9602, Val: 0.6932, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3828, Loss: 0.1249 Train: 0.9554, Val: 0.6856, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3829, Loss: 0.1114 Train: 0.9537, Val: 0.6780, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3830, Loss: 0.1164 Train: 0.9537, Val: 0.6856, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3831, Loss: 0.1172 Train: 0.9537, Val: 0.7083, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3832, Loss: 0.1349 Train: 0.9602, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3833, Loss: 0.1211 Train: 0.9610, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3834, Loss: 0.1033 Train: 0.9610, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3835, Loss: 0.1300 Train: 0.9602, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3836, Loss: 0.1161 Train: 0.9643, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3837, Loss: 0.1336 Train: 0.9651, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3838, Loss: 0.1192 Train: 0.9659, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3839, Loss: 0.1119 Train: 0.9627, Val: 0.7008, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3840, Loss: 0.1352 Train: 0.9545, Val: 0.6932, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3841, Loss: 0.1083 Train: 0.9537, Val: 0.6970, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3842, Loss: 0.1371 Train: 0.9562, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3843, Loss: 0.1234 Train: 0.9627, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3844, Loss: 0.1410 Train: 0.9586, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3845, Loss: 0.1073 Train: 0.9586, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3846, Loss: 0.1354 Train: 0.9578, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3847, Loss: 0.1513 Train: 0.9586, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3848, Loss: 0.1320 Train: 0.9619, Val: 0.7159, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3849, Loss: 0.1323 Train: 0.9578, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3850, Loss: 0.1158 Train: 0.9472, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3851, Loss: 0.1460 Train: 0.9497, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3852, Loss: 0.1368 Train: 0.9497, Val: 0.7045, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3853, Loss: 0.1443 Train: 0.9505, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3854, Loss: 0.1485 Train: 0.9578, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3855, Loss: 0.1332 Train: 0.9602, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3856, Loss: 0.1633 Train: 0.9667, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3857, Loss: 0.1520 Train: 0.9659, Val: 0.7689, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3858, Loss: 0.1275 Train: 0.9635, Val: 0.7727, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3859, Loss: 0.1302 Train: 0.9610, Val: 0.7765, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3860, Loss: 0.1225 Train: 0.9562, Val: 0.7803, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3861, Loss: 0.1157 Train: 0.9529, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3862, Loss: 0.1259 Train: 0.9570, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3863, Loss: 0.1351 Train: 0.9610, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3864, Loss: 0.1458 Train: 0.9594, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3865, Loss: 0.1165 Train: 0.9594, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3866, Loss: 0.1205 Train: 0.9610, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3867, Loss: 0.1600 Train: 0.9610, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3868, Loss: 0.1369 Train: 0.9708, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3869, Loss: 0.1324 Train: 0.9700, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3870, Loss: 0.1248 Train: 0.9683, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3871, Loss: 0.1387 Train: 0.9667, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3872, Loss: 0.1268 Train: 0.9675, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3873, Loss: 0.1244 Train: 0.9692, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3874, Loss: 0.1327 Train: 0.9667, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3875, Loss: 0.1319 Train: 0.9700, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3876, Loss: 0.1437 Train: 0.9675, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3877, Loss: 0.1185 Train: 0.9675, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3878, Loss: 0.1273 Train: 0.9651, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3879, Loss: 0.1224 Train: 0.9594, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3880, Loss: 0.1213 Train: 0.9602, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3881, Loss: 0.1301 Train: 0.9602, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3882, Loss: 0.1397 Train: 0.9602, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3883, Loss: 0.1176 Train: 0.9570, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3884, Loss: 0.1266 Train: 0.9619, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3885, Loss: 0.1272 Train: 0.9643, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3886, Loss: 0.1333 Train: 0.9627, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3887, Loss: 0.1294 Train: 0.9659, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3888, Loss: 0.1559 Train: 0.9659, Val: 0.7652, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3889, Loss: 0.1495 Train: 0.9610, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3890, Loss: 0.1406 Train: 0.9578, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3891, Loss: 0.1668 Train: 0.9513, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3892, Loss: 0.1766 Train: 0.9432, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3893, Loss: 0.1520 Train: 0.9391, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3894, Loss: 0.1835 Train: 0.9294, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3895, Loss: 0.2012 Train: 0.9375, Val: 0.7045, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3896, Loss: 0.1655 Train: 0.9432, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3897, Loss: 0.1618 Train: 0.9481, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3898, Loss: 0.1548 Train: 0.9505, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3899, Loss: 0.1731 Train: 0.9489, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3900, Loss: 0.1544 Train: 0.9497, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3901, Loss: 0.1670 Train: 0.9521, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3902, Loss: 0.1921 Train: 0.9562, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3903, Loss: 0.1456 Train: 0.9602, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3904, Loss: 0.1326 Train: 0.9545, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3905, Loss: 0.1463 Train: 0.9537, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3906, Loss: 0.1419 Train: 0.9570, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3907, Loss: 0.1291 Train: 0.9554, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3908, Loss: 0.1413 Train: 0.9505, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3909, Loss: 0.1522 Train: 0.9440, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3910, Loss: 0.1466 Train: 0.9464, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3911, Loss: 0.1389 Train: 0.9537, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3912, Loss: 0.1426 Train: 0.9594, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3913, Loss: 0.1582 Train: 0.9619, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3914, Loss: 0.1207 Train: 0.9619, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3915, Loss: 0.1208 Train: 0.9594, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3916, Loss: 0.1367 Train: 0.9578, Val: 0.7045, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3917, Loss: 0.1198 Train: 0.9537, Val: 0.7121, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3918, Loss: 0.1300 Train: 0.9562, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3919, Loss: 0.1332 Train: 0.9586, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3920, Loss: 0.1412 Train: 0.9643, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3921, Loss: 0.1341 Train: 0.9643, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3922, Loss: 0.1377 Train: 0.9659, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3923, Loss: 0.1251 Train: 0.9627, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3924, Loss: 0.1257 Train: 0.9627, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3925, Loss: 0.1253 Train: 0.9619, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3926, Loss: 0.1049 Train: 0.9586, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3927, Loss: 0.1168 Train: 0.9586, Val: 0.6970, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3928, Loss: 0.1351 Train: 0.9562, Val: 0.7008, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3929, Loss: 0.1197 Train: 0.9570, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3930, Loss: 0.1012 Train: 0.9562, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3931, Loss: 0.1151 Train: 0.9537, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3932, Loss: 0.1237 Train: 0.9545, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3933, Loss: 0.1330 Train: 0.9529, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3934, Loss: 0.1142 Train: 0.9554, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3935, Loss: 0.1244 Train: 0.9578, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3936, Loss: 0.1346 Train: 0.9545, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3937, Loss: 0.1120 Train: 0.9562, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3938, Loss: 0.1324 Train: 0.9578, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3939, Loss: 0.1247 Train: 0.9594, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3940, Loss: 0.1278 Train: 0.9586, Val: 0.7159, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 3941, Loss: 0.1407 Train: 0.9562, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3942, Loss: 0.1445 Train: 0.9627, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3943, Loss: 0.1274 Train: 0.9675, Val: 0.7500, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 3944, Loss: 0.1194 Train: 0.9675, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3945, Loss: 0.1275 Train: 0.9643, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3946, Loss: 0.1562 Train: 0.9619, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3947, Loss: 0.1284 Train: 0.9594, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3948, Loss: 0.1161 Train: 0.9586, Val: 0.7576, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3949, Loss: 0.1242 Train: 0.9554, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3950, Loss: 0.1387 Train: 0.9554, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3951, Loss: 0.1389 Train: 0.9602, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3952, Loss: 0.1382 Train: 0.9627, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3953, Loss: 0.1242 Train: 0.9619, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3954, Loss: 0.1134 Train: 0.9610, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3955, Loss: 0.1205 Train: 0.9610, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3956, Loss: 0.1420 Train: 0.9602, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3957, Loss: 0.1372 Train: 0.9635, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3958, Loss: 0.1177 Train: 0.9659, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 3959, Loss: 0.1286 Train: 0.9586, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3960, Loss: 0.1086 Train: 0.9570, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3961, Loss: 0.1319 Train: 0.9602, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3962, Loss: 0.1331 Train: 0.9659, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3963, Loss: 0.1085 Train: 0.9627, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3964, Loss: 0.1559 Train: 0.9586, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3965, Loss: 0.1053 Train: 0.9545, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3966, Loss: 0.1284 Train: 0.9529, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 3967, Loss: 0.1174 Train: 0.9562, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3968, Loss: 0.1092 Train: 0.9635, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3969, Loss: 0.1094 Train: 0.9627, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3970, Loss: 0.1245 Train: 0.9683, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3971, Loss: 0.1262 Train: 0.9675, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3972, Loss: 0.1276 Train: 0.9602, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3973, Loss: 0.1161 Train: 0.9619, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3974, Loss: 0.1318 Train: 0.9627, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3975, Loss: 0.1021 Train: 0.9610, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3976, Loss: 0.1234 Train: 0.9586, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3977, Loss: 0.1465 Train: 0.9594, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3978, Loss: 0.1285 Train: 0.9554, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 3979, Loss: 0.1181 Train: 0.9537, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3980, Loss: 0.1130 Train: 0.9554, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3981, Loss: 0.1187 Train: 0.9602, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 3982, Loss: 0.1165 Train: 0.9627, Val: 0.7500, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3983, Loss: 0.1197 Train: 0.9627, Val: 0.7386, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 3984, Loss: 0.1500 Train: 0.9675, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3985, Loss: 0.1128 Train: 0.9659, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3986, Loss: 0.1020 Train: 0.9610, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3987, Loss: 0.1109 Train: 0.9570, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 3988, Loss: 0.1385 Train: 0.9643, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3989, Loss: 0.1160 Train: 0.9643, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 3990, Loss: 0.1323 Train: 0.9659, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 3991, Loss: 0.1187 Train: 0.9740, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 3992, Loss: 0.1105 Train: 0.9700, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 3993, Loss: 0.1065 Train: 0.9667, Val: 0.7500, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 3994, Loss: 0.1033 Train: 0.9675, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3995, Loss: 0.1259 Train: 0.9667, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 3996, Loss: 0.1246 Train: 0.9619, Val: 0.7462, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 3997, Loss: 0.1319 Train: 0.9651, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 3998, Loss: 0.1196 Train: 0.9627, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 3999, Loss: 0.1273 Train: 0.9602, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4000, Loss: 0.1307 Train: 0.9619, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4001, Loss: 0.1143 Train: 0.9594, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4002, Loss: 0.1217 Train: 0.9643, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4003, Loss: 0.1094 Train: 0.9643, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4004, Loss: 0.1114 Train: 0.9610, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4005, Loss: 0.1241 Train: 0.9619, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4006, Loss: 0.1224 Train: 0.9627, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4007, Loss: 0.1246 Train: 0.9602, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4008, Loss: 0.1234 Train: 0.9619, Val: 0.7235, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4009, Loss: 0.1333 Train: 0.9675, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4010, Loss: 0.1029 Train: 0.9716, Val: 0.7576, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4011, Loss: 0.3238 Train: 0.9610, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4012, Loss: 0.1187 Train: 0.9505, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4013, Loss: 0.1917 Train: 0.9513, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4014, Loss: 0.2066 Train: 0.9521, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4015, Loss: 0.1701 Train: 0.9545, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4016, Loss: 0.1654 Train: 0.9497, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4017, Loss: 0.1612 Train: 0.9472, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4018, Loss: 0.1565 Train: 0.9399, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4019, Loss: 0.2125 Train: 0.9367, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4020, Loss: 0.1582 Train: 0.9391, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4021, Loss: 0.1567 Train: 0.9489, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4022, Loss: 0.1371 Train: 0.9562, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4023, Loss: 0.1385 Train: 0.9554, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4024, Loss: 0.1522 Train: 0.9554, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4025, Loss: 0.1607 Train: 0.9578, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4026, Loss: 0.1560 Train: 0.9562, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4027, Loss: 0.1469 Train: 0.9505, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4028, Loss: 0.1419 Train: 0.9497, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4029, Loss: 0.1962 Train: 0.9529, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4030, Loss: 0.1577 Train: 0.9562, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4031, Loss: 0.1504 Train: 0.9594, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4032, Loss: 0.1303 Train: 0.9651, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4033, Loss: 0.1459 Train: 0.9659, Val: 0.7538, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4034, Loss: 0.1273 Train: 0.9651, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4035, Loss: 0.1312 Train: 0.9594, Val: 0.7235, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4036, Loss: 0.1453 Train: 0.9635, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4037, Loss: 0.1397 Train: 0.9651, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4038, Loss: 0.1490 Train: 0.9594, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4039, Loss: 0.1478 Train: 0.9521, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4040, Loss: 0.1700 Train: 0.9529, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4041, Loss: 0.1318 Train: 0.9554, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4042, Loss: 0.1328 Train: 0.9570, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4043, Loss: 0.1251 Train: 0.9586, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4044, Loss: 0.1191 Train: 0.9643, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4045, Loss: 0.1389 Train: 0.9643, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4046, Loss: 0.1277 Train: 0.9586, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4047, Loss: 0.1209 Train: 0.9570, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4048, Loss: 0.1268 Train: 0.9545, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4049, Loss: 0.1625 Train: 0.9570, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4050, Loss: 0.1209 Train: 0.9586, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4051, Loss: 0.1409 Train: 0.9594, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4052, Loss: 0.1182 Train: 0.9602, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4053, Loss: 0.1331 Train: 0.9619, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4054, Loss: 0.1523 Train: 0.9627, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4055, Loss: 0.1785 Train: 0.9594, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4056, Loss: 0.1275 Train: 0.9619, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4057, Loss: 0.1323 Train: 0.9586, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4058, Loss: 0.1282 Train: 0.9602, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4059, Loss: 0.1295 Train: 0.9627, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4060, Loss: 0.1303 Train: 0.9651, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4061, Loss: 0.1241 Train: 0.9619, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4062, Loss: 0.1084 Train: 0.9651, Val: 0.7614, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4063, Loss: 0.1399 Train: 0.9683, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4064, Loss: 0.1168 Train: 0.9675, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4065, Loss: 0.1174 Train: 0.9667, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4066, Loss: 0.1242 Train: 0.9627, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4067, Loss: 0.1149 Train: 0.9635, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4068, Loss: 0.1158 Train: 0.9635, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4069, Loss: 0.1130 Train: 0.9651, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4070, Loss: 0.1140 Train: 0.9667, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4071, Loss: 0.0977 Train: 0.9667, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4072, Loss: 0.1313 Train: 0.9651, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4073, Loss: 0.1334 Train: 0.9675, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4074, Loss: 0.1045 Train: 0.9659, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4075, Loss: 0.1140 Train: 0.9627, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4076, Loss: 0.1123 Train: 0.9602, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4077, Loss: 0.1282 Train: 0.9602, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4078, Loss: 0.1204 Train: 0.9627, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4079, Loss: 0.1118 Train: 0.9659, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4080, Loss: 0.1181 Train: 0.9675, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4081, Loss: 0.1139 Train: 0.9667, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4082, Loss: 0.1229 Train: 0.9643, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4083, Loss: 0.1149 Train: 0.9659, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4084, Loss: 0.1012 Train: 0.9619, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4085, Loss: 0.1073 Train: 0.9586, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4086, Loss: 0.1248 Train: 0.9570, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4087, Loss: 0.1201 Train: 0.9610, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4088, Loss: 0.1416 Train: 0.9635, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4089, Loss: 0.1252 Train: 0.9667, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4090, Loss: 0.1132 Train: 0.9683, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4091, Loss: 0.1163 Train: 0.9675, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4092, Loss: 0.1267 Train: 0.9667, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4093, Loss: 0.1304 Train: 0.9635, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4094, Loss: 0.1202 Train: 0.9643, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4095, Loss: 0.1343 Train: 0.9651, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4096, Loss: 0.1192 Train: 0.9627, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4097, Loss: 0.1266 Train: 0.9586, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4098, Loss: 0.1366 Train: 0.9545, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4099, Loss: 0.1292 Train: 0.9529, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4100, Loss: 0.1519 Train: 0.9505, Val: 0.7045, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4101, Loss: 0.1479 Train: 0.9505, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4102, Loss: 0.1131 Train: 0.9610, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4103, Loss: 0.1122 Train: 0.9667, Val: 0.7576, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4104, Loss: 0.1258 Train: 0.9732, Val: 0.7614, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4105, Loss: 0.1211 Train: 0.9708, Val: 0.7538, Test: 0.8333, Final Test: 0.8182\n",
            "Epoch: 4106, Loss: 0.1328 Train: 0.9708, Val: 0.7386, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 4107, Loss: 0.1101 Train: 0.9667, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4108, Loss: 0.1121 Train: 0.9602, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4109, Loss: 0.1325 Train: 0.9643, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4110, Loss: 0.1254 Train: 0.9627, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4111, Loss: 0.1320 Train: 0.9594, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4112, Loss: 0.1260 Train: 0.9635, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4113, Loss: 0.1078 Train: 0.9683, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4114, Loss: 0.1282 Train: 0.9692, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4115, Loss: 0.1130 Train: 0.9659, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4116, Loss: 0.1250 Train: 0.9619, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4117, Loss: 0.1231 Train: 0.9602, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4118, Loss: 0.1294 Train: 0.9497, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4119, Loss: 0.1158 Train: 0.9489, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4120, Loss: 0.1303 Train: 0.9578, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4121, Loss: 0.1150 Train: 0.9586, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4122, Loss: 0.1299 Train: 0.9627, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4123, Loss: 0.1356 Train: 0.9651, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4124, Loss: 0.1276 Train: 0.9635, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4125, Loss: 0.1106 Train: 0.9627, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4126, Loss: 0.1138 Train: 0.9570, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4127, Loss: 0.1209 Train: 0.9586, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4128, Loss: 0.1058 Train: 0.9586, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4129, Loss: 0.1113 Train: 0.9619, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4130, Loss: 0.1125 Train: 0.9659, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4131, Loss: 0.1529 Train: 0.9700, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4132, Loss: 0.1207 Train: 0.9708, Val: 0.7614, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4133, Loss: 0.1194 Train: 0.9700, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4134, Loss: 0.1195 Train: 0.9692, Val: 0.7614, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4135, Loss: 0.1057 Train: 0.9667, Val: 0.7652, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4136, Loss: 0.1142 Train: 0.9659, Val: 0.7576, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4137, Loss: 0.1149 Train: 0.9651, Val: 0.7462, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4138, Loss: 0.1143 Train: 0.9619, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4139, Loss: 0.1193 Train: 0.9562, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4140, Loss: 0.1267 Train: 0.9497, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4141, Loss: 0.1274 Train: 0.9505, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4142, Loss: 0.1314 Train: 0.9529, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4143, Loss: 0.1175 Train: 0.9586, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4144, Loss: 0.1208 Train: 0.9594, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4145, Loss: 0.1181 Train: 0.9627, Val: 0.7462, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4146, Loss: 0.1271 Train: 0.9586, Val: 0.7462, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4147, Loss: 0.1247 Train: 0.9594, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4148, Loss: 0.1120 Train: 0.9537, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4149, Loss: 0.1281 Train: 0.9537, Val: 0.7121, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4150, Loss: 0.1189 Train: 0.9610, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4151, Loss: 0.1083 Train: 0.9602, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4152, Loss: 0.1070 Train: 0.9610, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4153, Loss: 0.1436 Train: 0.9627, Val: 0.7083, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4154, Loss: 0.1030 Train: 0.9619, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4155, Loss: 0.1377 Train: 0.9627, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4156, Loss: 0.1078 Train: 0.9578, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4157, Loss: 0.1039 Train: 0.9610, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4158, Loss: 0.1166 Train: 0.9619, Val: 0.7576, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4159, Loss: 0.1295 Train: 0.9659, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4160, Loss: 0.1083 Train: 0.9659, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4161, Loss: 0.1164 Train: 0.9675, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4162, Loss: 0.1365 Train: 0.9627, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4163, Loss: 0.1119 Train: 0.9594, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4164, Loss: 0.1242 Train: 0.9562, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4165, Loss: 0.1210 Train: 0.9610, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4166, Loss: 0.1225 Train: 0.9635, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4167, Loss: 0.1121 Train: 0.9627, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4168, Loss: 0.1284 Train: 0.9594, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4169, Loss: 0.1155 Train: 0.9635, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4170, Loss: 0.1287 Train: 0.9635, Val: 0.7008, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4171, Loss: 0.1378 Train: 0.9627, Val: 0.7045, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4172, Loss: 0.1213 Train: 0.9659, Val: 0.7121, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4173, Loss: 0.1317 Train: 0.9667, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4174, Loss: 0.1218 Train: 0.9667, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4175, Loss: 0.1076 Train: 0.9683, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4176, Loss: 0.1127 Train: 0.9643, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4177, Loss: 0.1317 Train: 0.9659, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4178, Loss: 0.1213 Train: 0.9659, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4179, Loss: 0.1130 Train: 0.9683, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4180, Loss: 0.1083 Train: 0.9675, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4181, Loss: 0.1371 Train: 0.9683, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4182, Loss: 0.1143 Train: 0.9716, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4183, Loss: 0.1241 Train: 0.9724, Val: 0.7614, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4184, Loss: 0.1160 Train: 0.9716, Val: 0.7689, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4185, Loss: 0.1138 Train: 0.9716, Val: 0.7614, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4186, Loss: 0.1309 Train: 0.9692, Val: 0.7386, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4187, Loss: 0.1060 Train: 0.9724, Val: 0.7386, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4188, Loss: 0.1027 Train: 0.9708, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4189, Loss: 0.0986 Train: 0.9700, Val: 0.7083, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4190, Loss: 0.1055 Train: 0.9659, Val: 0.7008, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4191, Loss: 0.1109 Train: 0.9643, Val: 0.6970, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4192, Loss: 0.1005 Train: 0.9659, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4193, Loss: 0.1121 Train: 0.9635, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4194, Loss: 0.1428 Train: 0.9610, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4195, Loss: 0.1082 Train: 0.9635, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4196, Loss: 0.1025 Train: 0.9651, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4197, Loss: 0.1171 Train: 0.9692, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4198, Loss: 0.1157 Train: 0.9724, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4199, Loss: 0.1187 Train: 0.9675, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4200, Loss: 0.1147 Train: 0.9651, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4201, Loss: 0.1024 Train: 0.9619, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4202, Loss: 0.1241 Train: 0.9627, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4203, Loss: 0.1005 Train: 0.9602, Val: 0.7083, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4204, Loss: 0.1156 Train: 0.9570, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4205, Loss: 0.1115 Train: 0.9602, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4206, Loss: 0.0979 Train: 0.9610, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4207, Loss: 0.1019 Train: 0.9619, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4208, Loss: 0.0938 Train: 0.9667, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4209, Loss: 0.1039 Train: 0.9700, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4210, Loss: 0.1136 Train: 0.9724, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4211, Loss: 0.1295 Train: 0.9716, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4212, Loss: 0.1041 Train: 0.9667, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4213, Loss: 0.1182 Train: 0.9675, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4214, Loss: 0.1089 Train: 0.9643, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4215, Loss: 0.0995 Train: 0.9610, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4216, Loss: 0.1035 Train: 0.9602, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4217, Loss: 0.1204 Train: 0.9635, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4218, Loss: 0.1042 Train: 0.9683, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4219, Loss: 0.0985 Train: 0.9692, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4220, Loss: 0.0949 Train: 0.9659, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4221, Loss: 0.0998 Train: 0.9651, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4222, Loss: 0.1183 Train: 0.9627, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4223, Loss: 0.1054 Train: 0.9619, Val: 0.7159, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4224, Loss: 0.0958 Train: 0.9643, Val: 0.7008, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4225, Loss: 0.1182 Train: 0.9659, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4226, Loss: 0.1161 Train: 0.9659, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4227, Loss: 0.1097 Train: 0.9635, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4228, Loss: 0.1236 Train: 0.9651, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4229, Loss: 0.1151 Train: 0.9659, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4230, Loss: 0.1401 Train: 0.9627, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4231, Loss: 0.1133 Train: 0.9594, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4232, Loss: 0.1405 Train: 0.9594, Val: 0.7083, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4233, Loss: 0.1160 Train: 0.9627, Val: 0.7045, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4234, Loss: 0.1286 Train: 0.9651, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4235, Loss: 0.1027 Train: 0.9683, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4236, Loss: 0.1023 Train: 0.9708, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4237, Loss: 0.1179 Train: 0.9708, Val: 0.7652, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4238, Loss: 0.1108 Train: 0.9708, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4239, Loss: 0.1270 Train: 0.9675, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4240, Loss: 0.1275 Train: 0.9594, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4241, Loss: 0.1244 Train: 0.9554, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4242, Loss: 0.1145 Train: 0.9586, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4243, Loss: 0.1211 Train: 0.9594, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4244, Loss: 0.1262 Train: 0.9619, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4245, Loss: 0.1225 Train: 0.9619, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4246, Loss: 0.1191 Train: 0.9627, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4247, Loss: 0.1139 Train: 0.9643, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4248, Loss: 0.1312 Train: 0.9610, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4249, Loss: 0.1051 Train: 0.9602, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4250, Loss: 0.1178 Train: 0.9578, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4251, Loss: 0.1411 Train: 0.9659, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4252, Loss: 0.1024 Train: 0.9659, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4253, Loss: 0.1166 Train: 0.9659, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4254, Loss: 0.1058 Train: 0.9667, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4255, Loss: 0.1183 Train: 0.9659, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4256, Loss: 0.1129 Train: 0.9659, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4257, Loss: 0.1025 Train: 0.9659, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4258, Loss: 0.1296 Train: 0.9651, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4259, Loss: 0.0975 Train: 0.9692, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4260, Loss: 0.1083 Train: 0.9675, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4261, Loss: 0.1132 Train: 0.9675, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4262, Loss: 0.1016 Train: 0.9675, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4263, Loss: 0.1069 Train: 0.9675, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4264, Loss: 0.1081 Train: 0.9708, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4265, Loss: 0.1213 Train: 0.9635, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4266, Loss: 0.1210 Train: 0.9643, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4267, Loss: 0.1272 Train: 0.9586, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4268, Loss: 0.1149 Train: 0.9570, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4269, Loss: 0.1102 Train: 0.9578, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4270, Loss: 0.1146 Train: 0.9602, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4271, Loss: 0.1133 Train: 0.9602, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4272, Loss: 0.1188 Train: 0.9545, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4273, Loss: 0.1527 Train: 0.9570, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4274, Loss: 0.1376 Train: 0.9627, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4275, Loss: 0.1195 Train: 0.9594, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4276, Loss: 0.1076 Train: 0.9537, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4277, Loss: 0.1246 Train: 0.9481, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4278, Loss: 0.1185 Train: 0.9529, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4279, Loss: 0.1441 Train: 0.9570, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4280, Loss: 0.1126 Train: 0.9610, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4281, Loss: 0.1262 Train: 0.9643, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4282, Loss: 0.1415 Train: 0.9627, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4283, Loss: 0.1503 Train: 0.9635, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4284, Loss: 0.1335 Train: 0.9643, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4285, Loss: 0.1181 Train: 0.9578, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4286, Loss: 0.1265 Train: 0.9586, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4287, Loss: 0.1103 Train: 0.9586, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4288, Loss: 0.1151 Train: 0.9562, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4289, Loss: 0.1250 Train: 0.9513, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4290, Loss: 0.1297 Train: 0.9554, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4291, Loss: 0.1027 Train: 0.9586, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4292, Loss: 0.1293 Train: 0.9667, Val: 0.7386, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4293, Loss: 0.0964 Train: 0.9700, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4294, Loss: 0.1171 Train: 0.9748, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4295, Loss: 0.1163 Train: 0.9716, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4296, Loss: 0.1285 Train: 0.9716, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4297, Loss: 0.1100 Train: 0.9716, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4298, Loss: 0.1032 Train: 0.9692, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4299, Loss: 0.1171 Train: 0.9667, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4300, Loss: 0.1007 Train: 0.9675, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4301, Loss: 0.1263 Train: 0.9667, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4302, Loss: 0.1089 Train: 0.9675, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4303, Loss: 0.1122 Train: 0.9700, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4304, Loss: 0.1199 Train: 0.9692, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4305, Loss: 0.1357 Train: 0.9675, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4306, Loss: 0.1294 Train: 0.9700, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4307, Loss: 0.1068 Train: 0.9692, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4308, Loss: 0.1228 Train: 0.9683, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4309, Loss: 0.1040 Train: 0.9692, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4310, Loss: 0.1187 Train: 0.9675, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4311, Loss: 0.1197 Train: 0.9675, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4312, Loss: 0.1121 Train: 0.9683, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4313, Loss: 0.1268 Train: 0.9700, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4314, Loss: 0.1694 Train: 0.9716, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4315, Loss: 0.1168 Train: 0.9683, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4316, Loss: 0.1073 Train: 0.9667, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4317, Loss: 0.1311 Train: 0.9675, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4318, Loss: 0.1444 Train: 0.9651, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4319, Loss: 0.1309 Train: 0.9619, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4320, Loss: 0.1373 Train: 0.9570, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4321, Loss: 0.1338 Train: 0.9529, Val: 0.7008, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4322, Loss: 0.1394 Train: 0.9578, Val: 0.7045, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4323, Loss: 0.1317 Train: 0.9659, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4324, Loss: 0.1330 Train: 0.9643, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4325, Loss: 0.1282 Train: 0.9675, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4326, Loss: 0.1418 Train: 0.9659, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4327, Loss: 0.1309 Train: 0.9675, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4328, Loss: 0.1788 Train: 0.9635, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4329, Loss: 0.1241 Train: 0.9497, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4330, Loss: 0.1218 Train: 0.9432, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4331, Loss: 0.1563 Train: 0.9472, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4332, Loss: 0.1435 Train: 0.9562, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4333, Loss: 0.1231 Train: 0.9570, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4334, Loss: 0.1265 Train: 0.9627, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4335, Loss: 0.1488 Train: 0.9627, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4336, Loss: 0.1417 Train: 0.9627, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4337, Loss: 0.1383 Train: 0.9619, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4338, Loss: 0.1139 Train: 0.9586, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4339, Loss: 0.1244 Train: 0.9554, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4340, Loss: 0.1541 Train: 0.9570, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4341, Loss: 0.1410 Train: 0.9594, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4342, Loss: 0.1228 Train: 0.9602, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4343, Loss: 0.1320 Train: 0.9619, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4344, Loss: 0.1149 Train: 0.9675, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4345, Loss: 0.1319 Train: 0.9675, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4346, Loss: 0.1159 Train: 0.9651, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4347, Loss: 0.1397 Train: 0.9659, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4348, Loss: 0.1269 Train: 0.9683, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4349, Loss: 0.1132 Train: 0.9651, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4350, Loss: 0.1214 Train: 0.9627, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4351, Loss: 0.1321 Train: 0.9716, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4352, Loss: 0.1082 Train: 0.9692, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4353, Loss: 0.1078 Train: 0.9683, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4354, Loss: 0.1149 Train: 0.9643, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4355, Loss: 0.1326 Train: 0.9635, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4356, Loss: 0.1109 Train: 0.9594, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4357, Loss: 0.1138 Train: 0.9619, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4358, Loss: 0.1339 Train: 0.9635, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4359, Loss: 0.1186 Train: 0.9667, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4360, Loss: 0.1118 Train: 0.9708, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4361, Loss: 0.1048 Train: 0.9708, Val: 0.7538, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4362, Loss: 0.1148 Train: 0.9748, Val: 0.7652, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4363, Loss: 0.1184 Train: 0.9740, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4364, Loss: 0.1004 Train: 0.9732, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4365, Loss: 0.1140 Train: 0.9724, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4366, Loss: 0.1114 Train: 0.9667, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4367, Loss: 0.1254 Train: 0.9683, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4368, Loss: 0.1079 Train: 0.9708, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4369, Loss: 0.0977 Train: 0.9724, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4370, Loss: 0.1430 Train: 0.9683, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4371, Loss: 0.1252 Train: 0.9667, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4372, Loss: 0.1109 Train: 0.9643, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4373, Loss: 0.1134 Train: 0.9667, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4374, Loss: 0.1156 Train: 0.9659, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4375, Loss: 0.1045 Train: 0.9651, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4376, Loss: 0.1139 Train: 0.9667, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4377, Loss: 0.1031 Train: 0.9651, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4378, Loss: 0.1035 Train: 0.9578, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4379, Loss: 0.1164 Train: 0.9594, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4380, Loss: 0.1095 Train: 0.9610, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4381, Loss: 0.1022 Train: 0.9667, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4382, Loss: 0.1128 Train: 0.9700, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4383, Loss: 0.1048 Train: 0.9740, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4384, Loss: 0.1222 Train: 0.9732, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4385, Loss: 0.1138 Train: 0.9732, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4386, Loss: 0.1021 Train: 0.9667, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4387, Loss: 0.0958 Train: 0.9683, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4388, Loss: 0.1408 Train: 0.9667, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4389, Loss: 0.1063 Train: 0.9635, Val: 0.7311, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4390, Loss: 0.1122 Train: 0.9635, Val: 0.7197, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4391, Loss: 0.1203 Train: 0.9659, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4392, Loss: 0.1009 Train: 0.9683, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4393, Loss: 0.0995 Train: 0.9708, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4394, Loss: 0.0958 Train: 0.9740, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4395, Loss: 0.1111 Train: 0.9700, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4396, Loss: 0.1070 Train: 0.9692, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4397, Loss: 0.1144 Train: 0.9667, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4398, Loss: 0.1069 Train: 0.9659, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4399, Loss: 0.1155 Train: 0.9675, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4400, Loss: 0.0943 Train: 0.9683, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4401, Loss: 0.1083 Train: 0.9635, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4402, Loss: 0.1126 Train: 0.9659, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4403, Loss: 0.1103 Train: 0.9667, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4404, Loss: 0.0925 Train: 0.9619, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4405, Loss: 0.1123 Train: 0.9594, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4406, Loss: 0.0956 Train: 0.9627, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4407, Loss: 0.1286 Train: 0.9643, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4408, Loss: 0.1312 Train: 0.9627, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4409, Loss: 0.1053 Train: 0.9619, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4410, Loss: 0.1281 Train: 0.9602, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4411, Loss: 0.1307 Train: 0.9602, Val: 0.7235, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4412, Loss: 0.1179 Train: 0.9602, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4413, Loss: 0.1199 Train: 0.9481, Val: 0.7045, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4414, Loss: 0.1577 Train: 0.9513, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4415, Loss: 0.1411 Train: 0.9586, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4416, Loss: 0.1440 Train: 0.9594, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4417, Loss: 0.1227 Train: 0.9602, Val: 0.7197, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4418, Loss: 0.1369 Train: 0.9627, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4419, Loss: 0.1394 Train: 0.9627, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4420, Loss: 0.1296 Train: 0.9619, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4421, Loss: 0.1274 Train: 0.9627, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4422, Loss: 0.1297 Train: 0.9667, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4423, Loss: 0.1311 Train: 0.9602, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4424, Loss: 0.1358 Train: 0.9586, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4425, Loss: 0.1361 Train: 0.9562, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4426, Loss: 0.1222 Train: 0.9578, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4427, Loss: 0.1235 Train: 0.9594, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4428, Loss: 0.1416 Train: 0.9610, Val: 0.7121, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4429, Loss: 0.1314 Train: 0.9602, Val: 0.7121, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4430, Loss: 0.1266 Train: 0.9635, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4431, Loss: 0.1271 Train: 0.9610, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4432, Loss: 0.1266 Train: 0.9586, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4433, Loss: 0.1289 Train: 0.9594, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4434, Loss: 0.1309 Train: 0.9578, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4435, Loss: 0.1157 Train: 0.9570, Val: 0.7045, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4436, Loss: 0.1199 Train: 0.9554, Val: 0.6970, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4437, Loss: 0.1347 Train: 0.9562, Val: 0.7008, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4438, Loss: 0.1456 Train: 0.9570, Val: 0.7159, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4439, Loss: 0.1094 Train: 0.9554, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4440, Loss: 0.1235 Train: 0.9562, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4441, Loss: 0.1383 Train: 0.9586, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4442, Loss: 0.1323 Train: 0.9594, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4443, Loss: 0.1155 Train: 0.9619, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4444, Loss: 0.1445 Train: 0.9619, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4445, Loss: 0.1281 Train: 0.9651, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4446, Loss: 0.1089 Train: 0.9659, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4447, Loss: 0.1179 Train: 0.9675, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4448, Loss: 0.1115 Train: 0.9667, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4449, Loss: 0.1115 Train: 0.9692, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4450, Loss: 0.1081 Train: 0.9700, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4451, Loss: 0.1102 Train: 0.9667, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4452, Loss: 0.1366 Train: 0.9651, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4453, Loss: 0.1172 Train: 0.9578, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4454, Loss: 0.1054 Train: 0.9586, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4455, Loss: 0.1057 Train: 0.9586, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4456, Loss: 0.1124 Train: 0.9586, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4457, Loss: 0.1158 Train: 0.9643, Val: 0.7197, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4458, Loss: 0.1070 Train: 0.9651, Val: 0.7083, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4459, Loss: 0.1219 Train: 0.9716, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4460, Loss: 0.1012 Train: 0.9765, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4461, Loss: 0.1134 Train: 0.9716, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4462, Loss: 0.1132 Train: 0.9716, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4463, Loss: 0.1166 Train: 0.9708, Val: 0.7614, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4464, Loss: 0.1316 Train: 0.9683, Val: 0.7652, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4465, Loss: 0.1147 Train: 0.9643, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4466, Loss: 0.1063 Train: 0.9635, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4467, Loss: 0.1104 Train: 0.9683, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4468, Loss: 0.1358 Train: 0.9740, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4469, Loss: 0.1187 Train: 0.9692, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4470, Loss: 0.1231 Train: 0.9708, Val: 0.6932, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4471, Loss: 0.1156 Train: 0.9692, Val: 0.7008, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4472, Loss: 0.1022 Train: 0.9659, Val: 0.6970, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4473, Loss: 0.1113 Train: 0.9594, Val: 0.7008, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4474, Loss: 0.1261 Train: 0.9545, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4475, Loss: 0.1221 Train: 0.9554, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4476, Loss: 0.1231 Train: 0.9627, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4477, Loss: 0.1122 Train: 0.9683, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4478, Loss: 0.1325 Train: 0.9708, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4479, Loss: 0.1166 Train: 0.9708, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4480, Loss: 0.1205 Train: 0.9675, Val: 0.7311, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4481, Loss: 0.1150 Train: 0.9651, Val: 0.7311, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4482, Loss: 0.1135 Train: 0.9594, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4483, Loss: 0.1202 Train: 0.9627, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4484, Loss: 0.1205 Train: 0.9643, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4485, Loss: 0.1408 Train: 0.9619, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4486, Loss: 0.1064 Train: 0.9610, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4487, Loss: 0.1072 Train: 0.9627, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4488, Loss: 0.1263 Train: 0.9610, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4489, Loss: 0.1079 Train: 0.9635, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4490, Loss: 0.1106 Train: 0.9667, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4491, Loss: 0.1251 Train: 0.9708, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4492, Loss: 0.1106 Train: 0.9740, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4493, Loss: 0.1218 Train: 0.9756, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4494, Loss: 0.1108 Train: 0.9773, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4495, Loss: 0.1178 Train: 0.9740, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4496, Loss: 0.1135 Train: 0.9716, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4497, Loss: 0.0981 Train: 0.9692, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4498, Loss: 0.1194 Train: 0.9692, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4499, Loss: 0.1021 Train: 0.9683, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4500, Loss: 0.1134 Train: 0.9683, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4501, Loss: 0.1131 Train: 0.9659, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4502, Loss: 0.1299 Train: 0.9683, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4503, Loss: 0.0994 Train: 0.9716, Val: 0.7576, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4504, Loss: 0.1130 Train: 0.9700, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4505, Loss: 0.1248 Train: 0.9748, Val: 0.7614, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4506, Loss: 0.1208 Train: 0.9740, Val: 0.7689, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4507, Loss: 0.0950 Train: 0.9740, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4508, Loss: 0.1062 Train: 0.9716, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4509, Loss: 0.1074 Train: 0.9667, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4510, Loss: 0.1151 Train: 0.9610, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4511, Loss: 0.1057 Train: 0.9627, Val: 0.7045, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4512, Loss: 0.1238 Train: 0.9619, Val: 0.6932, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4513, Loss: 0.1297 Train: 0.9643, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4514, Loss: 0.1252 Train: 0.9683, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4515, Loss: 0.0980 Train: 0.9732, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4516, Loss: 0.1086 Train: 0.9740, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4517, Loss: 0.1135 Train: 0.9700, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4518, Loss: 0.1240 Train: 0.9659, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4519, Loss: 0.1000 Train: 0.9643, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4520, Loss: 0.1110 Train: 0.9570, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4521, Loss: 0.1062 Train: 0.9578, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4522, Loss: 0.1213 Train: 0.9586, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4523, Loss: 0.1228 Train: 0.9619, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4524, Loss: 0.1169 Train: 0.9627, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4525, Loss: 0.1139 Train: 0.9692, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4526, Loss: 0.1090 Train: 0.9675, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4527, Loss: 0.1100 Train: 0.9667, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4528, Loss: 0.1077 Train: 0.9675, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4529, Loss: 0.1209 Train: 0.9659, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4530, Loss: 0.1087 Train: 0.9619, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4531, Loss: 0.1451 Train: 0.9594, Val: 0.7500, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 4532, Loss: 0.1233 Train: 0.9610, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4533, Loss: 0.1236 Train: 0.9627, Val: 0.7652, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4534, Loss: 0.1223 Train: 0.9627, Val: 0.7803, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4535, Loss: 0.1296 Train: 0.9602, Val: 0.7841, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4536, Loss: 0.1316 Train: 0.9627, Val: 0.7879, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4537, Loss: 0.1232 Train: 0.9627, Val: 0.7879, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4538, Loss: 0.1221 Train: 0.9643, Val: 0.7803, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4539, Loss: 0.1019 Train: 0.9675, Val: 0.7652, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4540, Loss: 0.1325 Train: 0.9740, Val: 0.7652, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4541, Loss: 0.1162 Train: 0.9675, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4542, Loss: 0.1495 Train: 0.9635, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4543, Loss: 0.1269 Train: 0.9659, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4544, Loss: 0.1458 Train: 0.9683, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4545, Loss: 0.1254 Train: 0.9594, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4546, Loss: 0.1233 Train: 0.9545, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4547, Loss: 0.1109 Train: 0.9537, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4548, Loss: 0.1463 Train: 0.9619, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4549, Loss: 0.1235 Train: 0.9643, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4550, Loss: 0.1014 Train: 0.9708, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4551, Loss: 0.1067 Train: 0.9692, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4552, Loss: 0.1199 Train: 0.9675, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4553, Loss: 0.1142 Train: 0.9659, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4554, Loss: 0.1197 Train: 0.9627, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4555, Loss: 0.1108 Train: 0.9619, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4556, Loss: 0.1153 Train: 0.9619, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4557, Loss: 0.1067 Train: 0.9610, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4558, Loss: 0.1182 Train: 0.9659, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4559, Loss: 0.1077 Train: 0.9700, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4560, Loss: 0.1369 Train: 0.9700, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4561, Loss: 0.1096 Train: 0.9683, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4562, Loss: 0.1015 Train: 0.9683, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4563, Loss: 0.1042 Train: 0.9700, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4564, Loss: 0.1038 Train: 0.9724, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4565, Loss: 0.0990 Train: 0.9748, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4566, Loss: 0.1074 Train: 0.9732, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4567, Loss: 0.1053 Train: 0.9716, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4568, Loss: 0.0893 Train: 0.9692, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4569, Loss: 0.1046 Train: 0.9683, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4570, Loss: 0.1002 Train: 0.9700, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4571, Loss: 0.1101 Train: 0.9692, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4572, Loss: 0.0961 Train: 0.9692, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4573, Loss: 0.1152 Train: 0.9683, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4574, Loss: 0.1288 Train: 0.9643, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4575, Loss: 0.1365 Train: 0.9635, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4576, Loss: 0.1233 Train: 0.9692, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4577, Loss: 0.0918 Train: 0.9708, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4578, Loss: 0.1085 Train: 0.9708, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4579, Loss: 0.0996 Train: 0.9700, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4580, Loss: 0.1053 Train: 0.9692, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4581, Loss: 0.1010 Train: 0.9708, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4582, Loss: 0.1104 Train: 0.9748, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4583, Loss: 0.1130 Train: 0.9692, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4584, Loss: 0.1153 Train: 0.9651, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4585, Loss: 0.1223 Train: 0.9570, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4586, Loss: 0.1322 Train: 0.9375, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4587, Loss: 0.1995 Train: 0.9318, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4588, Loss: 0.2501 Train: 0.9367, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4589, Loss: 0.2298 Train: 0.9456, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4590, Loss: 0.1779 Train: 0.9497, Val: 0.7121, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4591, Loss: 0.1782 Train: 0.9481, Val: 0.7159, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4592, Loss: 0.1544 Train: 0.9440, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4593, Loss: 0.1546 Train: 0.9407, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4594, Loss: 0.1996 Train: 0.9456, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4595, Loss: 0.1816 Train: 0.9464, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4596, Loss: 0.1887 Train: 0.9456, Val: 0.7083, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4597, Loss: 0.1565 Train: 0.9432, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4598, Loss: 0.1842 Train: 0.9456, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4599, Loss: 0.1767 Train: 0.9505, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4600, Loss: 0.1505 Train: 0.9529, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4601, Loss: 0.1547 Train: 0.9570, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4602, Loss: 0.1264 Train: 0.9594, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4603, Loss: 0.1362 Train: 0.9602, Val: 0.7576, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4604, Loss: 0.1292 Train: 0.9610, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4605, Loss: 0.1425 Train: 0.9594, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4606, Loss: 0.1227 Train: 0.9570, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4607, Loss: 0.1382 Train: 0.9562, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4608, Loss: 0.1452 Train: 0.9545, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4609, Loss: 0.1404 Train: 0.9570, Val: 0.7689, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4610, Loss: 0.1145 Train: 0.9570, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4611, Loss: 0.1698 Train: 0.9594, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4612, Loss: 0.1412 Train: 0.9602, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4613, Loss: 0.1256 Train: 0.9643, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4614, Loss: 0.1262 Train: 0.9675, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4615, Loss: 0.1491 Train: 0.9700, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4616, Loss: 0.1203 Train: 0.9740, Val: 0.7614, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4617, Loss: 0.1398 Train: 0.9732, Val: 0.7462, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4618, Loss: 0.1299 Train: 0.9724, Val: 0.7538, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4619, Loss: 0.1174 Train: 0.9716, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4620, Loss: 0.1393 Train: 0.9700, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4621, Loss: 0.1384 Train: 0.9700, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4622, Loss: 0.1308 Train: 0.9675, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4623, Loss: 0.1170 Train: 0.9643, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4624, Loss: 0.1282 Train: 0.9643, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4625, Loss: 0.1200 Train: 0.9659, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4626, Loss: 0.1282 Train: 0.9700, Val: 0.7576, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4627, Loss: 0.1264 Train: 0.9700, Val: 0.7614, Test: 0.8333, Final Test: 0.8182\n",
            "Epoch: 4628, Loss: 0.1368 Train: 0.9683, Val: 0.7652, Test: 0.8333, Final Test: 0.8182\n",
            "Epoch: 4629, Loss: 0.1362 Train: 0.9667, Val: 0.7424, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 4630, Loss: 0.1185 Train: 0.9675, Val: 0.7386, Test: 0.8333, Final Test: 0.8182\n",
            "Epoch: 4631, Loss: 0.1244 Train: 0.9667, Val: 0.7576, Test: 0.8333, Final Test: 0.8182\n",
            "Epoch: 4632, Loss: 0.1334 Train: 0.9651, Val: 0.7538, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4633, Loss: 0.1275 Train: 0.9643, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4634, Loss: 0.1190 Train: 0.9619, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4635, Loss: 0.1145 Train: 0.9602, Val: 0.7576, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4636, Loss: 0.1140 Train: 0.9562, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4637, Loss: 0.2255 Train: 0.9586, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4638, Loss: 0.1292 Train: 0.9602, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4639, Loss: 0.1259 Train: 0.9651, Val: 0.7462, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4640, Loss: 0.1299 Train: 0.9651, Val: 0.7424, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4641, Loss: 0.1282 Train: 0.9651, Val: 0.7386, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4642, Loss: 0.1218 Train: 0.9635, Val: 0.7424, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 4643, Loss: 0.1279 Train: 0.9683, Val: 0.7273, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4644, Loss: 0.1200 Train: 0.9651, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4645, Loss: 0.1247 Train: 0.9651, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4646, Loss: 0.1368 Train: 0.9700, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4647, Loss: 0.1075 Train: 0.9700, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4648, Loss: 0.1220 Train: 0.9708, Val: 0.7652, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4649, Loss: 0.0986 Train: 0.9708, Val: 0.7689, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4650, Loss: 0.0927 Train: 0.9724, Val: 0.7841, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4651, Loss: 0.1125 Train: 0.9732, Val: 0.7803, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4652, Loss: 0.1230 Train: 0.9724, Val: 0.7727, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4653, Loss: 0.1172 Train: 0.9708, Val: 0.7841, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4654, Loss: 0.0942 Train: 0.9708, Val: 0.7765, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4655, Loss: 0.1083 Train: 0.9740, Val: 0.7803, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4656, Loss: 0.1156 Train: 0.9732, Val: 0.7765, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4657, Loss: 0.1070 Train: 0.9700, Val: 0.7689, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4658, Loss: 0.0969 Train: 0.9700, Val: 0.7727, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4659, Loss: 0.1226 Train: 0.9740, Val: 0.7727, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4660, Loss: 0.1127 Train: 0.9724, Val: 0.7689, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4661, Loss: 0.1084 Train: 0.9692, Val: 0.7652, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4662, Loss: 0.1137 Train: 0.9667, Val: 0.7765, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4663, Loss: 0.1043 Train: 0.9643, Val: 0.7727, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4664, Loss: 0.1095 Train: 0.9651, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4665, Loss: 0.1181 Train: 0.9643, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4666, Loss: 0.1037 Train: 0.9651, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4667, Loss: 0.1155 Train: 0.9675, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4668, Loss: 0.1044 Train: 0.9692, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4669, Loss: 0.1091 Train: 0.9683, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4670, Loss: 0.1252 Train: 0.9700, Val: 0.7614, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 4671, Loss: 0.1120 Train: 0.9659, Val: 0.7576, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 4672, Loss: 0.1096 Train: 0.9659, Val: 0.7652, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4673, Loss: 0.1362 Train: 0.9708, Val: 0.7689, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4674, Loss: 0.1126 Train: 0.9732, Val: 0.7727, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4675, Loss: 0.0988 Train: 0.9708, Val: 0.7652, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4676, Loss: 0.1131 Train: 0.9708, Val: 0.7727, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4677, Loss: 0.1187 Train: 0.9724, Val: 0.7727, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4678, Loss: 0.1150 Train: 0.9700, Val: 0.7689, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4679, Loss: 0.1335 Train: 0.9659, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4680, Loss: 0.1095 Train: 0.9643, Val: 0.7614, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4681, Loss: 0.1245 Train: 0.9667, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4682, Loss: 0.1066 Train: 0.9716, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4683, Loss: 0.1091 Train: 0.9675, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4684, Loss: 0.1307 Train: 0.9732, Val: 0.7652, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4685, Loss: 0.1013 Train: 0.9692, Val: 0.7689, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4686, Loss: 0.1051 Train: 0.9651, Val: 0.7727, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4687, Loss: 0.1190 Train: 0.9692, Val: 0.7652, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4688, Loss: 0.1168 Train: 0.9643, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4689, Loss: 0.1221 Train: 0.9651, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4690, Loss: 0.1141 Train: 0.9675, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4691, Loss: 0.1143 Train: 0.9700, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4692, Loss: 0.1204 Train: 0.9675, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4693, Loss: 0.0976 Train: 0.9610, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4694, Loss: 0.1209 Train: 0.9627, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4695, Loss: 0.1238 Train: 0.9667, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4696, Loss: 0.1144 Train: 0.9683, Val: 0.7614, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4697, Loss: 0.1172 Train: 0.9675, Val: 0.7614, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4698, Loss: 0.0976 Train: 0.9708, Val: 0.7614, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4699, Loss: 0.1094 Train: 0.9667, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4700, Loss: 0.1444 Train: 0.9659, Val: 0.7576, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4701, Loss: 0.1140 Train: 0.9692, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4702, Loss: 0.0965 Train: 0.9675, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4703, Loss: 0.1302 Train: 0.9675, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4704, Loss: 0.1104 Train: 0.9651, Val: 0.7576, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4705, Loss: 0.1062 Train: 0.9667, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4706, Loss: 0.1060 Train: 0.9692, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4707, Loss: 0.1005 Train: 0.9675, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4708, Loss: 0.1066 Train: 0.9659, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4709, Loss: 0.1092 Train: 0.9692, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4710, Loss: 0.1138 Train: 0.9724, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4711, Loss: 0.1028 Train: 0.9740, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4712, Loss: 0.1055 Train: 0.9740, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4713, Loss: 0.1080 Train: 0.9748, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4714, Loss: 0.1000 Train: 0.9773, Val: 0.7235, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4715, Loss: 0.0930 Train: 0.9740, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4716, Loss: 0.0973 Train: 0.9765, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4717, Loss: 0.1217 Train: 0.9756, Val: 0.7311, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4718, Loss: 0.1025 Train: 0.9756, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4719, Loss: 0.1109 Train: 0.9740, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4720, Loss: 0.1043 Train: 0.9724, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4721, Loss: 0.1165 Train: 0.9692, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4722, Loss: 0.1074 Train: 0.9692, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4723, Loss: 0.1049 Train: 0.9683, Val: 0.7500, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4724, Loss: 0.0960 Train: 0.9692, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4725, Loss: 0.0881 Train: 0.9716, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4726, Loss: 0.0919 Train: 0.9732, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4727, Loss: 0.1145 Train: 0.9765, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4728, Loss: 0.0909 Train: 0.9732, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4729, Loss: 0.1200 Train: 0.9724, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4730, Loss: 0.0855 Train: 0.9724, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4731, Loss: 0.1172 Train: 0.9651, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4732, Loss: 0.1062 Train: 0.9570, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4733, Loss: 0.1134 Train: 0.9602, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4734, Loss: 0.1175 Train: 0.9602, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4735, Loss: 0.1081 Train: 0.9643, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4736, Loss: 0.0991 Train: 0.9659, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4737, Loss: 0.1247 Train: 0.9692, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4738, Loss: 0.1205 Train: 0.9683, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4739, Loss: 0.0993 Train: 0.9692, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4740, Loss: 0.0963 Train: 0.9692, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4741, Loss: 0.0945 Train: 0.9643, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4742, Loss: 0.1029 Train: 0.9643, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4743, Loss: 0.0841 Train: 0.9619, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4744, Loss: 0.0990 Train: 0.9602, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4745, Loss: 0.1091 Train: 0.9635, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4746, Loss: 0.1144 Train: 0.9724, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4747, Loss: 0.0999 Train: 0.9748, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4748, Loss: 0.1047 Train: 0.9724, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4749, Loss: 0.1188 Train: 0.9724, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4750, Loss: 0.1119 Train: 0.9732, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4751, Loss: 0.1011 Train: 0.9675, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4752, Loss: 0.1043 Train: 0.9651, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4753, Loss: 0.1330 Train: 0.9643, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4754, Loss: 0.0964 Train: 0.9667, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4755, Loss: 0.1022 Train: 0.9700, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4756, Loss: 0.0993 Train: 0.9716, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4757, Loss: 0.1080 Train: 0.9692, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4758, Loss: 0.1260 Train: 0.9692, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4759, Loss: 0.1273 Train: 0.9659, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4760, Loss: 0.0935 Train: 0.9627, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4761, Loss: 0.1067 Train: 0.9619, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4762, Loss: 0.1144 Train: 0.9635, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4763, Loss: 0.1060 Train: 0.9692, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4764, Loss: 0.1051 Train: 0.9740, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4765, Loss: 0.1066 Train: 0.9740, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4766, Loss: 0.0963 Train: 0.9740, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4767, Loss: 0.0999 Train: 0.9740, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4768, Loss: 0.1242 Train: 0.9700, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4769, Loss: 0.1122 Train: 0.9724, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4770, Loss: 0.1012 Train: 0.9692, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4771, Loss: 0.1022 Train: 0.9683, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4772, Loss: 0.0921 Train: 0.9700, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4773, Loss: 0.0987 Train: 0.9708, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4774, Loss: 0.0991 Train: 0.9716, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4775, Loss: 0.0864 Train: 0.9724, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4776, Loss: 0.1089 Train: 0.9659, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4777, Loss: 0.0993 Train: 0.9643, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4778, Loss: 0.1255 Train: 0.9683, Val: 0.6970, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4779, Loss: 0.1169 Train: 0.9659, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4780, Loss: 0.1012 Train: 0.9651, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4781, Loss: 0.1386 Train: 0.9683, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4782, Loss: 0.1125 Train: 0.9716, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4783, Loss: 0.1249 Train: 0.9716, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4784, Loss: 0.1447 Train: 0.9610, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4785, Loss: 0.1890 Train: 0.9708, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4786, Loss: 0.1021 Train: 0.9692, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4787, Loss: 0.1139 Train: 0.9586, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4788, Loss: 0.1335 Train: 0.9578, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4789, Loss: 0.1565 Train: 0.9602, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4790, Loss: 0.1283 Train: 0.9578, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4791, Loss: 0.1265 Train: 0.9529, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4792, Loss: 0.1577 Train: 0.9675, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4793, Loss: 0.1186 Train: 0.9643, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4794, Loss: 0.1247 Train: 0.9586, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4795, Loss: 0.1298 Train: 0.9602, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4796, Loss: 0.1226 Train: 0.9627, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4797, Loss: 0.1339 Train: 0.9692, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4798, Loss: 0.1170 Train: 0.9708, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4799, Loss: 0.1211 Train: 0.9692, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4800, Loss: 0.1396 Train: 0.9724, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4801, Loss: 0.1454 Train: 0.9716, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4802, Loss: 0.1171 Train: 0.9708, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4803, Loss: 0.1203 Train: 0.9716, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4804, Loss: 0.1033 Train: 0.9692, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4805, Loss: 0.1110 Train: 0.9700, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4806, Loss: 0.1164 Train: 0.9692, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4807, Loss: 0.1144 Train: 0.9659, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4808, Loss: 0.1415 Train: 0.9683, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4809, Loss: 0.0943 Train: 0.9700, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4810, Loss: 0.1115 Train: 0.9724, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4811, Loss: 0.0877 Train: 0.9732, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4812, Loss: 0.1263 Train: 0.9756, Val: 0.7159, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4813, Loss: 0.1112 Train: 0.9732, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4814, Loss: 0.0972 Train: 0.9740, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4815, Loss: 0.1258 Train: 0.9724, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4816, Loss: 0.1049 Train: 0.9692, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4817, Loss: 0.1200 Train: 0.9659, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4818, Loss: 0.0937 Train: 0.9667, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4819, Loss: 0.1193 Train: 0.9667, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4820, Loss: 0.0929 Train: 0.9643, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4821, Loss: 0.1025 Train: 0.9659, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4822, Loss: 0.1093 Train: 0.9675, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4823, Loss: 0.1110 Train: 0.9675, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4824, Loss: 0.1002 Train: 0.9683, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4825, Loss: 0.1138 Train: 0.9683, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4826, Loss: 0.0929 Train: 0.9724, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4827, Loss: 0.1015 Train: 0.9732, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4828, Loss: 0.1011 Train: 0.9748, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4829, Loss: 0.1013 Train: 0.9756, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4830, Loss: 0.1029 Train: 0.9748, Val: 0.7614, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4831, Loss: 0.0832 Train: 0.9700, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4832, Loss: 0.0863 Train: 0.9700, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4833, Loss: 0.1155 Train: 0.9651, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4834, Loss: 0.1017 Train: 0.9700, Val: 0.7689, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4835, Loss: 0.1043 Train: 0.9692, Val: 0.7689, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4836, Loss: 0.1024 Train: 0.9732, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4837, Loss: 0.0902 Train: 0.9740, Val: 0.7500, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4838, Loss: 0.0934 Train: 0.9740, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4839, Loss: 0.0823 Train: 0.9716, Val: 0.7462, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4840, Loss: 0.0776 Train: 0.9700, Val: 0.7462, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4841, Loss: 0.0859 Train: 0.9675, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4842, Loss: 0.0983 Train: 0.9659, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4843, Loss: 0.0994 Train: 0.9675, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4844, Loss: 0.0988 Train: 0.9667, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4845, Loss: 0.0980 Train: 0.9716, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4846, Loss: 0.0942 Train: 0.9675, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4847, Loss: 0.1015 Train: 0.9700, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4848, Loss: 0.0911 Train: 0.9708, Val: 0.7348, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4849, Loss: 0.1024 Train: 0.9692, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4850, Loss: 0.0803 Train: 0.9675, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4851, Loss: 0.1145 Train: 0.9700, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4852, Loss: 0.0863 Train: 0.9700, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4853, Loss: 0.0963 Train: 0.9732, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4854, Loss: 0.0902 Train: 0.9756, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4855, Loss: 0.1127 Train: 0.9732, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4856, Loss: 0.0965 Train: 0.9724, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4857, Loss: 0.1158 Train: 0.9700, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4858, Loss: 0.1006 Train: 0.9708, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4859, Loss: 0.1023 Train: 0.9692, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4860, Loss: 0.1082 Train: 0.9683, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4861, Loss: 0.0931 Train: 0.9659, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4862, Loss: 0.1201 Train: 0.9675, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4863, Loss: 0.1223 Train: 0.9700, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4864, Loss: 0.0869 Train: 0.9716, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4865, Loss: 0.1066 Train: 0.9708, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4866, Loss: 0.1000 Train: 0.9724, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4867, Loss: 0.1003 Train: 0.9716, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4868, Loss: 0.0978 Train: 0.9724, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4869, Loss: 0.1600 Train: 0.9724, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4870, Loss: 0.0885 Train: 0.9683, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4871, Loss: 0.0998 Train: 0.9683, Val: 0.7386, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 4872, Loss: 0.1343 Train: 0.9667, Val: 0.7273, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 4873, Loss: 0.1015 Train: 0.9627, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4874, Loss: 0.1301 Train: 0.9602, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4875, Loss: 0.1162 Train: 0.9578, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4876, Loss: 0.1085 Train: 0.9578, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4877, Loss: 0.1129 Train: 0.9594, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4878, Loss: 0.1196 Train: 0.9627, Val: 0.7159, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4879, Loss: 0.0964 Train: 0.9675, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4880, Loss: 0.0996 Train: 0.9667, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4881, Loss: 0.1049 Train: 0.9675, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4882, Loss: 0.1098 Train: 0.9716, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4883, Loss: 0.0928 Train: 0.9659, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4884, Loss: 0.1024 Train: 0.9643, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4885, Loss: 0.1324 Train: 0.9610, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4886, Loss: 0.1054 Train: 0.9627, Val: 0.7462, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4887, Loss: 0.1186 Train: 0.9675, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4888, Loss: 0.1328 Train: 0.9683, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4889, Loss: 0.0900 Train: 0.9675, Val: 0.7500, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4890, Loss: 0.1059 Train: 0.9667, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4891, Loss: 0.1119 Train: 0.9675, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4892, Loss: 0.1073 Train: 0.9724, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4893, Loss: 0.0933 Train: 0.9732, Val: 0.7538, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4894, Loss: 0.0934 Train: 0.9724, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4895, Loss: 0.1113 Train: 0.9740, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4896, Loss: 0.1160 Train: 0.9740, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4897, Loss: 0.1038 Train: 0.9692, Val: 0.7652, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4898, Loss: 0.0981 Train: 0.9683, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4899, Loss: 0.1182 Train: 0.9683, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4900, Loss: 0.0867 Train: 0.9635, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4901, Loss: 0.2122 Train: 0.9651, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4902, Loss: 0.1402 Train: 0.9440, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4903, Loss: 0.2172 Train: 0.9472, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4904, Loss: 0.2095 Train: 0.9456, Val: 0.7197, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4905, Loss: 0.2005 Train: 0.9424, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4906, Loss: 0.1861 Train: 0.9343, Val: 0.7576, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4907, Loss: 0.2789 Train: 0.9034, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4908, Loss: 0.3490 Train: 0.8912, Val: 0.7159, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4909, Loss: 0.4108 Train: 0.9010, Val: 0.6894, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4910, Loss: 0.3325 Train: 0.8985, Val: 0.6629, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4911, Loss: 0.2968 Train: 0.8872, Val: 0.6742, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 4912, Loss: 0.3267 Train: 0.8799, Val: 0.6742, Test: 0.7235, Final Test: 0.8182\n",
            "Epoch: 4913, Loss: 0.3640 Train: 0.8742, Val: 0.6894, Test: 0.7121, Final Test: 0.8182\n",
            "Epoch: 4914, Loss: 0.3289 Train: 0.8799, Val: 0.6742, Test: 0.7121, Final Test: 0.8182\n",
            "Epoch: 4915, Loss: 0.2928 Train: 0.8872, Val: 0.6477, Test: 0.7121, Final Test: 0.8182\n",
            "Epoch: 4916, Loss: 0.2521 Train: 0.9026, Val: 0.6667, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 4917, Loss: 0.2377 Train: 0.9131, Val: 0.6894, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 4918, Loss: 0.2626 Train: 0.9156, Val: 0.7235, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 4919, Loss: 0.2538 Train: 0.9253, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4920, Loss: 0.2385 Train: 0.9286, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4921, Loss: 0.2645 Train: 0.9310, Val: 0.7121, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4922, Loss: 0.2180 Train: 0.9326, Val: 0.6894, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4923, Loss: 0.2202 Train: 0.9318, Val: 0.6932, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4924, Loss: 0.1961 Train: 0.9286, Val: 0.6780, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 4925, Loss: 0.2323 Train: 0.9213, Val: 0.6780, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 4926, Loss: 0.2200 Train: 0.9156, Val: 0.6780, Test: 0.7348, Final Test: 0.8182\n",
            "Epoch: 4927, Loss: 0.1878 Train: 0.9131, Val: 0.6856, Test: 0.7348, Final Test: 0.8182\n",
            "Epoch: 4928, Loss: 0.2005 Train: 0.9213, Val: 0.7159, Test: 0.7386, Final Test: 0.8182\n",
            "Epoch: 4929, Loss: 0.2049 Train: 0.9302, Val: 0.7197, Test: 0.7311, Final Test: 0.8182\n",
            "Epoch: 4930, Loss: 0.1913 Train: 0.9464, Val: 0.7008, Test: 0.7462, Final Test: 0.8182\n",
            "Epoch: 4931, Loss: 0.1842 Train: 0.9489, Val: 0.7159, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 4932, Loss: 0.1998 Train: 0.9529, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4933, Loss: 0.1637 Train: 0.9513, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4934, Loss: 0.1807 Train: 0.9529, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4935, Loss: 0.1674 Train: 0.9489, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4936, Loss: 0.1727 Train: 0.9472, Val: 0.7159, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 4937, Loss: 0.1936 Train: 0.9456, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4938, Loss: 0.1574 Train: 0.9464, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4939, Loss: 0.1673 Train: 0.9399, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4940, Loss: 0.1584 Train: 0.9424, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4941, Loss: 0.1679 Train: 0.9497, Val: 0.7386, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4942, Loss: 0.1399 Train: 0.9497, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4943, Loss: 0.1456 Train: 0.9537, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4944, Loss: 0.1449 Train: 0.9594, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4945, Loss: 0.1756 Train: 0.9586, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4946, Loss: 0.1409 Train: 0.9562, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4947, Loss: 0.1763 Train: 0.9578, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4948, Loss: 0.1529 Train: 0.9627, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4949, Loss: 0.1363 Train: 0.9651, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4950, Loss: 0.1464 Train: 0.9602, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4951, Loss: 0.1395 Train: 0.9586, Val: 0.7121, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4952, Loss: 0.1391 Train: 0.9578, Val: 0.7045, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4953, Loss: 0.1331 Train: 0.9545, Val: 0.7083, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4954, Loss: 0.1465 Train: 0.9521, Val: 0.7045, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4955, Loss: 0.1499 Train: 0.9537, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4956, Loss: 0.1207 Train: 0.9570, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4957, Loss: 0.1276 Train: 0.9586, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4958, Loss: 0.1325 Train: 0.9602, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 4959, Loss: 0.1193 Train: 0.9619, Val: 0.7159, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4960, Loss: 0.1145 Train: 0.9643, Val: 0.7121, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 4961, Loss: 0.1275 Train: 0.9643, Val: 0.7159, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4962, Loss: 0.1204 Train: 0.9675, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4963, Loss: 0.1314 Train: 0.9667, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 4964, Loss: 0.1298 Train: 0.9700, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4965, Loss: 0.1286 Train: 0.9627, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4966, Loss: 0.1213 Train: 0.9651, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 4967, Loss: 0.1414 Train: 0.9692, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4968, Loss: 0.1322 Train: 0.9683, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4969, Loss: 0.1091 Train: 0.9675, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4970, Loss: 0.1119 Train: 0.9651, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4971, Loss: 0.1183 Train: 0.9667, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4972, Loss: 0.1328 Train: 0.9675, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4973, Loss: 0.1021 Train: 0.9659, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4974, Loss: 0.1197 Train: 0.9627, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 4975, Loss: 0.1174 Train: 0.9651, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4976, Loss: 0.1086 Train: 0.9643, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4977, Loss: 0.1355 Train: 0.9659, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4978, Loss: 0.1119 Train: 0.9692, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4979, Loss: 0.1314 Train: 0.9748, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4980, Loss: 0.1227 Train: 0.9756, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4981, Loss: 0.1252 Train: 0.9740, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4982, Loss: 0.1009 Train: 0.9716, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 4983, Loss: 0.1357 Train: 0.9724, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4984, Loss: 0.1170 Train: 0.9748, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4985, Loss: 0.1160 Train: 0.9748, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4986, Loss: 0.0994 Train: 0.9724, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4987, Loss: 0.1129 Train: 0.9708, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 4988, Loss: 0.1115 Train: 0.9740, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4989, Loss: 0.1222 Train: 0.9716, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4990, Loss: 0.1190 Train: 0.9700, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 4991, Loss: 0.1122 Train: 0.9716, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 4992, Loss: 0.0989 Train: 0.9732, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 4993, Loss: 0.1089 Train: 0.9765, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 4994, Loss: 0.1041 Train: 0.9740, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4995, Loss: 0.0992 Train: 0.9765, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4996, Loss: 0.1062 Train: 0.9756, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4997, Loss: 0.1092 Train: 0.9716, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 4998, Loss: 0.1273 Train: 0.9683, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 4999, Loss: 0.1030 Train: 0.9651, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5000, Loss: 0.1105 Train: 0.9627, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5001, Loss: 0.1095 Train: 0.9643, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5002, Loss: 0.1030 Train: 0.9675, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5003, Loss: 0.0987 Train: 0.9716, Val: 0.7614, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5004, Loss: 0.0995 Train: 0.9716, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5005, Loss: 0.1089 Train: 0.9716, Val: 0.7576, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5006, Loss: 0.1011 Train: 0.9724, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5007, Loss: 0.1107 Train: 0.9740, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5008, Loss: 0.1191 Train: 0.9708, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5009, Loss: 0.1024 Train: 0.9708, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5010, Loss: 0.1192 Train: 0.9700, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5011, Loss: 0.1193 Train: 0.9692, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5012, Loss: 0.1134 Train: 0.9675, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5013, Loss: 0.1007 Train: 0.9692, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5014, Loss: 0.1096 Train: 0.9692, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5015, Loss: 0.0967 Train: 0.9675, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5016, Loss: 0.1139 Train: 0.9732, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5017, Loss: 0.1071 Train: 0.9708, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5018, Loss: 0.1042 Train: 0.9675, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5019, Loss: 0.1075 Train: 0.9667, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5020, Loss: 0.1131 Train: 0.9667, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5021, Loss: 0.1088 Train: 0.9659, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5022, Loss: 0.1147 Train: 0.9675, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5023, Loss: 0.1007 Train: 0.9708, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5024, Loss: 0.1003 Train: 0.9692, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5025, Loss: 0.1024 Train: 0.9700, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5026, Loss: 0.1204 Train: 0.9724, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5027, Loss: 0.1047 Train: 0.9716, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5028, Loss: 0.1028 Train: 0.9716, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5029, Loss: 0.1099 Train: 0.9716, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5030, Loss: 0.1140 Train: 0.9708, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5031, Loss: 0.1017 Train: 0.9708, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5032, Loss: 0.1037 Train: 0.9716, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5033, Loss: 0.1115 Train: 0.9716, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5034, Loss: 0.0995 Train: 0.9740, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5035, Loss: 0.0971 Train: 0.9765, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5036, Loss: 0.1035 Train: 0.9740, Val: 0.7576, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5037, Loss: 0.1061 Train: 0.9748, Val: 0.7614, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5038, Loss: 0.0970 Train: 0.9773, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5039, Loss: 0.1126 Train: 0.9773, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5040, Loss: 0.1012 Train: 0.9748, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5041, Loss: 0.0876 Train: 0.9748, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5042, Loss: 0.0870 Train: 0.9700, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5043, Loss: 0.1026 Train: 0.9683, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5044, Loss: 0.1060 Train: 0.9708, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5045, Loss: 0.1016 Train: 0.9675, Val: 0.7538, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5046, Loss: 0.1047 Train: 0.9683, Val: 0.7538, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5047, Loss: 0.1309 Train: 0.9683, Val: 0.7614, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5048, Loss: 0.1010 Train: 0.9683, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5049, Loss: 0.0912 Train: 0.9675, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5050, Loss: 0.1200 Train: 0.9659, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 5051, Loss: 0.0997 Train: 0.9700, Val: 0.7273, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5052, Loss: 0.0984 Train: 0.9724, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5053, Loss: 0.1085 Train: 0.9692, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5054, Loss: 0.0941 Train: 0.9700, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5055, Loss: 0.0943 Train: 0.9700, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5056, Loss: 0.1066 Train: 0.9700, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5057, Loss: 0.1202 Train: 0.9651, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5058, Loss: 0.1321 Train: 0.9635, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5059, Loss: 0.1191 Train: 0.9659, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5060, Loss: 0.1061 Train: 0.9659, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5061, Loss: 0.1170 Train: 0.9724, Val: 0.7462, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 5062, Loss: 0.1275 Train: 0.9716, Val: 0.7614, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 5063, Loss: 0.1421 Train: 0.9700, Val: 0.7727, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5064, Loss: 0.1305 Train: 0.9724, Val: 0.7727, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5065, Loss: 0.1069 Train: 0.9692, Val: 0.7614, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5066, Loss: 0.1125 Train: 0.9700, Val: 0.7727, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5067, Loss: 0.1229 Train: 0.9683, Val: 0.7652, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5068, Loss: 0.1204 Train: 0.9708, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5069, Loss: 0.1356 Train: 0.9708, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5070, Loss: 0.1234 Train: 0.9643, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5071, Loss: 0.1137 Train: 0.9667, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5072, Loss: 0.1321 Train: 0.9659, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5073, Loss: 0.1366 Train: 0.9667, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5074, Loss: 0.1152 Train: 0.9683, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5075, Loss: 0.1092 Train: 0.9683, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5076, Loss: 0.1048 Train: 0.9667, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5077, Loss: 0.1166 Train: 0.9683, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5078, Loss: 0.1159 Train: 0.9692, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5079, Loss: 0.1109 Train: 0.9675, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5080, Loss: 0.2263 Train: 0.9610, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5081, Loss: 0.1622 Train: 0.9586, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5082, Loss: 0.1353 Train: 0.9570, Val: 0.7689, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5083, Loss: 0.1779 Train: 0.9505, Val: 0.7614, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5084, Loss: 0.2026 Train: 0.9399, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5085, Loss: 0.2269 Train: 0.9399, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5086, Loss: 0.1902 Train: 0.9383, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5087, Loss: 0.1672 Train: 0.9367, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5088, Loss: 0.1993 Train: 0.9391, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5089, Loss: 0.1732 Train: 0.9416, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5090, Loss: 0.1788 Train: 0.9399, Val: 0.7614, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 5091, Loss: 0.1794 Train: 0.9464, Val: 0.7727, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5092, Loss: 0.1758 Train: 0.9529, Val: 0.7689, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5093, Loss: 0.1731 Train: 0.9416, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5094, Loss: 0.1409 Train: 0.9407, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5095, Loss: 0.1772 Train: 0.9432, Val: 0.7424, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5096, Loss: 0.1700 Train: 0.9456, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5097, Loss: 0.1668 Train: 0.9472, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5098, Loss: 0.1694 Train: 0.9497, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5099, Loss: 0.1517 Train: 0.9537, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5100, Loss: 0.1635 Train: 0.9537, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5101, Loss: 0.1492 Train: 0.9424, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5102, Loss: 0.1523 Train: 0.9383, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5103, Loss: 0.1756 Train: 0.9432, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5104, Loss: 0.1361 Train: 0.9472, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5105, Loss: 0.1266 Train: 0.9513, Val: 0.7273, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5106, Loss: 0.1359 Train: 0.9545, Val: 0.7008, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5107, Loss: 0.1322 Train: 0.9545, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5108, Loss: 0.1204 Train: 0.9537, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5109, Loss: 0.1212 Train: 0.9554, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5110, Loss: 0.1420 Train: 0.9562, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5111, Loss: 0.1456 Train: 0.9610, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5112, Loss: 0.1170 Train: 0.9481, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5113, Loss: 0.1352 Train: 0.9472, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5114, Loss: 0.1333 Train: 0.9456, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5115, Loss: 0.1406 Train: 0.9521, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5116, Loss: 0.1481 Train: 0.9570, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5117, Loss: 0.1292 Train: 0.9627, Val: 0.7727, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5118, Loss: 0.1196 Train: 0.9700, Val: 0.7765, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5119, Loss: 0.1272 Train: 0.9675, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5120, Loss: 0.1121 Train: 0.9659, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5121, Loss: 0.1224 Train: 0.9643, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5122, Loss: 0.1179 Train: 0.9635, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5123, Loss: 0.1333 Train: 0.9594, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5124, Loss: 0.1323 Train: 0.9643, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5125, Loss: 0.1275 Train: 0.9627, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5126, Loss: 0.1296 Train: 0.9619, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5127, Loss: 0.1343 Train: 0.9619, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5128, Loss: 0.1203 Train: 0.9594, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5129, Loss: 0.1078 Train: 0.9619, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5130, Loss: 0.1212 Train: 0.9594, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5131, Loss: 0.1144 Train: 0.9586, Val: 0.7689, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5132, Loss: 0.1328 Train: 0.9610, Val: 0.7652, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5133, Loss: 0.1088 Train: 0.9651, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5134, Loss: 0.0992 Train: 0.9635, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5135, Loss: 0.1010 Train: 0.9619, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5136, Loss: 0.1224 Train: 0.9610, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5137, Loss: 0.1089 Train: 0.9627, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5138, Loss: 0.1362 Train: 0.9619, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5139, Loss: 0.1112 Train: 0.9619, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5140, Loss: 0.1181 Train: 0.9619, Val: 0.7576, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5141, Loss: 0.1209 Train: 0.9659, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5142, Loss: 0.1095 Train: 0.9651, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5143, Loss: 0.1759 Train: 0.9602, Val: 0.7197, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5144, Loss: 0.1593 Train: 0.9521, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5145, Loss: 0.2085 Train: 0.9537, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5146, Loss: 0.1432 Train: 0.9481, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5147, Loss: 0.1683 Train: 0.9416, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5148, Loss: 0.1835 Train: 0.9343, Val: 0.7424, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 5149, Loss: 0.2391 Train: 0.9318, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5150, Loss: 0.1879 Train: 0.9367, Val: 0.7197, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5151, Loss: 0.1866 Train: 0.9391, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5152, Loss: 0.2004 Train: 0.9456, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5153, Loss: 0.1852 Train: 0.9472, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5154, Loss: 0.1693 Train: 0.9505, Val: 0.7273, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5155, Loss: 0.1971 Train: 0.9383, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5156, Loss: 0.1887 Train: 0.9359, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5157, Loss: 0.1670 Train: 0.9334, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5158, Loss: 0.1752 Train: 0.9326, Val: 0.7159, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 5159, Loss: 0.1738 Train: 0.9416, Val: 0.7159, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5160, Loss: 0.1818 Train: 0.9391, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5161, Loss: 0.1562 Train: 0.9334, Val: 0.7159, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5162, Loss: 0.1806 Train: 0.9343, Val: 0.7045, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5163, Loss: 0.1577 Train: 0.9359, Val: 0.7083, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5164, Loss: 0.1569 Train: 0.9351, Val: 0.7008, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5165, Loss: 0.1563 Train: 0.9359, Val: 0.7121, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5166, Loss: 0.1609 Train: 0.9399, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5167, Loss: 0.1614 Train: 0.9432, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5168, Loss: 0.1645 Train: 0.9497, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5169, Loss: 0.1486 Train: 0.9619, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5170, Loss: 0.1409 Train: 0.9537, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5171, Loss: 0.1554 Train: 0.9545, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5172, Loss: 0.1633 Train: 0.9586, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5173, Loss: 0.1652 Train: 0.9651, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5174, Loss: 0.1566 Train: 0.9700, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5175, Loss: 0.1277 Train: 0.9635, Val: 0.7424, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5176, Loss: 0.1344 Train: 0.9619, Val: 0.7424, Test: 0.7576, Final Test: 0.8182\n",
            "Epoch: 5177, Loss: 0.1631 Train: 0.9683, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5178, Loss: 0.1461 Train: 0.9562, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5179, Loss: 0.1583 Train: 0.9456, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5180, Loss: 0.1668 Train: 0.9513, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5181, Loss: 0.1617 Train: 0.9619, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5182, Loss: 0.1520 Train: 0.9675, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5183, Loss: 0.1376 Train: 0.9602, Val: 0.7235, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5184, Loss: 0.1233 Train: 0.9578, Val: 0.7083, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 5185, Loss: 0.1903 Train: 0.9667, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5186, Loss: 0.1307 Train: 0.9651, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5187, Loss: 0.1343 Train: 0.9554, Val: 0.7348, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 5188, Loss: 0.1795 Train: 0.9545, Val: 0.7311, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5189, Loss: 0.1610 Train: 0.9554, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5190, Loss: 0.1647 Train: 0.9627, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5191, Loss: 0.1204 Train: 0.9619, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5192, Loss: 0.1522 Train: 0.9594, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5193, Loss: 0.1436 Train: 0.9578, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5194, Loss: 0.1366 Train: 0.9610, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5195, Loss: 0.1333 Train: 0.9627, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5196, Loss: 0.1289 Train: 0.9627, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5197, Loss: 0.1303 Train: 0.9627, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5198, Loss: 0.1238 Train: 0.9619, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5199, Loss: 0.1448 Train: 0.9627, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5200, Loss: 0.1339 Train: 0.9594, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5201, Loss: 0.1189 Train: 0.9570, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5202, Loss: 0.1277 Train: 0.9529, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5203, Loss: 0.1319 Train: 0.9545, Val: 0.7083, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5204, Loss: 0.1470 Train: 0.9610, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5205, Loss: 0.1021 Train: 0.9627, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5206, Loss: 0.1239 Train: 0.9635, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5207, Loss: 0.1238 Train: 0.9643, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5208, Loss: 0.1472 Train: 0.9643, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5209, Loss: 0.1237 Train: 0.9659, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5210, Loss: 0.1051 Train: 0.9627, Val: 0.7273, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5211, Loss: 0.1323 Train: 0.9627, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5212, Loss: 0.1287 Train: 0.9635, Val: 0.7386, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5213, Loss: 0.1187 Train: 0.9659, Val: 0.7424, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5214, Loss: 0.1150 Train: 0.9675, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5215, Loss: 0.1040 Train: 0.9667, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5216, Loss: 0.1328 Train: 0.9659, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5217, Loss: 0.1520 Train: 0.9651, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5218, Loss: 0.1137 Train: 0.9627, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5219, Loss: 0.1003 Train: 0.9602, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5220, Loss: 0.1122 Train: 0.9554, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5221, Loss: 0.1266 Train: 0.9521, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5222, Loss: 0.1330 Train: 0.9570, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5223, Loss: 0.1336 Train: 0.9602, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5224, Loss: 0.1140 Train: 0.9659, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5225, Loss: 0.1139 Train: 0.9675, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5226, Loss: 0.1197 Train: 0.9651, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5227, Loss: 0.1244 Train: 0.9659, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5228, Loss: 0.1089 Train: 0.9675, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5229, Loss: 0.1064 Train: 0.9643, Val: 0.7538, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5230, Loss: 0.1134 Train: 0.9635, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5231, Loss: 0.1108 Train: 0.9659, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5232, Loss: 0.1160 Train: 0.9667, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5233, Loss: 0.1231 Train: 0.9683, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5234, Loss: 0.1043 Train: 0.9708, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5235, Loss: 0.0972 Train: 0.9692, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5236, Loss: 0.0973 Train: 0.9700, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5237, Loss: 0.1030 Train: 0.9692, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5238, Loss: 0.1156 Train: 0.9692, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5239, Loss: 0.1191 Train: 0.9708, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5240, Loss: 0.1115 Train: 0.9692, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5241, Loss: 0.0959 Train: 0.9692, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5242, Loss: 0.1209 Train: 0.9692, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5243, Loss: 0.1136 Train: 0.9675, Val: 0.7348, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5244, Loss: 0.1090 Train: 0.9708, Val: 0.7197, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 5245, Loss: 0.0984 Train: 0.9700, Val: 0.7235, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5246, Loss: 0.0979 Train: 0.9700, Val: 0.7311, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5247, Loss: 0.1071 Train: 0.9683, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5248, Loss: 0.1194 Train: 0.9692, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5249, Loss: 0.1002 Train: 0.9716, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5250, Loss: 0.0903 Train: 0.9740, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5251, Loss: 0.0946 Train: 0.9708, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5252, Loss: 0.0976 Train: 0.9708, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5253, Loss: 0.0972 Train: 0.9675, Val: 0.7273, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5254, Loss: 0.1151 Train: 0.9659, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5255, Loss: 0.0988 Train: 0.9635, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5256, Loss: 0.1165 Train: 0.9627, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5257, Loss: 0.1031 Train: 0.9619, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5258, Loss: 0.1061 Train: 0.9619, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5259, Loss: 0.1112 Train: 0.9594, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5260, Loss: 0.0995 Train: 0.9610, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5261, Loss: 0.1095 Train: 0.9586, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5262, Loss: 0.0996 Train: 0.9578, Val: 0.7159, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5263, Loss: 0.0973 Train: 0.9594, Val: 0.7159, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5264, Loss: 0.1079 Train: 0.9651, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5265, Loss: 0.1179 Train: 0.9610, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5266, Loss: 0.1134 Train: 0.9667, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5267, Loss: 0.1137 Train: 0.9732, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5268, Loss: 0.1063 Train: 0.9675, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5269, Loss: 0.1231 Train: 0.9659, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5270, Loss: 0.1193 Train: 0.9619, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5271, Loss: 0.1224 Train: 0.9635, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5272, Loss: 0.1282 Train: 0.9692, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5273, Loss: 0.1108 Train: 0.9716, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5274, Loss: 0.1016 Train: 0.9700, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5275, Loss: 0.1119 Train: 0.9692, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5276, Loss: 0.1213 Train: 0.9708, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5277, Loss: 0.1259 Train: 0.9683, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5278, Loss: 0.1070 Train: 0.9659, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5279, Loss: 0.1217 Train: 0.9700, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5280, Loss: 0.1249 Train: 0.9724, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5281, Loss: 0.1154 Train: 0.9740, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5282, Loss: 0.1029 Train: 0.9740, Val: 0.7576, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5283, Loss: 0.1199 Train: 0.9748, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5284, Loss: 0.1121 Train: 0.9692, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5285, Loss: 0.1053 Train: 0.9692, Val: 0.7500, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 5286, Loss: 0.1047 Train: 0.9708, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5287, Loss: 0.1083 Train: 0.9700, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5288, Loss: 0.1050 Train: 0.9667, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5289, Loss: 0.0968 Train: 0.9683, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5290, Loss: 0.1108 Train: 0.9708, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5291, Loss: 0.1068 Train: 0.9732, Val: 0.7689, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5292, Loss: 0.1159 Train: 0.9756, Val: 0.7652, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5293, Loss: 0.0966 Train: 0.9756, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5294, Loss: 0.0885 Train: 0.9716, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5295, Loss: 0.1107 Train: 0.9724, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5296, Loss: 0.1220 Train: 0.9716, Val: 0.7538, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5297, Loss: 0.1316 Train: 0.9700, Val: 0.7652, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5298, Loss: 0.0956 Train: 0.9708, Val: 0.7727, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5299, Loss: 0.1090 Train: 0.9724, Val: 0.7652, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 5300, Loss: 0.1131 Train: 0.9683, Val: 0.7576, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 5301, Loss: 0.1139 Train: 0.9692, Val: 0.7424, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5302, Loss: 0.1031 Train: 0.9651, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5303, Loss: 0.1145 Train: 0.9667, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5304, Loss: 0.1298 Train: 0.9716, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5305, Loss: 0.1166 Train: 0.9683, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5306, Loss: 0.1169 Train: 0.9659, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5307, Loss: 0.1247 Train: 0.9700, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5308, Loss: 0.1192 Train: 0.9732, Val: 0.7500, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5309, Loss: 0.1196 Train: 0.9732, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5310, Loss: 0.1111 Train: 0.9700, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5311, Loss: 0.1139 Train: 0.9700, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5312, Loss: 0.1521 Train: 0.9700, Val: 0.7689, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5313, Loss: 0.1180 Train: 0.9716, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5314, Loss: 0.1131 Train: 0.9708, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5315, Loss: 0.1034 Train: 0.9692, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5316, Loss: 0.1122 Train: 0.9700, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5317, Loss: 0.0977 Train: 0.9643, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5318, Loss: 0.1195 Train: 0.9659, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5319, Loss: 0.1168 Train: 0.9675, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5320, Loss: 0.1143 Train: 0.9667, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5321, Loss: 0.0978 Train: 0.9635, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5322, Loss: 0.1020 Train: 0.9643, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5323, Loss: 0.1228 Train: 0.9675, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5324, Loss: 0.1741 Train: 0.9610, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5325, Loss: 0.1227 Train: 0.9537, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5326, Loss: 0.1263 Train: 0.9537, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5327, Loss: 0.1214 Train: 0.9578, Val: 0.7500, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5328, Loss: 0.1355 Train: 0.9627, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5329, Loss: 0.0961 Train: 0.9643, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5330, Loss: 0.1409 Train: 0.9651, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5331, Loss: 0.1454 Train: 0.9619, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5332, Loss: 0.1298 Train: 0.9570, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5333, Loss: 0.1714 Train: 0.9545, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5334, Loss: 0.1319 Train: 0.9554, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5335, Loss: 0.1217 Train: 0.9570, Val: 0.7462, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5336, Loss: 0.1273 Train: 0.9513, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5337, Loss: 0.1308 Train: 0.9562, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5338, Loss: 0.1245 Train: 0.9602, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5339, Loss: 0.1314 Train: 0.9651, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5340, Loss: 0.1300 Train: 0.9643, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5341, Loss: 0.1025 Train: 0.9675, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5342, Loss: 0.1221 Train: 0.9700, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5343, Loss: 0.1193 Train: 0.9708, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5344, Loss: 0.1048 Train: 0.9683, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5345, Loss: 0.1202 Train: 0.9667, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5346, Loss: 0.1078 Train: 0.9659, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5347, Loss: 0.1145 Train: 0.9659, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5348, Loss: 0.1169 Train: 0.9675, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5349, Loss: 0.1223 Train: 0.9700, Val: 0.7311, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5350, Loss: 0.1129 Train: 0.9700, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5351, Loss: 0.1176 Train: 0.9683, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5352, Loss: 0.1259 Train: 0.9675, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5353, Loss: 0.1214 Train: 0.9724, Val: 0.7386, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5354, Loss: 0.1250 Train: 0.9773, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5355, Loss: 0.1120 Train: 0.9797, Val: 0.7500, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5356, Loss: 0.1154 Train: 0.9797, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5357, Loss: 0.1333 Train: 0.9797, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5358, Loss: 0.1041 Train: 0.9765, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5359, Loss: 0.1120 Train: 0.9756, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5360, Loss: 0.1027 Train: 0.9724, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5361, Loss: 0.1264 Train: 0.9708, Val: 0.7386, Test: 0.7652, Final Test: 0.8182\n",
            "Epoch: 5362, Loss: 0.0965 Train: 0.9635, Val: 0.7311, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5363, Loss: 0.1294 Train: 0.9602, Val: 0.7235, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 5364, Loss: 0.1328 Train: 0.9610, Val: 0.7121, Test: 0.7614, Final Test: 0.8182\n",
            "Epoch: 5365, Loss: 0.1128 Train: 0.9627, Val: 0.7121, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5366, Loss: 0.1156 Train: 0.9635, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5367, Loss: 0.1345 Train: 0.9651, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5368, Loss: 0.1213 Train: 0.9667, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5369, Loss: 0.1215 Train: 0.9692, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5370, Loss: 0.1039 Train: 0.9692, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5371, Loss: 0.1043 Train: 0.9700, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5372, Loss: 0.1153 Train: 0.9659, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5373, Loss: 0.1124 Train: 0.9643, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5374, Loss: 0.1026 Train: 0.9627, Val: 0.7311, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5375, Loss: 0.1120 Train: 0.9651, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5376, Loss: 0.1107 Train: 0.9683, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5377, Loss: 0.1059 Train: 0.9683, Val: 0.7462, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5378, Loss: 0.0998 Train: 0.9716, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5379, Loss: 0.1160 Train: 0.9740, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5380, Loss: 0.1149 Train: 0.9724, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5381, Loss: 0.0944 Train: 0.9716, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5382, Loss: 0.1008 Train: 0.9724, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5383, Loss: 0.1063 Train: 0.9708, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5384, Loss: 0.1048 Train: 0.9724, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5385, Loss: 0.1153 Train: 0.9740, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5386, Loss: 0.1040 Train: 0.9740, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5387, Loss: 0.0925 Train: 0.9724, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5388, Loss: 0.1019 Train: 0.9683, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5389, Loss: 0.1204 Train: 0.9659, Val: 0.7197, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5390, Loss: 0.0970 Train: 0.9692, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5391, Loss: 0.1069 Train: 0.9716, Val: 0.7235, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 5392, Loss: 0.1004 Train: 0.9724, Val: 0.7348, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 5393, Loss: 0.1170 Train: 0.9708, Val: 0.7576, Test: 0.8333, Final Test: 0.8182\n",
            "Epoch: 5394, Loss: 0.1047 Train: 0.9675, Val: 0.7652, Test: 0.8409, Final Test: 0.8182\n",
            "Epoch: 5395, Loss: 0.1127 Train: 0.9675, Val: 0.7727, Test: 0.8371, Final Test: 0.8182\n",
            "Epoch: 5396, Loss: 0.1029 Train: 0.9667, Val: 0.7652, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 5397, Loss: 0.1079 Train: 0.9667, Val: 0.7538, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5398, Loss: 0.1157 Train: 0.9643, Val: 0.7462, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5399, Loss: 0.1162 Train: 0.9724, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5400, Loss: 0.1164 Train: 0.9781, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5401, Loss: 0.1140 Train: 0.9765, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5402, Loss: 0.1208 Train: 0.9765, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5403, Loss: 0.1236 Train: 0.9789, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5404, Loss: 0.0992 Train: 0.9756, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5405, Loss: 0.1040 Train: 0.9700, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5406, Loss: 0.1072 Train: 0.9692, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5407, Loss: 0.1106 Train: 0.9667, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5408, Loss: 0.1093 Train: 0.9675, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5409, Loss: 0.1014 Train: 0.9643, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5410, Loss: 0.1145 Train: 0.9643, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5411, Loss: 0.1163 Train: 0.9675, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5412, Loss: 0.1405 Train: 0.9748, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5413, Loss: 0.1018 Train: 0.9773, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5414, Loss: 0.1150 Train: 0.9765, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5415, Loss: 0.1136 Train: 0.9724, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5416, Loss: 0.1992 Train: 0.9724, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5417, Loss: 0.1253 Train: 0.9643, Val: 0.7197, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5418, Loss: 0.1108 Train: 0.9627, Val: 0.7121, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5419, Loss: 0.1388 Train: 0.9578, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5420, Loss: 0.1195 Train: 0.9602, Val: 0.7045, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5421, Loss: 0.1303 Train: 0.9594, Val: 0.7083, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 5422, Loss: 0.1149 Train: 0.9635, Val: 0.7273, Test: 0.8220, Final Test: 0.8182\n",
            "Epoch: 5423, Loss: 0.1048 Train: 0.9627, Val: 0.7311, Test: 0.8295, Final Test: 0.8182\n",
            "Epoch: 5424, Loss: 0.1173 Train: 0.9643, Val: 0.7273, Test: 0.8258, Final Test: 0.8182\n",
            "Epoch: 5425, Loss: 0.1236 Train: 0.9659, Val: 0.7273, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5426, Loss: 0.1352 Train: 0.9683, Val: 0.7197, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5427, Loss: 0.1154 Train: 0.9643, Val: 0.7045, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5428, Loss: 0.1067 Train: 0.9627, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5429, Loss: 0.1271 Train: 0.9610, Val: 0.7197, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5430, Loss: 0.1141 Train: 0.9627, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5431, Loss: 0.1110 Train: 0.9692, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5432, Loss: 0.1150 Train: 0.9683, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5433, Loss: 0.0873 Train: 0.9724, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5434, Loss: 0.1190 Train: 0.9748, Val: 0.7424, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5435, Loss: 0.1030 Train: 0.9773, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5436, Loss: 0.1436 Train: 0.9765, Val: 0.7386, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5437, Loss: 0.1016 Train: 0.9700, Val: 0.7348, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5438, Loss: 0.0969 Train: 0.9692, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5439, Loss: 0.0949 Train: 0.9667, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5440, Loss: 0.1127 Train: 0.9659, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5441, Loss: 0.1212 Train: 0.9708, Val: 0.7348, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5442, Loss: 0.1229 Train: 0.9773, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5443, Loss: 0.1241 Train: 0.9740, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5444, Loss: 0.1131 Train: 0.9732, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5445, Loss: 0.1029 Train: 0.9732, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5446, Loss: 0.0987 Train: 0.9716, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5447, Loss: 0.1101 Train: 0.9716, Val: 0.7159, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5448, Loss: 0.1067 Train: 0.9716, Val: 0.6970, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5449, Loss: 0.1242 Train: 0.9651, Val: 0.6932, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5450, Loss: 0.0941 Train: 0.9610, Val: 0.6970, Test: 0.7689, Final Test: 0.8182\n",
            "Epoch: 5451, Loss: 0.1365 Train: 0.9675, Val: 0.7424, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5452, Loss: 0.0889 Train: 0.9643, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5453, Loss: 0.1098 Train: 0.9610, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5454, Loss: 0.1169 Train: 0.9610, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5455, Loss: 0.1267 Train: 0.9675, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5456, Loss: 0.1249 Train: 0.9667, Val: 0.7576, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5457, Loss: 0.1364 Train: 0.9651, Val: 0.7614, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5458, Loss: 0.0971 Train: 0.9651, Val: 0.7576, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5459, Loss: 0.1319 Train: 0.9651, Val: 0.7538, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5460, Loss: 0.1142 Train: 0.9635, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5461, Loss: 0.1340 Train: 0.9594, Val: 0.7348, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5462, Loss: 0.1327 Train: 0.9578, Val: 0.7197, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5463, Loss: 0.1399 Train: 0.9610, Val: 0.7197, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5464, Loss: 0.1236 Train: 0.9643, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5465, Loss: 0.1182 Train: 0.9651, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5466, Loss: 0.1169 Train: 0.9635, Val: 0.7311, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5467, Loss: 0.1219 Train: 0.9667, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5468, Loss: 0.1343 Train: 0.9651, Val: 0.7348, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5469, Loss: 0.1067 Train: 0.9667, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5470, Loss: 0.1078 Train: 0.9635, Val: 0.7500, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5471, Loss: 0.1096 Train: 0.9675, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5472, Loss: 0.1054 Train: 0.9748, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5473, Loss: 0.0927 Train: 0.9732, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5474, Loss: 0.1200 Train: 0.9708, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5475, Loss: 0.1003 Train: 0.9708, Val: 0.7500, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5476, Loss: 0.1559 Train: 0.9683, Val: 0.7614, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5477, Loss: 0.1046 Train: 0.9683, Val: 0.7614, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5478, Loss: 0.1021 Train: 0.9667, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5479, Loss: 0.1036 Train: 0.9602, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5480, Loss: 0.1219 Train: 0.9602, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5481, Loss: 0.1149 Train: 0.9610, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5482, Loss: 0.1098 Train: 0.9659, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5483, Loss: 0.1151 Train: 0.9675, Val: 0.7386, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5484, Loss: 0.1023 Train: 0.9700, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5485, Loss: 0.1068 Train: 0.9708, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5486, Loss: 0.1169 Train: 0.9683, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5487, Loss: 0.1019 Train: 0.9659, Val: 0.7462, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5488, Loss: 0.0984 Train: 0.9627, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5489, Loss: 0.1056 Train: 0.9619, Val: 0.7273, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5490, Loss: 0.1061 Train: 0.9675, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5491, Loss: 0.1008 Train: 0.9683, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5492, Loss: 0.1034 Train: 0.9683, Val: 0.7538, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5493, Loss: 0.0935 Train: 0.9700, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5494, Loss: 0.0918 Train: 0.9732, Val: 0.7614, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5495, Loss: 0.1175 Train: 0.9732, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5496, Loss: 0.1138 Train: 0.9773, Val: 0.7727, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5497, Loss: 0.1062 Train: 0.9732, Val: 0.7727, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5498, Loss: 0.0889 Train: 0.9740, Val: 0.7727, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5499, Loss: 0.1024 Train: 0.9692, Val: 0.7576, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5500, Loss: 0.0837 Train: 0.9635, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5501, Loss: 0.0975 Train: 0.9635, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5502, Loss: 0.1031 Train: 0.9651, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5503, Loss: 0.1075 Train: 0.9683, Val: 0.7386, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5504, Loss: 0.0888 Train: 0.9700, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5505, Loss: 0.0786 Train: 0.9740, Val: 0.7386, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5506, Loss: 0.0932 Train: 0.9756, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5507, Loss: 0.1035 Train: 0.9765, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5508, Loss: 0.0871 Train: 0.9781, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5509, Loss: 0.0809 Train: 0.9756, Val: 0.7386, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5510, Loss: 0.0886 Train: 0.9756, Val: 0.7348, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5511, Loss: 0.1049 Train: 0.9724, Val: 0.7348, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5512, Loss: 0.0902 Train: 0.9675, Val: 0.7462, Test: 0.8182, Final Test: 0.8182\n",
            "Epoch: 5513, Loss: 0.0921 Train: 0.9667, Val: 0.7311, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5514, Loss: 0.0992 Train: 0.9651, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5515, Loss: 0.0911 Train: 0.9659, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5516, Loss: 0.0929 Train: 0.9675, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5517, Loss: 0.1061 Train: 0.9675, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5518, Loss: 0.0929 Train: 0.9667, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5519, Loss: 0.0879 Train: 0.9675, Val: 0.7311, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5520, Loss: 0.0971 Train: 0.9724, Val: 0.7235, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5521, Loss: 0.1004 Train: 0.9740, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5522, Loss: 0.0863 Train: 0.9740, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5523, Loss: 0.1151 Train: 0.9724, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5524, Loss: 0.1110 Train: 0.9724, Val: 0.7386, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5525, Loss: 0.0977 Train: 0.9692, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5526, Loss: 0.0963 Train: 0.9700, Val: 0.7500, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5527, Loss: 0.1163 Train: 0.9683, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5528, Loss: 0.1078 Train: 0.9683, Val: 0.7538, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5529, Loss: 0.0872 Train: 0.9692, Val: 0.7424, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5530, Loss: 0.1065 Train: 0.9700, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5531, Loss: 0.1022 Train: 0.9716, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5532, Loss: 0.1139 Train: 0.9675, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5533, Loss: 0.1117 Train: 0.9675, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5534, Loss: 0.1005 Train: 0.9667, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5535, Loss: 0.0798 Train: 0.9708, Val: 0.7348, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5536, Loss: 0.1032 Train: 0.9732, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5537, Loss: 0.0913 Train: 0.9789, Val: 0.7273, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5538, Loss: 0.1023 Train: 0.9781, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5539, Loss: 0.1025 Train: 0.9748, Val: 0.7386, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5540, Loss: 0.1002 Train: 0.9724, Val: 0.7462, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5541, Loss: 0.0923 Train: 0.9708, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5542, Loss: 0.0998 Train: 0.9708, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5543, Loss: 0.0831 Train: 0.9708, Val: 0.7500, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5544, Loss: 0.1003 Train: 0.9692, Val: 0.7500, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5545, Loss: 0.1087 Train: 0.9683, Val: 0.7538, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5546, Loss: 0.0981 Train: 0.9732, Val: 0.7424, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5547, Loss: 0.0825 Train: 0.9748, Val: 0.7273, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5548, Loss: 0.0895 Train: 0.9724, Val: 0.7348, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5549, Loss: 0.1056 Train: 0.9765, Val: 0.7235, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5550, Loss: 0.1040 Train: 0.9765, Val: 0.7235, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5551, Loss: 0.0954 Train: 0.9748, Val: 0.7386, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5552, Loss: 0.0910 Train: 0.9740, Val: 0.7462, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5553, Loss: 0.0916 Train: 0.9716, Val: 0.7576, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5554, Loss: 0.1129 Train: 0.9692, Val: 0.7538, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5555, Loss: 0.1034 Train: 0.9667, Val: 0.7538, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5556, Loss: 0.0965 Train: 0.9635, Val: 0.7462, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5557, Loss: 0.1106 Train: 0.9651, Val: 0.7197, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5558, Loss: 0.0993 Train: 0.9700, Val: 0.7273, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5559, Loss: 0.1004 Train: 0.9732, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5560, Loss: 0.0966 Train: 0.9716, Val: 0.7121, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5561, Loss: 0.1011 Train: 0.9700, Val: 0.7121, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5562, Loss: 0.1664 Train: 0.9692, Val: 0.7159, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5563, Loss: 0.1157 Train: 0.9651, Val: 0.7235, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5564, Loss: 0.1555 Train: 0.9659, Val: 0.7311, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5565, Loss: 0.1261 Train: 0.9635, Val: 0.7424, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5566, Loss: 0.1036 Train: 0.9635, Val: 0.7424, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5567, Loss: 0.1160 Train: 0.9619, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5568, Loss: 0.1263 Train: 0.9651, Val: 0.7576, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5569, Loss: 0.1302 Train: 0.9659, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5570, Loss: 0.1047 Train: 0.9692, Val: 0.7538, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5571, Loss: 0.1211 Train: 0.9732, Val: 0.7576, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5572, Loss: 0.1411 Train: 0.9740, Val: 0.7462, Test: 0.8144, Final Test: 0.8182\n",
            "Epoch: 5573, Loss: 0.1279 Train: 0.9683, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5574, Loss: 0.1056 Train: 0.9643, Val: 0.7348, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5575, Loss: 0.1147 Train: 0.9562, Val: 0.7273, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5576, Loss: 0.1154 Train: 0.9545, Val: 0.7235, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5577, Loss: 0.1119 Train: 0.9570, Val: 0.7311, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5578, Loss: 0.1042 Train: 0.9602, Val: 0.7348, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5579, Loss: 0.1334 Train: 0.9627, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5580, Loss: 0.1247 Train: 0.9643, Val: 0.7462, Test: 0.8068, Final Test: 0.8182\n",
            "Epoch: 5581, Loss: 0.1177 Train: 0.9683, Val: 0.7462, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5582, Loss: 0.1111 Train: 0.9659, Val: 0.7348, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5583, Loss: 0.1255 Train: 0.9683, Val: 0.7311, Test: 0.7917, Final Test: 0.8182\n",
            "Epoch: 5584, Loss: 0.1051 Train: 0.9700, Val: 0.7386, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5585, Loss: 0.1057 Train: 0.9692, Val: 0.7311, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5586, Loss: 0.1063 Train: 0.9627, Val: 0.7235, Test: 0.7841, Final Test: 0.8182\n",
            "Epoch: 5587, Loss: 0.1226 Train: 0.9651, Val: 0.7273, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5588, Loss: 0.1244 Train: 0.9732, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5589, Loss: 0.1053 Train: 0.9683, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5590, Loss: 0.1000 Train: 0.9659, Val: 0.7159, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5591, Loss: 0.0997 Train: 0.9635, Val: 0.7235, Test: 0.7765, Final Test: 0.8182\n",
            "Epoch: 5592, Loss: 0.1040 Train: 0.9635, Val: 0.7273, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5593, Loss: 0.0890 Train: 0.9635, Val: 0.7197, Test: 0.7803, Final Test: 0.8182\n",
            "Epoch: 5594, Loss: 0.1030 Train: 0.9635, Val: 0.7348, Test: 0.7727, Final Test: 0.8182\n",
            "Epoch: 5595, Loss: 0.1019 Train: 0.9651, Val: 0.7462, Test: 0.7879, Final Test: 0.8182\n",
            "Epoch: 5596, Loss: 0.1399 Train: 0.9700, Val: 0.7500, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5597, Loss: 0.0979 Train: 0.9716, Val: 0.7424, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5598, Loss: 0.1076 Train: 0.9748, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5599, Loss: 0.0894 Train: 0.9740, Val: 0.7614, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5600, Loss: 0.0968 Train: 0.9740, Val: 0.7576, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5601, Loss: 0.1234 Train: 0.9716, Val: 0.7576, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5602, Loss: 0.1133 Train: 0.9708, Val: 0.7424, Test: 0.8030, Final Test: 0.8182\n",
            "Epoch: 5603, Loss: 0.0787 Train: 0.9700, Val: 0.7424, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5604, Loss: 0.1209 Train: 0.9732, Val: 0.7386, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5605, Loss: 0.1256 Train: 0.9724, Val: 0.7235, Test: 0.7955, Final Test: 0.8182\n",
            "Epoch: 5606, Loss: 0.1040 Train: 0.9756, Val: 0.7159, Test: 0.7992, Final Test: 0.8182\n",
            "Epoch: 5607, Loss: 0.0973 Train: 0.9724, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n",
            "Epoch: 5608, Loss: 0.0889 Train: 0.9732, Val: 0.7235, Test: 0.8106, Final Test: 0.8182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pqlg6QdpK45V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}