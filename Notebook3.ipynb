{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batu-el/l65_be301_dc755/blob/main/Notebook3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvyH2qfOICyX"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz8rUSv3IEd9",
        "outputId": "ced04186-2171-4113-ece3-6a7da0734cfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dgl in ./.venv/lib/python3.10/site-packages (1.1.3)\n",
            "Requirement already satisfied: torch_geometric in ./.venv/lib/python3.10/site-packages (2.4.0)\n",
            "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in ./.venv/lib/python3.10/site-packages (from dgl) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in ./.venv/lib/python3.10/site-packages (from dgl) (1.12.0)\n",
            "Requirement already satisfied: networkx>=2.1 in ./.venv/lib/python3.10/site-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.10/site-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from dgl) (4.66.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in ./.venv/lib/python3.10/site-packages (from dgl) (5.9.8)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: pyparsing in ./.venv/lib/python3.10/site-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (from torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.venv/lib/python3.10/site-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in ./.venv/lib/python3.10/site-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Installing PyTorch Geometric\n",
            "Installing other libraries\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (3.2.1)\n",
            "Requirement already satisfied: lovely-tensors in ./.venv/lib/python3.10/site-packages (0.1.15)\n",
            "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from lovely-tensors) (2.1.0)\n",
            "Requirement already satisfied: lovely-numpy>=0.2.9 in ./.venv/lib/python3.10/site-packages (from lovely-tensors) (0.2.11)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.26.4)\n",
            "Requirement already satisfied: fastcore in ./.venv/lib/python3.10/site-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.5.29)\n",
            "Requirement already satisfied: ipython in ./.venv/lib/python3.10/site-packages (from lovely-numpy>=0.2.9->lovely-tensors) (8.21.0)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from lovely-numpy>=0.2.9->lovely-tensors) (3.8.2)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (4.9.0)\n",
            "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (1.12)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (3.1.3)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->lovely-tensors) (12.3.101)\n",
            "Requirement already satisfied: pip in ./.venv/lib/python3.10/site-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (24.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (23.2)\n",
            "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.17.2)\n",
            "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (5.14.1)\n",
            "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (1.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch->lovely-tensors) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.4.5)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch->lovely-tensors) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.16.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl torch_geometric torch\n",
        "\n",
        "# Install required python libraries\n",
        "import os\n",
        "\n",
        "# Install PyTorch Geometric and other libraries\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    print(\"Installing PyTorch Geometric\")\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-geometric\n",
        "    print(\"Installing other libraries\")\n",
        "    !pip install networkx\n",
        "    !pip install lovely-tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSQFqwmpONsR",
        "outputId": "ea52a721-9aca-47a4-ef57-6f8f51cc4969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports succeeded.\n",
            "Python version 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "PyTorch version 2.1.0+cu121\n",
            "PyG version 2.4.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "\n",
        "from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n",
        "\n",
        "import lovely_tensors as lt\n",
        "lt.monkey_patch()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll72cAftH0bk"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iXJ-ymZLH3lD"
      },
      "outputs": [],
      "source": [
        "# ## Outline ###\n",
        "\n",
        "# STEP 1. - Datasets\n",
        "\n",
        "# 1.1 Synthetic Datasets\n",
        "# 1.1.1 Homophilic Node Classification\n",
        "# 1.1.2 Heterophilic Node Classification\n",
        "# 1.1.3 Homophilic Graph Classification\n",
        "# 1.1.4 Heterophilic Graph Classification\n",
        "\n",
        "# 1.2 Real Datasets\n",
        "# 1.2.1 Homophilic Node Classification - Cora\n",
        "# 1.2.2 Heterophilic Node Classification - Texas\n",
        "# 1.2.3 Homophilic Graph Classification - QM9\n",
        "# 1.2.4 Heterophilic Graph Classification - (?)\n",
        "\n",
        "# STEP 2. Models\n",
        "\n",
        "# 2.1 Baselines to Compare Model Accuracies\n",
        "# 2.1.1 GCN\n",
        "# 2.1.2 Sparse Transformer\n",
        "# 2.1.3 MPNN\n",
        "# 2.1.4 Dense Transformer with Attention Mask\n",
        "# 2.1.5 Dense Transformer with Positional Encodings\n",
        "\n",
        "# 2.2 Comparison of 2 Models: Dense (w/ PosEnc) & Sparse Transformer\n",
        "# 2.2.1 1 Head 1 Layer\n",
        "# 2.2.1 4 Head 1 Layer\n",
        "# 2.2.1 1 Head 3 Layer\n",
        "# 2.2.1 4 Head 3 Layer\n",
        "\n",
        "# STEP 3. Evaluation\n",
        "\n",
        "# Comparisons:\n",
        "# A: Adjacency vs Sparse Attention\n",
        "# B: Adjacency vs Dense Attention\n",
        "# C: Sparse Attention vs Dense Attention\n",
        "\n",
        "# 3.1 Combining Multiple Attention Matrices from 2.2\n",
        "# 3.1.1 If Edge Exists\n",
        "# 3.1.2 PCA\n",
        "\n",
        "# 3.2 1D (Vector) Similarity Comparison\n",
        "# 3.2.1 Node Degree (histogram)\n",
        "# 3.2.2 Substructures (histogram)\n",
        "\n",
        "# 3.3 2D (Matrix) Similarity Comparison\n",
        "# 3.3.1 Adjacency Matrix (Graph Edit Dist & Kernel 1 WL)\n",
        "# 3.3.2 Shortest Path (Graph Edit Dist & Kernel 1 WL)\n",
        "\n",
        "# STEP 4. Discussion\n",
        "# Note: Future research can look at how attention evolves over the course of training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvIJYQ5OH0xD"
      },
      "source": [
        "# Synthetic Dataset Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGHfKMjeH80g",
        "outputId": "4993e68b-1a10-4693-80d4-169534f8666d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done saving data into cached files.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import numpy as np\n",
        "\n",
        "def preprocess(data, train_ratio = 0.7, val_ratio = 0.15, test_ratio = 0.15):\n",
        "    g = dataset[0]\n",
        "    y = g.ndata['label']\n",
        "    feat = g.ndata['feat']\n",
        "\n",
        "    num_nodes = len(y)\n",
        "    indices = torch.randperm(num_nodes)\n",
        "\n",
        "    num_train, num_val = int(num_nodes * train_ratio), int(num_nodes * val_ratio)\n",
        "    num_test = num_nodes - num_train - num_val\n",
        "\n",
        "    train_mask, val_mask, test_mask = torch.zeros(num_nodes, dtype=torch.bool), torch.zeros(num_nodes, dtype=torch.bool), torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[indices[:num_train]] = True\n",
        "    val_mask[indices[num_train:num_train+num_val]] = True\n",
        "    test_mask[indices[num_train+num_val:]] = True\n",
        "\n",
        "    # Convert NetworkX graph to edge list\n",
        "    src, dst = g.edges()\n",
        "    edge_list = list(zip(src.tolist(), dst.tolist()))\n",
        "    # Create a set for symmetric edges to avoid duplicates\n",
        "    symmetric_edges = set()\n",
        "\n",
        "    # Add each edge and its reverse to the set\n",
        "    for u, v in edge_list:\n",
        "        symmetric_edges.add((u, v))\n",
        "        symmetric_edges.add((v, u))\n",
        "    edge_list = list(symmetric_edges)\n",
        "\n",
        "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "    node_features = feat #[g.nodes[node]['feat'] for node in G.nodes()]\n",
        "    # Create a Data object\n",
        "    if len(np.array(node_features).shape) == 1:\n",
        "      data = Data(x=torch.tensor(np.array(node_features)).unsqueeze(1), edge_index=torch.tensor(np.array(edge_index)), y=torch.tensor(np.array(y)), train_mask=torch.tensor(np.array(train_mask)), val_mask=torch.tensor(np.array(val_mask)), test_mask=torch.tensor(np.array(test_mask)))\n",
        "    else:\n",
        "      data = Data(x=torch.tensor(np.array(node_features, dtype=float)).float(), edge_index=torch.tensor(np.array(edge_index)), y=torch.tensor(np.array(y)), train_mask=torch.tensor(np.array(train_mask)), val_mask=torch.tensor(np.array(val_mask)), test_mask=torch.tensor(np.array(test_mask)))\n",
        "    return data\n",
        "\n",
        "from dgl.data import BACommunityDataset\n",
        "dataset = BACommunityDataset(num_base_nodes=160,\n",
        "                             num_base_edges_per_node=4,\n",
        "                             num_motifs=80,\n",
        "                             perturb_ratio=0.01,\n",
        "                             num_inter_edges=350,\n",
        "                             seed=None,\n",
        "                             raw_dir=None,\n",
        "                             force_reload=True,\n",
        "                             verbose=True,\n",
        "                             transform=None)\n",
        "\n",
        "dataset.num_classes\n",
        "\n",
        "g = dataset[0]\n",
        "label = g.ndata['label']\n",
        "feat = g.ndata['feat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oB9OLabhMEZ7"
      },
      "outputs": [],
      "source": [
        "data = preprocess(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24vAm7R9H1Br"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PDcNKiW4N0o1"
      },
      "outputs": [],
      "source": [
        "# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n",
        "\n",
        "class GNNModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                # GCNConv(hidden_dim, hidden_dim)\n",
        "                GATConv(hidden_dim, hidden_dim // num_heads, num_heads)\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # conv -> activation ->  dropout -> residual\n",
        "            x_in = x\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "class SparseGraphTransformerModel(Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, dense_adj):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = ~dense_adj.bool())\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # boolean mask enforces graph structure\n",
        "            # x = layer(x, src_mask = ~dense_adj.bool())\n",
        "\n",
        "            # MHSA layer\n",
        "            # boolean mask enforces graph structure\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = ~dense_adj.bool(),\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "class DenseGraphTransformerModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        # x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "            (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "            nan=0, posinf=0, neginf=0\n",
        "        )\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # float mask adds learnable additive attention bias\n",
        "            # x = layer(x, src_mask = attn_bias)\n",
        "\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "class DenseGraphTransformerModel_V2(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        # x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        # attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "        #     (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "        #     nan=0, posinf=0, neginf=0\n",
        "        # )\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # float mask adds learnable additive attention bias\n",
        "            # x = layer(x, src_mask = attn_bias)\n",
        "\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                # attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuBf3YVWH1FL"
      },
      "source": [
        "# Train GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsfKCwj4O4xC",
        "outputId": "d06a01d8-4b2e-45bc-e487-f47d465e51b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1000  class accuracies:  [0.987500011920929, 0.9125000238418579, 0.793749988079071, 0.7875000238418579, 0.96875, 0.856249988079071, 0.824999988079071, 0.6625000238418579]\n",
            "Epoch:  2000  class accuracies:  [0.987500011920929, 0.90625, 0.793749988079071, 0.7749999761581421, 0.9624999761581421, 0.8500000238418579, 0.831250011920929, 0.625]\n",
            "Epoch:  3000  class accuracies:  [0.9937499761581421, 0.90625, 0.762499988079071, 0.7875000238418579, 0.9750000238418579, 0.856249988079071, 0.824999988079071, 0.612500011920929]\n",
            "Epoch:  4000  class accuracies:  [0.9937499761581421, 0.90625, 0.7875000238418579, 0.800000011920929, 0.9624999761581421, 0.8500000238418579, 0.8374999761581421, 0.625]\n",
            "Epoch:  5000  class accuracies:  [0.987500011920929, 0.8999999761581421, 0.793749988079071, 0.7875000238418579, 0.96875, 0.84375, 0.8500000238418579, 0.625]\n",
            "Epoch:  6000  class accuracies:  [0.987500011920929, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.96875, 0.8500000238418579, 0.831250011920929, 0.612500011920929]\n",
            "Epoch:  7000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.7875000238418579, 0.800000011920929, 0.9624999761581421, 0.8500000238418579, 0.831250011920929, 0.625]\n",
            "Epoch:  8000  class accuracies:  [0.987500011920929, 0.9125000238418579, 0.7875000238418579, 0.800000011920929, 0.96875, 0.84375, 0.824999988079071, 0.625]\n",
            "Epoch:  9000  class accuracies:  [0.987500011920929, 0.90625, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.84375, 0.831250011920929, 0.625]\n",
            "Epoch:  10000  class accuracies:  [0.9937499761581421, 0.8999999761581421, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.84375, 0.8374999761581421, 0.625]\n",
            "Epoch:  11000  class accuracies:  [0.9937499761581421, 0.8999999761581421, 0.793749988079071, 0.800000011920929, 0.96875, 0.8500000238418579, 0.824999988079071, 0.612500011920929]\n",
            "Epoch:  12000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.84375, 0.824999988079071, 0.625]\n",
            "Epoch:  13000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.8500000238418579, 0.831250011920929, 0.612500011920929]\n",
            "Epoch:  14000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.8500000238418579, 0.831250011920929, 0.625]\n",
            "Epoch:  15000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.96875, 0.8500000238418579, 0.824999988079071, 0.625]\n",
            "Epoch:  16000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.84375, 0.831250011920929, 0.625]\n",
            "Epoch:  17000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.84375, 0.824999988079071, 0.625]\n",
            "Epoch:  18000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.8500000238418579, 0.824999988079071, 0.625]\n",
            "Epoch:  19000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.8500000238418579, 0.824999988079071, 0.625]\n",
            "Epoch:  20000  class accuracies:  [0.9937499761581421, 0.893750011920929, 0.793749988079071, 0.800000011920929, 0.9624999761581421, 0.8500000238418579, 0.831250011920929, 0.625]\n",
            "Median time per epoch: 0.0094s\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = GNNModel(num_heads=1, num_layers=3).to(device)\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,  weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.5)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
        "    class_correct = torch.zeros(data.y.max() + 1)\n",
        "    class_total = torch.zeros(data.y.max() + 1)\n",
        "\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        mask_pred = pred[mask]\n",
        "        mask_true = data.y[mask]\n",
        "\n",
        "        for i in range(data.y.max() + 1):\n",
        "            class_total[i] += (mask_true == i).sum().item()\n",
        "            class_correct[i] += ((mask_pred == i) & (mask_true == i)).sum().item()\n",
        "\n",
        "    class_accs = class_correct / class_total\n",
        "    return class_accs.tolist()\n",
        "\n",
        "best_val_acc = [0] * (data.y.max() + 1)\n",
        "test_acc = [0] * (data.y.max() + 1)\n",
        "times = []\n",
        "\n",
        "num_epochs = 20000\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    if (epoch % 1000 == 0 or epoch == num_epochs):\n",
        "        print(\"Epoch: \", epoch, \" class accuracies: \", test()) \n",
        "\n",
        "    # train_accs, val_accs, tmp_test_acc = test()\n",
        "    # Update the best validation and test accuracy\n",
        "    # for i, (val_acc, test_acc) in enumerate(zip(val_accs, test_accs)):\n",
        "    #     if val_acc > best_val_acc[i]:\n",
        "    #         best_val_acc[i] = val_acc\n",
        "    #         test_acc[i] = test_acc\n",
        "\n",
        "    # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f}')\n",
        "    # for i, (train_acc, val_acc, tmp_test_acc, best_test_acc) in enumerate(zip(train_accs, val_accs, test_accs, test_acc)):\n",
        "    #     print(f'Class {i}: Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, Best Test: {best_test_acc:.4f}')\n",
        "\n",
        "    times.append(time.time() - start)\n",
        "    scheduler.step()\n",
        "\n",
        "# Print the median time per epoch\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlvxBw7gbYxZ"
      },
      "outputs": [],
      "source": [
        "[0.981249988079071, 0.862500011920929, 0.762499988079071, 0.6875, 0.9937499761581421, 0.856249988079071, 0.8187500238418579, 0.4625000059604645]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Epoch:  19999  class accuracies:  [0.9937499761581421, 0.831250011920929, 0.862500011920929, 0.6875, 1.0, 0.8374999761581421, 0.8500000238418579, 0.8125]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Epoch:  19999  class accuracies:  [0.987500011920929, 0.862500011920929, 0.84375, 0.800000011920929, 0.987500011920929, 0.862500011920929, 0.8687499761581421, 0.6000000238418579]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_cfYcA7PUOs"
      },
      "source": [
        "# Sparse Transformer Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9qRktkBPUxx",
        "outputId": "2f0a20c5-1cf4-4525-e15d-3037d78e7c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001, Loss: 2.1278 Train: 0.1977, Val: 0.2024, Test: 0.2321, Final Test: 0.2321\n",
            "Epoch: 0002, Loss: 1.9987 Train: 0.2589, Val: 0.2500, Test: 0.2917, Final Test: 0.2917\n",
            "Epoch: 0003, Loss: 1.8868 Train: 0.2793, Val: 0.2857, Test: 0.3393, Final Test: 0.3393\n",
            "Epoch: 0004, Loss: 1.7920 Train: 0.3023, Val: 0.3214, Test: 0.3393, Final Test: 0.3393\n",
            "Epoch: 0005, Loss: 1.7236 Train: 0.3240, Val: 0.3095, Test: 0.3393, Final Test: 0.3393\n",
            "Epoch: 0006, Loss: 1.6534 Train: 0.3431, Val: 0.3036, Test: 0.3631, Final Test: 0.3393\n",
            "Epoch: 0007, Loss: 1.6038 Train: 0.3457, Val: 0.2976, Test: 0.3631, Final Test: 0.3393\n",
            "Epoch: 0008, Loss: 1.5441 Train: 0.3482, Val: 0.3095, Test: 0.3690, Final Test: 0.3393\n",
            "Epoch: 0009, Loss: 1.5255 Train: 0.3673, Val: 0.2976, Test: 0.3631, Final Test: 0.3393\n",
            "Epoch: 0010, Loss: 1.5169 Train: 0.3852, Val: 0.3095, Test: 0.3512, Final Test: 0.3393\n",
            "Epoch: 0011, Loss: 1.4800 Train: 0.3852, Val: 0.3155, Test: 0.3333, Final Test: 0.3393\n",
            "Epoch: 0012, Loss: 1.4466 Train: 0.4018, Val: 0.3333, Test: 0.3512, Final Test: 0.3512\n",
            "Epoch: 0013, Loss: 1.4574 Train: 0.4082, Val: 0.3333, Test: 0.3690, Final Test: 0.3512\n",
            "Epoch: 0014, Loss: 1.4428 Train: 0.4273, Val: 0.3393, Test: 0.3810, Final Test: 0.3810\n",
            "Epoch: 0015, Loss: 1.4224 Train: 0.4388, Val: 0.3690, Test: 0.4167, Final Test: 0.4167\n",
            "Epoch: 0016, Loss: 1.4342 Train: 0.4452, Val: 0.3869, Test: 0.4286, Final Test: 0.4286\n",
            "Epoch: 0017, Loss: 1.4043 Train: 0.4579, Val: 0.3690, Test: 0.4524, Final Test: 0.4286\n",
            "Epoch: 0018, Loss: 1.3898 Train: 0.4643, Val: 0.3750, Test: 0.4643, Final Test: 0.4286\n",
            "Epoch: 0019, Loss: 1.3661 Train: 0.4707, Val: 0.3810, Test: 0.4583, Final Test: 0.4286\n",
            "Epoch: 0020, Loss: 1.3370 Train: 0.4809, Val: 0.3929, Test: 0.4702, Final Test: 0.4702\n",
            "Epoch: 0021, Loss: 1.3875 Train: 0.4821, Val: 0.4048, Test: 0.4464, Final Test: 0.4464\n",
            "Epoch: 0022, Loss: 1.3659 Train: 0.4847, Val: 0.4167, Test: 0.4524, Final Test: 0.4524\n",
            "Epoch: 0023, Loss: 1.3055 Train: 0.4911, Val: 0.4226, Test: 0.4524, Final Test: 0.4524\n",
            "Epoch: 0024, Loss: 1.3185 Train: 0.4974, Val: 0.4286, Test: 0.4405, Final Test: 0.4405\n",
            "Epoch: 0025, Loss: 1.3156 Train: 0.5026, Val: 0.4345, Test: 0.4524, Final Test: 0.4524\n",
            "Epoch: 0026, Loss: 1.3057 Train: 0.5077, Val: 0.4226, Test: 0.4643, Final Test: 0.4524\n",
            "Epoch: 0027, Loss: 1.3127 Train: 0.5115, Val: 0.4345, Test: 0.4821, Final Test: 0.4524\n",
            "Epoch: 0028, Loss: 1.2833 Train: 0.5204, Val: 0.4226, Test: 0.5000, Final Test: 0.4524\n",
            "Epoch: 0029, Loss: 1.2964 Train: 0.5166, Val: 0.4167, Test: 0.5060, Final Test: 0.4524\n",
            "Epoch: 0030, Loss: 1.2748 Train: 0.5153, Val: 0.4345, Test: 0.5060, Final Test: 0.4524\n",
            "Epoch: 0031, Loss: 1.2361 Train: 0.5242, Val: 0.4286, Test: 0.5060, Final Test: 0.4524\n",
            "Epoch: 0032, Loss: 1.2554 Train: 0.5408, Val: 0.4226, Test: 0.4881, Final Test: 0.4524\n",
            "Epoch: 0033, Loss: 1.2578 Train: 0.5459, Val: 0.4286, Test: 0.5000, Final Test: 0.4524\n",
            "Epoch: 0034, Loss: 1.2396 Train: 0.5459, Val: 0.4405, Test: 0.5000, Final Test: 0.5000\n",
            "Epoch: 0035, Loss: 1.2450 Train: 0.5446, Val: 0.4345, Test: 0.4940, Final Test: 0.5000\n",
            "Epoch: 0036, Loss: 1.2569 Train: 0.5421, Val: 0.4524, Test: 0.4881, Final Test: 0.4881\n",
            "Epoch: 0037, Loss: 1.2182 Train: 0.5319, Val: 0.4524, Test: 0.4762, Final Test: 0.4881\n",
            "Epoch: 0038, Loss: 1.2161 Train: 0.5255, Val: 0.4464, Test: 0.4821, Final Test: 0.4881\n",
            "Epoch: 0039, Loss: 1.2282 Train: 0.5166, Val: 0.4524, Test: 0.4821, Final Test: 0.4881\n",
            "Epoch: 0040, Loss: 1.2079 Train: 0.5115, Val: 0.4405, Test: 0.4881, Final Test: 0.4881\n",
            "Epoch: 0041, Loss: 1.2179 Train: 0.5089, Val: 0.4405, Test: 0.4881, Final Test: 0.4881\n",
            "Epoch: 0042, Loss: 1.2042 Train: 0.5166, Val: 0.4583, Test: 0.5060, Final Test: 0.5060\n",
            "Epoch: 0043, Loss: 1.1991 Train: 0.5332, Val: 0.4702, Test: 0.5000, Final Test: 0.5000\n",
            "Epoch: 0044, Loss: 1.1759 Train: 0.5574, Val: 0.4821, Test: 0.5000, Final Test: 0.5000\n",
            "Epoch: 0045, Loss: 1.1926 Train: 0.5714, Val: 0.4643, Test: 0.5060, Final Test: 0.5000\n",
            "Epoch: 0046, Loss: 1.1817 Train: 0.5829, Val: 0.4643, Test: 0.5060, Final Test: 0.5000\n",
            "Epoch: 0047, Loss: 1.1684 Train: 0.5791, Val: 0.4643, Test: 0.5060, Final Test: 0.5000\n",
            "Epoch: 0048, Loss: 1.1926 Train: 0.5842, Val: 0.4702, Test: 0.5060, Final Test: 0.5000\n",
            "Epoch: 0049, Loss: 1.1641 Train: 0.5765, Val: 0.4881, Test: 0.5238, Final Test: 0.5238\n",
            "Epoch: 0050, Loss: 1.1447 Train: 0.5689, Val: 0.4821, Test: 0.5179, Final Test: 0.5238\n",
            "Epoch: 0051, Loss: 1.1212 Train: 0.5536, Val: 0.4821, Test: 0.5238, Final Test: 0.5238\n",
            "Epoch: 0052, Loss: 1.1574 Train: 0.5523, Val: 0.4821, Test: 0.5298, Final Test: 0.5238\n",
            "Epoch: 0053, Loss: 1.1034 Train: 0.5727, Val: 0.4940, Test: 0.5238, Final Test: 0.5238\n",
            "Epoch: 0054, Loss: 1.1134 Train: 0.5957, Val: 0.5119, Test: 0.5298, Final Test: 0.5298\n",
            "Epoch: 0055, Loss: 1.1174 Train: 0.6046, Val: 0.4940, Test: 0.5417, Final Test: 0.5298\n",
            "Epoch: 0056, Loss: 1.1291 Train: 0.6173, Val: 0.4762, Test: 0.5595, Final Test: 0.5298\n",
            "Epoch: 0057, Loss: 1.0731 Train: 0.6110, Val: 0.4762, Test: 0.5536, Final Test: 0.5298\n",
            "Epoch: 0058, Loss: 1.1163 Train: 0.5931, Val: 0.4643, Test: 0.5595, Final Test: 0.5298\n",
            "Epoch: 0059, Loss: 1.1089 Train: 0.5867, Val: 0.4643, Test: 0.5595, Final Test: 0.5298\n",
            "Epoch: 0060, Loss: 1.0870 Train: 0.5842, Val: 0.4821, Test: 0.5536, Final Test: 0.5298\n",
            "Epoch: 0061, Loss: 1.0847 Train: 0.6122, Val: 0.5000, Test: 0.5476, Final Test: 0.5298\n",
            "Epoch: 0062, Loss: 1.1037 Train: 0.6339, Val: 0.4940, Test: 0.5536, Final Test: 0.5298\n",
            "Epoch: 0063, Loss: 1.0801 Train: 0.6480, Val: 0.4940, Test: 0.5476, Final Test: 0.5298\n",
            "Epoch: 0064, Loss: 1.0708 Train: 0.6480, Val: 0.4881, Test: 0.5476, Final Test: 0.5298\n",
            "Epoch: 0065, Loss: 1.0529 Train: 0.6339, Val: 0.4821, Test: 0.5417, Final Test: 0.5298\n",
            "Epoch: 0066, Loss: 1.0907 Train: 0.6122, Val: 0.4762, Test: 0.5417, Final Test: 0.5298\n",
            "Epoch: 0067, Loss: 1.0746 Train: 0.6071, Val: 0.4702, Test: 0.5476, Final Test: 0.5298\n",
            "Epoch: 0068, Loss: 1.0347 Train: 0.6033, Val: 0.4762, Test: 0.5476, Final Test: 0.5298\n",
            "Epoch: 0069, Loss: 1.0622 Train: 0.6097, Val: 0.4762, Test: 0.5476, Final Test: 0.5298\n",
            "Epoch: 0070, Loss: 1.0348 Train: 0.6263, Val: 0.4940, Test: 0.5476, Final Test: 0.5298\n",
            "Epoch: 0071, Loss: 1.0512 Train: 0.6390, Val: 0.5000, Test: 0.5595, Final Test: 0.5298\n",
            "Epoch: 0072, Loss: 1.0243 Train: 0.6492, Val: 0.5060, Test: 0.5714, Final Test: 0.5298\n",
            "Epoch: 0073, Loss: 1.0453 Train: 0.6378, Val: 0.5119, Test: 0.5655, Final Test: 0.5298\n",
            "Epoch: 0074, Loss: 0.9874 Train: 0.6390, Val: 0.5238, Test: 0.5595, Final Test: 0.5595\n",
            "Epoch: 0075, Loss: 1.0089 Train: 0.6314, Val: 0.5238, Test: 0.5714, Final Test: 0.5595\n",
            "Epoch: 0076, Loss: 1.0077 Train: 0.6403, Val: 0.5238, Test: 0.5655, Final Test: 0.5595\n",
            "Epoch: 0077, Loss: 1.0177 Train: 0.6429, Val: 0.5238, Test: 0.5595, Final Test: 0.5595\n",
            "Epoch: 0078, Loss: 0.9938 Train: 0.6607, Val: 0.5357, Test: 0.5536, Final Test: 0.5536\n",
            "Epoch: 0079, Loss: 1.0103 Train: 0.6722, Val: 0.5298, Test: 0.5536, Final Test: 0.5536\n",
            "Epoch: 0080, Loss: 1.0069 Train: 0.6811, Val: 0.5417, Test: 0.5714, Final Test: 0.5714\n",
            "Epoch: 0081, Loss: 0.9907 Train: 0.6735, Val: 0.5476, Test: 0.5774, Final Test: 0.5774\n",
            "Epoch: 0082, Loss: 1.0290 Train: 0.6454, Val: 0.5179, Test: 0.5714, Final Test: 0.5774\n",
            "Epoch: 0083, Loss: 1.0036 Train: 0.6250, Val: 0.5179, Test: 0.5536, Final Test: 0.5774\n",
            "Epoch: 0084, Loss: 1.0079 Train: 0.6071, Val: 0.5000, Test: 0.5357, Final Test: 0.5774\n",
            "Epoch: 0085, Loss: 0.9628 Train: 0.6148, Val: 0.4940, Test: 0.5476, Final Test: 0.5774\n",
            "Epoch: 0086, Loss: 0.9893 Train: 0.6212, Val: 0.5119, Test: 0.5417, Final Test: 0.5774\n",
            "Epoch: 0087, Loss: 0.9471 Train: 0.6416, Val: 0.5179, Test: 0.5655, Final Test: 0.5774\n",
            "Epoch: 0088, Loss: 1.0051 Train: 0.6531, Val: 0.5238, Test: 0.5774, Final Test: 0.5774\n",
            "Epoch: 0089, Loss: 0.9891 Train: 0.6645, Val: 0.5417, Test: 0.5893, Final Test: 0.5774\n",
            "Epoch: 0090, Loss: 1.0053 Train: 0.6977, Val: 0.5417, Test: 0.6190, Final Test: 0.5774\n",
            "Epoch: 0091, Loss: 0.9459 Train: 0.6875, Val: 0.5536, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0092, Loss: 0.9564 Train: 0.6671, Val: 0.5298, Test: 0.6012, Final Test: 0.6071\n",
            "Epoch: 0093, Loss: 0.9742 Train: 0.6543, Val: 0.5179, Test: 0.5774, Final Test: 0.6071\n",
            "Epoch: 0094, Loss: 0.9427 Train: 0.6339, Val: 0.5119, Test: 0.5595, Final Test: 0.6071\n",
            "Epoch: 0095, Loss: 0.9861 Train: 0.6339, Val: 0.5179, Test: 0.5476, Final Test: 0.6071\n",
            "Epoch: 0096, Loss: 0.9319 Train: 0.6352, Val: 0.5000, Test: 0.5536, Final Test: 0.6071\n",
            "Epoch: 0097, Loss: 0.9160 Train: 0.6607, Val: 0.5179, Test: 0.5714, Final Test: 0.6071\n",
            "Epoch: 0098, Loss: 0.9333 Train: 0.6862, Val: 0.5298, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0099, Loss: 0.9459 Train: 0.6990, Val: 0.5417, Test: 0.6250, Final Test: 0.6071\n",
            "Epoch: 0100, Loss: 0.9511 Train: 0.6990, Val: 0.5476, Test: 0.6190, Final Test: 0.6071\n",
            "Epoch: 0101, Loss: 0.9490 Train: 0.6786, Val: 0.5476, Test: 0.6012, Final Test: 0.6071\n",
            "Epoch: 0102, Loss: 0.9284 Train: 0.6569, Val: 0.5357, Test: 0.5833, Final Test: 0.6071\n",
            "Epoch: 0103, Loss: 0.9128 Train: 0.6722, Val: 0.5060, Test: 0.5774, Final Test: 0.6071\n",
            "Epoch: 0104, Loss: 0.9242 Train: 0.6773, Val: 0.4881, Test: 0.5774, Final Test: 0.6071\n",
            "Epoch: 0105, Loss: 0.9385 Train: 0.6837, Val: 0.5238, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0106, Loss: 0.9463 Train: 0.6939, Val: 0.5119, Test: 0.6012, Final Test: 0.6071\n",
            "Epoch: 0107, Loss: 0.9478 Train: 0.6926, Val: 0.5179, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0108, Loss: 0.9264 Train: 0.6849, Val: 0.5060, Test: 0.5952, Final Test: 0.6071\n",
            "Epoch: 0109, Loss: 0.9170 Train: 0.6582, Val: 0.5060, Test: 0.5774, Final Test: 0.6071\n",
            "Epoch: 0110, Loss: 0.9476 Train: 0.6607, Val: 0.5060, Test: 0.5774, Final Test: 0.6071\n",
            "Epoch: 0111, Loss: 0.9123 Train: 0.6862, Val: 0.5238, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0112, Loss: 0.9110 Train: 0.7054, Val: 0.5476, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0113, Loss: 0.9149 Train: 0.7117, Val: 0.5536, Test: 0.6131, Final Test: 0.6071\n",
            "Epoch: 0114, Loss: 0.9387 Train: 0.7105, Val: 0.5476, Test: 0.6131, Final Test: 0.6071\n",
            "Epoch: 0115, Loss: 0.9401 Train: 0.7066, Val: 0.5417, Test: 0.6012, Final Test: 0.6071\n",
            "Epoch: 0116, Loss: 0.9020 Train: 0.6837, Val: 0.5417, Test: 0.5833, Final Test: 0.6071\n",
            "Epoch: 0117, Loss: 0.9284 Train: 0.6684, Val: 0.5298, Test: 0.5714, Final Test: 0.6071\n",
            "Epoch: 0118, Loss: 0.9475 Train: 0.6441, Val: 0.5000, Test: 0.5655, Final Test: 0.6071\n",
            "Epoch: 0119, Loss: 0.8689 Train: 0.6518, Val: 0.5000, Test: 0.5774, Final Test: 0.6071\n",
            "Epoch: 0120, Loss: 0.9149 Train: 0.6786, Val: 0.5298, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0121, Loss: 0.9194 Train: 0.6939, Val: 0.5298, Test: 0.6012, Final Test: 0.6071\n",
            "Epoch: 0122, Loss: 0.9089 Train: 0.7028, Val: 0.5417, Test: 0.6250, Final Test: 0.6071\n",
            "Epoch: 0123, Loss: 0.9288 Train: 0.7003, Val: 0.5536, Test: 0.6310, Final Test: 0.6071\n",
            "Epoch: 0124, Loss: 0.9071 Train: 0.6990, Val: 0.5417, Test: 0.6250, Final Test: 0.6071\n",
            "Epoch: 0125, Loss: 0.8979 Train: 0.6849, Val: 0.5357, Test: 0.6190, Final Test: 0.6071\n",
            "Epoch: 0126, Loss: 0.8670 Train: 0.6735, Val: 0.5476, Test: 0.6190, Final Test: 0.6071\n",
            "Epoch: 0127, Loss: 0.9098 Train: 0.6556, Val: 0.5417, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0128, Loss: 0.8649 Train: 0.6416, Val: 0.5417, Test: 0.5655, Final Test: 0.6071\n",
            "Epoch: 0129, Loss: 0.8874 Train: 0.6390, Val: 0.5357, Test: 0.5714, Final Test: 0.6071\n",
            "Epoch: 0130, Loss: 0.9012 Train: 0.6594, Val: 0.5179, Test: 0.5833, Final Test: 0.6071\n",
            "Epoch: 0131, Loss: 0.8448 Train: 0.6747, Val: 0.5298, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0132, Loss: 0.8792 Train: 0.6913, Val: 0.5298, Test: 0.6012, Final Test: 0.6071\n",
            "Epoch: 0133, Loss: 0.8748 Train: 0.7054, Val: 0.5357, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0134, Loss: 0.8725 Train: 0.7143, Val: 0.5238, Test: 0.6190, Final Test: 0.6071\n",
            "Epoch: 0135, Loss: 0.8642 Train: 0.7015, Val: 0.5357, Test: 0.6131, Final Test: 0.6071\n",
            "Epoch: 0136, Loss: 0.8582 Train: 0.6952, Val: 0.5298, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0137, Loss: 0.9167 Train: 0.6811, Val: 0.4940, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0138, Loss: 0.8894 Train: 0.6875, Val: 0.5238, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0139, Loss: 0.8616 Train: 0.6901, Val: 0.5179, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0140, Loss: 0.8613 Train: 0.6773, Val: 0.5179, Test: 0.5952, Final Test: 0.6071\n",
            "Epoch: 0141, Loss: 0.8901 Train: 0.6735, Val: 0.5179, Test: 0.5952, Final Test: 0.6071\n",
            "Epoch: 0142, Loss: 0.8879 Train: 0.6913, Val: 0.5298, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0143, Loss: 0.8940 Train: 0.7015, Val: 0.5238, Test: 0.6012, Final Test: 0.6071\n",
            "Epoch: 0144, Loss: 0.8621 Train: 0.7003, Val: 0.5298, Test: 0.6071, Final Test: 0.6071\n",
            "Epoch: 0145, Loss: 0.8664 Train: 0.7105, Val: 0.5357, Test: 0.6131, Final Test: 0.6071\n",
            "Epoch: 0146, Loss: 0.8301 Train: 0.7270, Val: 0.5357, Test: 0.6012, Final Test: 0.6071\n",
            "Epoch: 0147, Loss: 0.8405 Train: 0.7245, Val: 0.5476, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0148, Loss: 0.8467 Train: 0.7066, Val: 0.5536, Test: 0.5893, Final Test: 0.6071\n",
            "Epoch: 0149, Loss: 0.8577 Train: 0.6849, Val: 0.5298, Test: 0.5833, Final Test: 0.6071\n",
            "Epoch: 0150, Loss: 0.8835 Train: 0.6760, Val: 0.5298, Test: 0.5774, Final Test: 0.6071\n",
            "Epoch: 0151, Loss: 0.8351 Train: 0.6582, Val: 0.5417, Test: 0.5714, Final Test: 0.6071\n",
            "Epoch: 0152, Loss: 0.8655 Train: 0.6620, Val: 0.5357, Test: 0.5655, Final Test: 0.6071\n",
            "Epoch: 0153, Loss: 0.8458 Train: 0.6824, Val: 0.5357, Test: 0.5714, Final Test: 0.6071\n",
            "Epoch: 0154, Loss: 0.8437 Train: 0.7130, Val: 0.5476, Test: 0.5952, Final Test: 0.6071\n",
            "Epoch: 0155, Loss: 0.8431 Train: 0.7168, Val: 0.5655, Test: 0.5952, Final Test: 0.5952\n",
            "Epoch: 0156, Loss: 0.8439 Train: 0.7245, Val: 0.5774, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0157, Loss: 0.8381 Train: 0.7143, Val: 0.5833, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0158, Loss: 0.8254 Train: 0.6990, Val: 0.5714, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0159, Loss: 0.8183 Train: 0.6773, Val: 0.5536, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0160, Loss: 0.8207 Train: 0.6594, Val: 0.5476, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0161, Loss: 0.8453 Train: 0.6645, Val: 0.5476, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0162, Loss: 0.8514 Train: 0.6901, Val: 0.5714, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0163, Loss: 0.7976 Train: 0.7143, Val: 0.5655, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0164, Loss: 0.8418 Train: 0.7258, Val: 0.5833, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0165, Loss: 0.8061 Train: 0.7347, Val: 0.5774, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0166, Loss: 0.8194 Train: 0.7270, Val: 0.5714, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0167, Loss: 0.8203 Train: 0.7041, Val: 0.5595, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0168, Loss: 0.8106 Train: 0.6952, Val: 0.5714, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0169, Loss: 0.8058 Train: 0.7003, Val: 0.5774, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0170, Loss: 0.8556 Train: 0.6939, Val: 0.5714, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0171, Loss: 0.8245 Train: 0.7015, Val: 0.5714, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0172, Loss: 0.8354 Train: 0.7194, Val: 0.5774, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0173, Loss: 0.8274 Train: 0.7207, Val: 0.5833, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0174, Loss: 0.8253 Train: 0.7309, Val: 0.5893, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0175, Loss: 0.8179 Train: 0.7347, Val: 0.5655, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0176, Loss: 0.8378 Train: 0.7283, Val: 0.5774, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0177, Loss: 0.8325 Train: 0.7194, Val: 0.5714, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0178, Loss: 0.7768 Train: 0.7207, Val: 0.5833, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0179, Loss: 0.8197 Train: 0.7105, Val: 0.5833, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0180, Loss: 0.7936 Train: 0.6990, Val: 0.5714, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0181, Loss: 0.8219 Train: 0.6990, Val: 0.5774, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0182, Loss: 0.8012 Train: 0.7143, Val: 0.5833, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0183, Loss: 0.8071 Train: 0.7130, Val: 0.5774, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0184, Loss: 0.8225 Train: 0.7092, Val: 0.5714, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0185, Loss: 0.8077 Train: 0.7143, Val: 0.5655, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0186, Loss: 0.8053 Train: 0.7232, Val: 0.5595, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0187, Loss: 0.7658 Train: 0.7436, Val: 0.5774, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0188, Loss: 0.7863 Train: 0.7360, Val: 0.5655, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0189, Loss: 0.8183 Train: 0.7309, Val: 0.5595, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0190, Loss: 0.7736 Train: 0.7143, Val: 0.5417, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0191, Loss: 0.8322 Train: 0.7054, Val: 0.5417, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0192, Loss: 0.7807 Train: 0.7054, Val: 0.5417, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0193, Loss: 0.8371 Train: 0.7143, Val: 0.5357, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0194, Loss: 0.8269 Train: 0.7194, Val: 0.5595, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0195, Loss: 0.7869 Train: 0.7168, Val: 0.5536, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0196, Loss: 0.7874 Train: 0.7181, Val: 0.5595, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0197, Loss: 0.7773 Train: 0.7105, Val: 0.5476, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0198, Loss: 0.8073 Train: 0.7015, Val: 0.5298, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0199, Loss: 0.7632 Train: 0.7054, Val: 0.5298, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0200, Loss: 0.7939 Train: 0.7258, Val: 0.5476, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0201, Loss: 0.7803 Train: 0.7296, Val: 0.5595, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0202, Loss: 0.7918 Train: 0.7309, Val: 0.5357, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0203, Loss: 0.7850 Train: 0.7270, Val: 0.5238, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0204, Loss: 0.7719 Train: 0.7219, Val: 0.5298, Test: 0.5595, Final Test: 0.6012\n",
            "Epoch: 0205, Loss: 0.7954 Train: 0.7054, Val: 0.5298, Test: 0.5655, Final Test: 0.6012\n",
            "Epoch: 0206, Loss: 0.7729 Train: 0.6926, Val: 0.5238, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0207, Loss: 0.7954 Train: 0.6888, Val: 0.5655, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0208, Loss: 0.8202 Train: 0.7041, Val: 0.5595, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0209, Loss: 0.7888 Train: 0.7105, Val: 0.5655, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0210, Loss: 0.7714 Train: 0.7066, Val: 0.5655, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0211, Loss: 0.7374 Train: 0.6977, Val: 0.5357, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0212, Loss: 0.7766 Train: 0.6888, Val: 0.5179, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0213, Loss: 0.7768 Train: 0.6926, Val: 0.5179, Test: 0.5655, Final Test: 0.6012\n",
            "Epoch: 0214, Loss: 0.7878 Train: 0.7079, Val: 0.5298, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0215, Loss: 0.7662 Train: 0.7194, Val: 0.5595, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0216, Loss: 0.7638 Train: 0.7181, Val: 0.5714, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0217, Loss: 0.7752 Train: 0.7232, Val: 0.5714, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0218, Loss: 0.7631 Train: 0.7207, Val: 0.5595, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0219, Loss: 0.7304 Train: 0.7207, Val: 0.5357, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0220, Loss: 0.7624 Train: 0.7143, Val: 0.5238, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0221, Loss: 0.7581 Train: 0.7181, Val: 0.5238, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0222, Loss: 0.7929 Train: 0.7296, Val: 0.5179, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0223, Loss: 0.7338 Train: 0.7283, Val: 0.5119, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0224, Loss: 0.7513 Train: 0.7360, Val: 0.5417, Test: 0.6131, Final Test: 0.6012\n",
            "Epoch: 0225, Loss: 0.7962 Train: 0.7385, Val: 0.5476, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0226, Loss: 0.7385 Train: 0.7360, Val: 0.5417, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0227, Loss: 0.8093 Train: 0.7270, Val: 0.5357, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0228, Loss: 0.7782 Train: 0.7156, Val: 0.5476, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0229, Loss: 0.7688 Train: 0.7054, Val: 0.5417, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0230, Loss: 0.7859 Train: 0.7054, Val: 0.5357, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0231, Loss: 0.8115 Train: 0.7207, Val: 0.5357, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0232, Loss: 0.7844 Train: 0.7258, Val: 0.5357, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0233, Loss: 0.7736 Train: 0.7321, Val: 0.5298, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0234, Loss: 0.7282 Train: 0.7283, Val: 0.5357, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0235, Loss: 0.7677 Train: 0.7245, Val: 0.5298, Test: 0.5595, Final Test: 0.6012\n",
            "Epoch: 0236, Loss: 0.7534 Train: 0.7258, Val: 0.5357, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0237, Loss: 0.7785 Train: 0.7156, Val: 0.5179, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0238, Loss: 0.7235 Train: 0.7015, Val: 0.5060, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0239, Loss: 0.7496 Train: 0.6964, Val: 0.5119, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0240, Loss: 0.7496 Train: 0.6888, Val: 0.5179, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0241, Loss: 0.7278 Train: 0.6952, Val: 0.5179, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0242, Loss: 0.7261 Train: 0.6977, Val: 0.5119, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0243, Loss: 0.7372 Train: 0.7003, Val: 0.5179, Test: 0.5655, Final Test: 0.6012\n",
            "Epoch: 0244, Loss: 0.7737 Train: 0.7092, Val: 0.5060, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0245, Loss: 0.7657 Train: 0.7207, Val: 0.5298, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0246, Loss: 0.7399 Train: 0.7334, Val: 0.5357, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0247, Loss: 0.7304 Train: 0.7270, Val: 0.5357, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0248, Loss: 0.7593 Train: 0.7283, Val: 0.5298, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0249, Loss: 0.7284 Train: 0.7347, Val: 0.5595, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0250, Loss: 0.7256 Train: 0.7334, Val: 0.5476, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0251, Loss: 0.7535 Train: 0.7347, Val: 0.5476, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0252, Loss: 0.7525 Train: 0.7296, Val: 0.5417, Test: 0.5595, Final Test: 0.6012\n",
            "Epoch: 0253, Loss: 0.7425 Train: 0.7041, Val: 0.5357, Test: 0.5476, Final Test: 0.6012\n",
            "Epoch: 0254, Loss: 0.7461 Train: 0.7130, Val: 0.5357, Test: 0.5476, Final Test: 0.6012\n",
            "Epoch: 0255, Loss: 0.7293 Train: 0.7258, Val: 0.5417, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0256, Loss: 0.7420 Train: 0.7347, Val: 0.5595, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0257, Loss: 0.7173 Train: 0.7538, Val: 0.5714, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0258, Loss: 0.7426 Train: 0.7551, Val: 0.5595, Test: 0.6250, Final Test: 0.6012\n",
            "Epoch: 0259, Loss: 0.7442 Train: 0.7347, Val: 0.5476, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0260, Loss: 0.7620 Train: 0.7181, Val: 0.5417, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0261, Loss: 0.7431 Train: 0.7066, Val: 0.5298, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0262, Loss: 0.7304 Train: 0.7015, Val: 0.5238, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0263, Loss: 0.7192 Train: 0.7105, Val: 0.5179, Test: 0.5655, Final Test: 0.6012\n",
            "Epoch: 0264, Loss: 0.7381 Train: 0.7347, Val: 0.5238, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0265, Loss: 0.7391 Train: 0.7577, Val: 0.5476, Test: 0.6369, Final Test: 0.6012\n",
            "Epoch: 0266, Loss: 0.7195 Train: 0.7628, Val: 0.5536, Test: 0.6369, Final Test: 0.6012\n",
            "Epoch: 0267, Loss: 0.6861 Train: 0.7577, Val: 0.5595, Test: 0.6250, Final Test: 0.6012\n",
            "Epoch: 0268, Loss: 0.7528 Train: 0.7551, Val: 0.5476, Test: 0.6310, Final Test: 0.6012\n",
            "Epoch: 0269, Loss: 0.7275 Train: 0.7347, Val: 0.5357, Test: 0.6310, Final Test: 0.6012\n",
            "Epoch: 0270, Loss: 0.7291 Train: 0.7309, Val: 0.5417, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0271, Loss: 0.7279 Train: 0.7270, Val: 0.5417, Test: 0.5714, Final Test: 0.6012\n",
            "Epoch: 0272, Loss: 0.7660 Train: 0.7309, Val: 0.5417, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0273, Loss: 0.7430 Train: 0.7347, Val: 0.5655, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0274, Loss: 0.6982 Train: 0.7411, Val: 0.5714, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0275, Loss: 0.7361 Train: 0.7334, Val: 0.5595, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0276, Loss: 0.7222 Train: 0.7207, Val: 0.5655, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0277, Loss: 0.7376 Train: 0.7117, Val: 0.5536, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0278, Loss: 0.7571 Train: 0.7181, Val: 0.5595, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0279, Loss: 0.7438 Train: 0.7309, Val: 0.5714, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0280, Loss: 0.6936 Train: 0.7526, Val: 0.5595, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0281, Loss: 0.7064 Train: 0.7589, Val: 0.5714, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0282, Loss: 0.7131 Train: 0.7589, Val: 0.5536, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0283, Loss: 0.7241 Train: 0.7551, Val: 0.5476, Test: 0.5833, Final Test: 0.6012\n",
            "Epoch: 0284, Loss: 0.7479 Train: 0.7449, Val: 0.5298, Test: 0.5893, Final Test: 0.6012\n",
            "Epoch: 0285, Loss: 0.7394 Train: 0.7194, Val: 0.5179, Test: 0.5655, Final Test: 0.6012\n",
            "Epoch: 0286, Loss: 0.7345 Train: 0.7079, Val: 0.5179, Test: 0.5655, Final Test: 0.6012\n",
            "Epoch: 0287, Loss: 0.7074 Train: 0.7207, Val: 0.5417, Test: 0.5655, Final Test: 0.6012\n",
            "Epoch: 0288, Loss: 0.7271 Train: 0.7207, Val: 0.5476, Test: 0.5774, Final Test: 0.6012\n",
            "Epoch: 0289, Loss: 0.7108 Train: 0.7219, Val: 0.5357, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0290, Loss: 0.7350 Train: 0.7258, Val: 0.5357, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0291, Loss: 0.7338 Train: 0.7283, Val: 0.5238, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0292, Loss: 0.7104 Train: 0.7207, Val: 0.5238, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0293, Loss: 0.7235 Train: 0.7270, Val: 0.5238, Test: 0.5952, Final Test: 0.6012\n",
            "Epoch: 0294, Loss: 0.7480 Train: 0.7500, Val: 0.5298, Test: 0.6012, Final Test: 0.6012\n",
            "Epoch: 0295, Loss: 0.7223 Train: 0.7589, Val: 0.5417, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0296, Loss: 0.6799 Train: 0.7628, Val: 0.5417, Test: 0.6071, Final Test: 0.6012\n",
            "Epoch: 0297, Loss: 0.7101 Train: 0.7538, Val: 0.5357, Test: 0.6190, Final Test: 0.6012\n",
            "Epoch: 0298, Loss: 0.7330 Train: 0.7283, Val: 0.5357, Test: 0.6131, Final Test: 0.6012\n",
            "Epoch: 0299, Loss: 0.7046 Train: 0.7066, Val: 0.5179, Test: 0.6012, Final Test: 0.6012\n",
            "Median time per epoch: 0.0084s\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = SparseGraphTransformerModel(num_heads=1, num_layers=3).to(device)\n",
        "\n",
        "data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,  weight_decay=1e-4)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.dense_adj)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred, accs = model(data.x, data.dense_adj).argmax(dim=-1), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "times = []\n",
        "for epoch in range(1, 300):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "          f'Final Test: {test_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WO6X1ZWPU81"
      },
      "source": [
        "# Dense Transformer Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvDCYQiiPVKI",
        "outputId": "a31e7bf0-5144-43e3-f6f4-699444f8c421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001, Loss: 2.1736 Train: 0.1671, Val: 0.1310, Test: 0.1488, Final Test: 0.1488\n",
            "Epoch: 0002, Loss: 2.0509 Train: 0.2398, Val: 0.1726, Test: 0.1726, Final Test: 0.1726\n",
            "Epoch: 0003, Loss: 1.9389 Train: 0.2730, Val: 0.1726, Test: 0.1964, Final Test: 0.1726\n",
            "Epoch: 0004, Loss: 1.8611 Train: 0.2997, Val: 0.2083, Test: 0.2381, Final Test: 0.2381\n",
            "Epoch: 0005, Loss: 1.7776 Train: 0.3291, Val: 0.2143, Test: 0.2738, Final Test: 0.2738\n",
            "Epoch: 0006, Loss: 1.6961 Train: 0.3495, Val: 0.2202, Test: 0.3095, Final Test: 0.3095\n",
            "Epoch: 0007, Loss: 1.6288 Train: 0.3571, Val: 0.2500, Test: 0.2917, Final Test: 0.2917\n",
            "Epoch: 0008, Loss: 1.5785 Train: 0.3750, Val: 0.2440, Test: 0.3333, Final Test: 0.2917\n",
            "Epoch: 0009, Loss: 1.5491 Train: 0.3890, Val: 0.2738, Test: 0.3512, Final Test: 0.3512\n",
            "Epoch: 0010, Loss: 1.5214 Train: 0.3839, Val: 0.2976, Test: 0.3214, Final Test: 0.3214\n",
            "Epoch: 0011, Loss: 1.5161 Train: 0.3827, Val: 0.2798, Test: 0.3333, Final Test: 0.3214\n",
            "Epoch: 0012, Loss: 1.4741 Train: 0.3763, Val: 0.2857, Test: 0.3631, Final Test: 0.3214\n",
            "Epoch: 0013, Loss: 1.4633 Train: 0.3980, Val: 0.3036, Test: 0.3036, Final Test: 0.3036\n",
            "Epoch: 0014, Loss: 1.4206 Train: 0.4069, Val: 0.2917, Test: 0.2917, Final Test: 0.3036\n",
            "Epoch: 0015, Loss: 1.4279 Train: 0.4043, Val: 0.3155, Test: 0.2798, Final Test: 0.2798\n",
            "Epoch: 0016, Loss: 1.4070 Train: 0.4184, Val: 0.3333, Test: 0.3333, Final Test: 0.3333\n",
            "Epoch: 0017, Loss: 1.4007 Train: 0.4324, Val: 0.3274, Test: 0.3571, Final Test: 0.3333\n",
            "Epoch: 0018, Loss: 1.3798 Train: 0.4464, Val: 0.3274, Test: 0.3810, Final Test: 0.3333\n",
            "Epoch: 0019, Loss: 1.3926 Train: 0.4452, Val: 0.3274, Test: 0.4107, Final Test: 0.3333\n",
            "Epoch: 0020, Loss: 1.3632 Train: 0.4515, Val: 0.3393, Test: 0.4107, Final Test: 0.4107\n",
            "Epoch: 0021, Loss: 1.3754 Train: 0.4490, Val: 0.3571, Test: 0.4167, Final Test: 0.4167\n",
            "Epoch: 0022, Loss: 1.3566 Train: 0.4707, Val: 0.3810, Test: 0.4107, Final Test: 0.4107\n",
            "Epoch: 0023, Loss: 1.3427 Train: 0.4783, Val: 0.3929, Test: 0.4286, Final Test: 0.4286\n",
            "Epoch: 0024, Loss: 1.3422 Train: 0.4898, Val: 0.3929, Test: 0.4286, Final Test: 0.4286\n",
            "Epoch: 0025, Loss: 1.3188 Train: 0.4974, Val: 0.4107, Test: 0.4405, Final Test: 0.4405\n",
            "Epoch: 0026, Loss: 1.3311 Train: 0.5064, Val: 0.4405, Test: 0.4524, Final Test: 0.4524\n",
            "Epoch: 0027, Loss: 1.2996 Train: 0.4987, Val: 0.4405, Test: 0.4405, Final Test: 0.4524\n",
            "Epoch: 0028, Loss: 1.3117 Train: 0.4936, Val: 0.4464, Test: 0.4405, Final Test: 0.4405\n",
            "Epoch: 0029, Loss: 1.2975 Train: 0.4923, Val: 0.4464, Test: 0.4405, Final Test: 0.4405\n",
            "Epoch: 0030, Loss: 1.2920 Train: 0.4949, Val: 0.4524, Test: 0.4167, Final Test: 0.4167\n",
            "Epoch: 0031, Loss: 1.2623 Train: 0.5115, Val: 0.4583, Test: 0.4286, Final Test: 0.4286\n",
            "Epoch: 0032, Loss: 1.2537 Train: 0.5255, Val: 0.4702, Test: 0.4345, Final Test: 0.4345\n",
            "Epoch: 0033, Loss: 1.2630 Train: 0.5332, Val: 0.4702, Test: 0.4464, Final Test: 0.4345\n",
            "Epoch: 0034, Loss: 1.2834 Train: 0.5485, Val: 0.4762, Test: 0.4583, Final Test: 0.4583\n",
            "Epoch: 0035, Loss: 1.2484 Train: 0.5561, Val: 0.4821, Test: 0.4583, Final Test: 0.4583\n",
            "Epoch: 0036, Loss: 1.2398 Train: 0.5510, Val: 0.4702, Test: 0.4405, Final Test: 0.4583\n",
            "Epoch: 0037, Loss: 1.2292 Train: 0.5599, Val: 0.4643, Test: 0.4464, Final Test: 0.4583\n",
            "Epoch: 0038, Loss: 1.2098 Train: 0.5561, Val: 0.4762, Test: 0.4583, Final Test: 0.4583\n",
            "Epoch: 0039, Loss: 1.2223 Train: 0.5536, Val: 0.4702, Test: 0.4524, Final Test: 0.4583\n",
            "Epoch: 0040, Loss: 1.2231 Train: 0.5536, Val: 0.4702, Test: 0.4524, Final Test: 0.4583\n",
            "Epoch: 0041, Loss: 1.2139 Train: 0.5625, Val: 0.4702, Test: 0.4583, Final Test: 0.4583\n",
            "Epoch: 0042, Loss: 1.1932 Train: 0.5727, Val: 0.4821, Test: 0.4702, Final Test: 0.4583\n",
            "Epoch: 0043, Loss: 1.2179 Train: 0.5804, Val: 0.4940, Test: 0.4762, Final Test: 0.4762\n",
            "Epoch: 0044, Loss: 1.1785 Train: 0.5893, Val: 0.4881, Test: 0.4821, Final Test: 0.4762\n",
            "Epoch: 0045, Loss: 1.1308 Train: 0.5804, Val: 0.4821, Test: 0.4762, Final Test: 0.4762\n",
            "Epoch: 0046, Loss: 1.1687 Train: 0.5855, Val: 0.4821, Test: 0.4821, Final Test: 0.4762\n",
            "Epoch: 0047, Loss: 1.1410 Train: 0.5867, Val: 0.4881, Test: 0.4821, Final Test: 0.4762\n",
            "Epoch: 0048, Loss: 1.1303 Train: 0.5944, Val: 0.5060, Test: 0.4643, Final Test: 0.4643\n",
            "Epoch: 0049, Loss: 1.1362 Train: 0.5969, Val: 0.5000, Test: 0.4821, Final Test: 0.4643\n",
            "Epoch: 0050, Loss: 1.1389 Train: 0.5995, Val: 0.5060, Test: 0.4762, Final Test: 0.4643\n",
            "Epoch: 0051, Loss: 1.1475 Train: 0.6097, Val: 0.5238, Test: 0.4821, Final Test: 0.4821\n",
            "Epoch: 0052, Loss: 1.1195 Train: 0.6135, Val: 0.5298, Test: 0.5060, Final Test: 0.5060\n",
            "Epoch: 0053, Loss: 1.1351 Train: 0.6237, Val: 0.5060, Test: 0.5000, Final Test: 0.5060\n",
            "Epoch: 0054, Loss: 1.0765 Train: 0.6161, Val: 0.5000, Test: 0.4940, Final Test: 0.5060\n",
            "Epoch: 0055, Loss: 1.1093 Train: 0.6033, Val: 0.5000, Test: 0.4762, Final Test: 0.5060\n",
            "Epoch: 0056, Loss: 1.1199 Train: 0.6084, Val: 0.4881, Test: 0.4702, Final Test: 0.5060\n",
            "Epoch: 0057, Loss: 1.0695 Train: 0.6263, Val: 0.5298, Test: 0.4881, Final Test: 0.5060\n",
            "Epoch: 0058, Loss: 1.0974 Train: 0.6161, Val: 0.5357, Test: 0.4762, Final Test: 0.4762\n",
            "Epoch: 0059, Loss: 1.1194 Train: 0.6199, Val: 0.5238, Test: 0.4940, Final Test: 0.4762\n",
            "Epoch: 0060, Loss: 1.0782 Train: 0.6301, Val: 0.5060, Test: 0.4821, Final Test: 0.4762\n",
            "Epoch: 0061, Loss: 1.0835 Train: 0.6110, Val: 0.4762, Test: 0.4821, Final Test: 0.4762\n",
            "Epoch: 0062, Loss: 1.0802 Train: 0.6110, Val: 0.4524, Test: 0.4821, Final Test: 0.4762\n",
            "Epoch: 0063, Loss: 1.1154 Train: 0.6237, Val: 0.4940, Test: 0.4881, Final Test: 0.4762\n",
            "Epoch: 0064, Loss: 1.0825 Train: 0.6276, Val: 0.5060, Test: 0.4881, Final Test: 0.4762\n",
            "Epoch: 0065, Loss: 1.0453 Train: 0.6288, Val: 0.5060, Test: 0.4881, Final Test: 0.4762\n",
            "Epoch: 0066, Loss: 1.0519 Train: 0.6263, Val: 0.5000, Test: 0.5000, Final Test: 0.4762\n",
            "Epoch: 0067, Loss: 1.0376 Train: 0.6276, Val: 0.4881, Test: 0.4940, Final Test: 0.4762\n",
            "Epoch: 0068, Loss: 1.0445 Train: 0.6378, Val: 0.4702, Test: 0.5179, Final Test: 0.4762\n",
            "Epoch: 0069, Loss: 1.0191 Train: 0.6416, Val: 0.4583, Test: 0.5179, Final Test: 0.4762\n",
            "Epoch: 0070, Loss: 1.0526 Train: 0.6505, Val: 0.4881, Test: 0.5298, Final Test: 0.4762\n",
            "Epoch: 0071, Loss: 1.0380 Train: 0.6518, Val: 0.5179, Test: 0.5298, Final Test: 0.4762\n",
            "Epoch: 0072, Loss: 1.0043 Train: 0.6480, Val: 0.5238, Test: 0.4821, Final Test: 0.4762\n",
            "Epoch: 0073, Loss: 1.0356 Train: 0.6480, Val: 0.5238, Test: 0.5000, Final Test: 0.4762\n",
            "Epoch: 0074, Loss: 1.0141 Train: 0.6186, Val: 0.4940, Test: 0.4821, Final Test: 0.4762\n",
            "Epoch: 0075, Loss: 1.0176 Train: 0.6059, Val: 0.4821, Test: 0.4940, Final Test: 0.4762\n",
            "Epoch: 0076, Loss: 1.0215 Train: 0.6173, Val: 0.4821, Test: 0.4940, Final Test: 0.4762\n",
            "Epoch: 0077, Loss: 1.0245 Train: 0.6454, Val: 0.5000, Test: 0.5238, Final Test: 0.4762\n",
            "Epoch: 0078, Loss: 1.0195 Train: 0.6480, Val: 0.4940, Test: 0.5298, Final Test: 0.4762\n",
            "Epoch: 0079, Loss: 1.0057 Train: 0.6429, Val: 0.5060, Test: 0.5357, Final Test: 0.4762\n",
            "Epoch: 0080, Loss: 0.9952 Train: 0.6480, Val: 0.5000, Test: 0.5119, Final Test: 0.4762\n",
            "Epoch: 0081, Loss: 0.9760 Train: 0.6429, Val: 0.4940, Test: 0.5238, Final Test: 0.4762\n",
            "Epoch: 0082, Loss: 0.9665 Train: 0.6365, Val: 0.5000, Test: 0.5119, Final Test: 0.4762\n",
            "Epoch: 0083, Loss: 1.0348 Train: 0.6390, Val: 0.4881, Test: 0.5119, Final Test: 0.4762\n",
            "Epoch: 0084, Loss: 0.9827 Train: 0.6518, Val: 0.5000, Test: 0.5298, Final Test: 0.4762\n",
            "Epoch: 0085, Loss: 0.9568 Train: 0.6773, Val: 0.5000, Test: 0.5298, Final Test: 0.4762\n",
            "Epoch: 0086, Loss: 0.9492 Train: 0.6747, Val: 0.5119, Test: 0.5179, Final Test: 0.4762\n",
            "Epoch: 0087, Loss: 1.0066 Train: 0.6862, Val: 0.5298, Test: 0.5476, Final Test: 0.4762\n",
            "Epoch: 0088, Loss: 0.9827 Train: 0.6786, Val: 0.5119, Test: 0.5357, Final Test: 0.4762\n",
            "Epoch: 0089, Loss: 0.9394 Train: 0.6492, Val: 0.4821, Test: 0.5179, Final Test: 0.4762\n",
            "Epoch: 0090, Loss: 0.9783 Train: 0.6276, Val: 0.4762, Test: 0.4881, Final Test: 0.4762\n",
            "Epoch: 0091, Loss: 0.9124 Train: 0.6250, Val: 0.4821, Test: 0.4940, Final Test: 0.4762\n",
            "Epoch: 0092, Loss: 0.9750 Train: 0.6378, Val: 0.4821, Test: 0.5060, Final Test: 0.4762\n",
            "Epoch: 0093, Loss: 0.9567 Train: 0.6543, Val: 0.5119, Test: 0.5357, Final Test: 0.4762\n",
            "Epoch: 0094, Loss: 0.9348 Train: 0.6480, Val: 0.5060, Test: 0.5298, Final Test: 0.4762\n",
            "Epoch: 0095, Loss: 0.9927 Train: 0.6403, Val: 0.5000, Test: 0.5179, Final Test: 0.4762\n",
            "Epoch: 0096, Loss: 0.9019 Train: 0.6403, Val: 0.4821, Test: 0.5298, Final Test: 0.4762\n",
            "Epoch: 0097, Loss: 0.9221 Train: 0.6263, Val: 0.4881, Test: 0.5119, Final Test: 0.4762\n",
            "Epoch: 0098, Loss: 0.9532 Train: 0.6416, Val: 0.5000, Test: 0.5060, Final Test: 0.4762\n",
            "Epoch: 0099, Loss: 0.9465 Train: 0.6607, Val: 0.5179, Test: 0.5179, Final Test: 0.4762\n",
            "Median time per epoch: 0.0084s\n"
          ]
        }
      ],
      "source": [
        "def get_shortest_path_matrix(adjacency_matrix):\n",
        "    # Convert the adjacency matrix to a NetworkX graph\n",
        "    graph = nx.from_numpy_array(adjacency_matrix.cpu().numpy(), create_using=nx.DiGraph)\n",
        "    # Compute the shortest path matrix using Floyd-Warshall algorithm in NetworkX\n",
        "    shortest_path_matrix = nx.floyd_warshall_numpy(graph)\n",
        "    # Convert numpy array back to torch tensor\n",
        "    shortest_path_matrix = torch.tensor(shortest_path_matrix).float()\n",
        "    return shortest_path_matrix\n",
        "\n",
        "dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "dense_shortest_path_matrix = get_shortest_path_matrix(dense_adj)  # takes about 1-2 mins\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DenseGraphTransformerModel(num_heads=1, num_layers=3).to(device)\n",
        "\n",
        "data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n",
        "# data = T.AddRandomWalkPE(walk_length = 16, attr_name = 'pos_enc')(data)\n",
        "data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "data.dense_sp_matrix = dense_shortest_path_matrix.float()  # pre-computed in previous cell\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,  weight_decay=1e-4)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "times = []\n",
        "for epoch in range(1, 100):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "          f'Final Test: {test_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "\n",
        "# Notes\n",
        "# - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "# - Node positional encodings are not particularly useful\n",
        "# - Edge distance encodings are very useful\n",
        "# - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NUy9QAQPVXO"
      },
      "source": [
        "# Dense Transformer v2 Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV2VlyWrO7H-",
        "outputId": "b6f2b738-e212-4096-d59a-312599ad0d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001, Loss: 2.0431 Train: 0.2500, Val: 0.2143, Test: 0.2917, Final Test: 0.2917\n",
            "Epoch: 0002, Loss: 1.9753 Train: 0.2730, Val: 0.2381, Test: 0.3095, Final Test: 0.3095\n",
            "Epoch: 0003, Loss: 1.9235 Train: 0.2768, Val: 0.2381, Test: 0.3155, Final Test: 0.3095\n",
            "Epoch: 0004, Loss: 1.8631 Train: 0.2857, Val: 0.2381, Test: 0.3393, Final Test: 0.3095\n",
            "Epoch: 0005, Loss: 1.7991 Train: 0.2755, Val: 0.2440, Test: 0.3333, Final Test: 0.3333\n",
            "Epoch: 0006, Loss: 1.7316 Train: 0.2704, Val: 0.2321, Test: 0.3214, Final Test: 0.3333\n",
            "Epoch: 0007, Loss: 1.6701 Train: 0.2883, Val: 0.2500, Test: 0.3036, Final Test: 0.3036\n",
            "Epoch: 0008, Loss: 1.6010 Train: 0.2946, Val: 0.2262, Test: 0.2976, Final Test: 0.3036\n",
            "Epoch: 0009, Loss: 1.5371 Train: 0.2895, Val: 0.2262, Test: 0.2917, Final Test: 0.3036\n",
            "Epoch: 0010, Loss: 1.5012 Train: 0.3048, Val: 0.2083, Test: 0.3274, Final Test: 0.3036\n",
            "Epoch: 0011, Loss: 1.4489 Train: 0.3061, Val: 0.2619, Test: 0.2798, Final Test: 0.2798\n",
            "Epoch: 0012, Loss: 1.4781 Train: 0.3048, Val: 0.2798, Test: 0.3393, Final Test: 0.3393\n",
            "Epoch: 0013, Loss: 1.5052 Train: 0.3036, Val: 0.2798, Test: 0.2976, Final Test: 0.3393\n",
            "Epoch: 0014, Loss: 1.4707 Train: 0.3163, Val: 0.2857, Test: 0.3036, Final Test: 0.3036\n",
            "Epoch: 0015, Loss: 1.4488 Train: 0.3329, Val: 0.2738, Test: 0.3036, Final Test: 0.3036\n",
            "Epoch: 0016, Loss: 1.4213 Train: 0.3253, Val: 0.2560, Test: 0.3155, Final Test: 0.3036\n",
            "Epoch: 0017, Loss: 1.4387 Train: 0.3355, Val: 0.2560, Test: 0.2798, Final Test: 0.3036\n",
            "Epoch: 0018, Loss: 1.4210 Train: 0.3240, Val: 0.2440, Test: 0.3036, Final Test: 0.3036\n",
            "Epoch: 0019, Loss: 1.4116 Train: 0.3189, Val: 0.2440, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0020, Loss: 1.4120 Train: 0.3495, Val: 0.2619, Test: 0.2917, Final Test: 0.3036\n",
            "Epoch: 0021, Loss: 1.4195 Train: 0.3329, Val: 0.2500, Test: 0.3095, Final Test: 0.3036\n",
            "Epoch: 0022, Loss: 1.4107 Train: 0.3418, Val: 0.2381, Test: 0.3155, Final Test: 0.3036\n",
            "Epoch: 0023, Loss: 1.4181 Train: 0.3482, Val: 0.2321, Test: 0.3155, Final Test: 0.3036\n",
            "Epoch: 0024, Loss: 1.4136 Train: 0.3520, Val: 0.2440, Test: 0.2798, Final Test: 0.3036\n",
            "Epoch: 0025, Loss: 1.4010 Train: 0.3635, Val: 0.2381, Test: 0.2917, Final Test: 0.3036\n",
            "Epoch: 0026, Loss: 1.3842 Train: 0.3661, Val: 0.2679, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0027, Loss: 1.3975 Train: 0.3712, Val: 0.2798, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0028, Loss: 1.3780 Train: 0.3597, Val: 0.2798, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0029, Loss: 1.3773 Train: 0.3546, Val: 0.2798, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0030, Loss: 1.3807 Train: 0.3610, Val: 0.2679, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0031, Loss: 1.3884 Train: 0.3673, Val: 0.2321, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0032, Loss: 1.3804 Train: 0.3635, Val: 0.2143, Test: 0.2798, Final Test: 0.3036\n",
            "Epoch: 0033, Loss: 1.3843 Train: 0.3584, Val: 0.2202, Test: 0.3036, Final Test: 0.3036\n",
            "Epoch: 0034, Loss: 1.3726 Train: 0.3559, Val: 0.2262, Test: 0.2917, Final Test: 0.3036\n",
            "Epoch: 0035, Loss: 1.3636 Train: 0.3610, Val: 0.2381, Test: 0.2976, Final Test: 0.3036\n",
            "Epoch: 0036, Loss: 1.3793 Train: 0.3635, Val: 0.2321, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0037, Loss: 1.3729 Train: 0.3686, Val: 0.2202, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0038, Loss: 1.3700 Train: 0.3622, Val: 0.2381, Test: 0.2857, Final Test: 0.3036\n",
            "Epoch: 0039, Loss: 1.3524 Train: 0.3571, Val: 0.2440, Test: 0.2857, Final Test: 0.3036\n",
            "Epoch: 0040, Loss: 1.3591 Train: 0.3673, Val: 0.2262, Test: 0.2917, Final Test: 0.3036\n",
            "Epoch: 0041, Loss: 1.3581 Train: 0.3635, Val: 0.2321, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0042, Loss: 1.3569 Train: 0.3648, Val: 0.2262, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0043, Loss: 1.3492 Train: 0.3571, Val: 0.2202, Test: 0.2798, Final Test: 0.3036\n",
            "Epoch: 0044, Loss: 1.3796 Train: 0.3571, Val: 0.2024, Test: 0.2857, Final Test: 0.3036\n",
            "Epoch: 0045, Loss: 1.3529 Train: 0.3584, Val: 0.1964, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0046, Loss: 1.3616 Train: 0.3622, Val: 0.2381, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0047, Loss: 1.3562 Train: 0.3597, Val: 0.2083, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0048, Loss: 1.3655 Train: 0.3635, Val: 0.2202, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0049, Loss: 1.3431 Train: 0.3597, Val: 0.2202, Test: 0.2857, Final Test: 0.3036\n",
            "Epoch: 0050, Loss: 1.3674 Train: 0.3584, Val: 0.2143, Test: 0.2857, Final Test: 0.3036\n",
            "Epoch: 0051, Loss: 1.3507 Train: 0.3648, Val: 0.2083, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0052, Loss: 1.3596 Train: 0.3699, Val: 0.2202, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0053, Loss: 1.3562 Train: 0.3584, Val: 0.2321, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0054, Loss: 1.3521 Train: 0.3584, Val: 0.2381, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0055, Loss: 1.3440 Train: 0.3584, Val: 0.2262, Test: 0.2857, Final Test: 0.3036\n",
            "Epoch: 0056, Loss: 1.3522 Train: 0.3661, Val: 0.2560, Test: 0.2976, Final Test: 0.3036\n",
            "Epoch: 0057, Loss: 1.3580 Train: 0.3712, Val: 0.2440, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0058, Loss: 1.3409 Train: 0.3776, Val: 0.2560, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0059, Loss: 1.3440 Train: 0.3712, Val: 0.2381, Test: 0.2798, Final Test: 0.3036\n",
            "Epoch: 0060, Loss: 1.3519 Train: 0.3686, Val: 0.2321, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0061, Loss: 1.3414 Train: 0.3673, Val: 0.2381, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0062, Loss: 1.3505 Train: 0.3610, Val: 0.2321, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0063, Loss: 1.3350 Train: 0.3635, Val: 0.2440, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0064, Loss: 1.3392 Train: 0.3686, Val: 0.2440, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0065, Loss: 1.3529 Train: 0.3776, Val: 0.2381, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0066, Loss: 1.3491 Train: 0.3712, Val: 0.2440, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0067, Loss: 1.3505 Train: 0.3686, Val: 0.2440, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0068, Loss: 1.3415 Train: 0.3699, Val: 0.2500, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0069, Loss: 1.3414 Train: 0.3686, Val: 0.2381, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0070, Loss: 1.3408 Train: 0.3724, Val: 0.2440, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0071, Loss: 1.3330 Train: 0.3724, Val: 0.2500, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0072, Loss: 1.3497 Train: 0.3712, Val: 0.2560, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0073, Loss: 1.3402 Train: 0.3852, Val: 0.2619, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0074, Loss: 1.3383 Train: 0.3865, Val: 0.2560, Test: 0.2500, Final Test: 0.3036\n",
            "Epoch: 0075, Loss: 1.3365 Train: 0.3801, Val: 0.2560, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0076, Loss: 1.3448 Train: 0.3712, Val: 0.2262, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0077, Loss: 1.3373 Train: 0.3763, Val: 0.2381, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0078, Loss: 1.3360 Train: 0.3776, Val: 0.2381, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0079, Loss: 1.3404 Train: 0.3763, Val: 0.2381, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0080, Loss: 1.3432 Train: 0.3686, Val: 0.2500, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0081, Loss: 1.3323 Train: 0.3712, Val: 0.2738, Test: 0.2560, Final Test: 0.3036\n",
            "Epoch: 0082, Loss: 1.3419 Train: 0.3788, Val: 0.2679, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0083, Loss: 1.3374 Train: 0.3814, Val: 0.2500, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0084, Loss: 1.3292 Train: 0.3801, Val: 0.2500, Test: 0.2798, Final Test: 0.3036\n",
            "Epoch: 0085, Loss: 1.3352 Train: 0.3852, Val: 0.2440, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0086, Loss: 1.3407 Train: 0.3788, Val: 0.2500, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0087, Loss: 1.3284 Train: 0.3839, Val: 0.2381, Test: 0.2679, Final Test: 0.3036\n",
            "Epoch: 0088, Loss: 1.3327 Train: 0.3916, Val: 0.2381, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0089, Loss: 1.3311 Train: 0.3865, Val: 0.2440, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0090, Loss: 1.3215 Train: 0.3801, Val: 0.2321, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0091, Loss: 1.3315 Train: 0.3827, Val: 0.2321, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0092, Loss: 1.3240 Train: 0.3839, Val: 0.2321, Test: 0.2798, Final Test: 0.3036\n",
            "Epoch: 0093, Loss: 1.3331 Train: 0.3865, Val: 0.2321, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0094, Loss: 1.3149 Train: 0.3865, Val: 0.2321, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0095, Loss: 1.3231 Train: 0.3890, Val: 0.2440, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0096, Loss: 1.3377 Train: 0.3801, Val: 0.2560, Test: 0.2619, Final Test: 0.3036\n",
            "Epoch: 0097, Loss: 1.3149 Train: 0.3865, Val: 0.2440, Test: 0.2738, Final Test: 0.3036\n",
            "Epoch: 0098, Loss: 1.3228 Train: 0.3954, Val: 0.2321, Test: 0.2798, Final Test: 0.3036\n",
            "Epoch: 0099, Loss: 1.3230 Train: 0.3980, Val: 0.2202, Test: 0.2917, Final Test: 0.3036\n",
            "Median time per epoch: 0.0080s\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DenseGraphTransformerModel_V2(num_heads=1, num_layers=3).to(device)\n",
        "\n",
        "data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n",
        "# data = T.AddRandomWalkPE(walk_length = 16, attr_name = 'pos_enc')(data)\n",
        "data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "data.dense_sp_matrix = dense_shortest_path_matrix.float()  # pre-computed in previous cell\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,  weight_decay=1e-4)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "times = []\n",
        "for epoch in range(1, 100):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "          f'Final Test: {test_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "\n",
        "# Notes\n",
        "# - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "# - Node positional encodings are not particularly useful\n",
        "# - Edge distance encodings are very useful\n",
        "# - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP6OBX80HfFD0mFeGtfzpEo",
      "collapsed_sections": [
        "tvyH2qfOICyX",
        "Ll72cAftH0bk",
        "IvIJYQ5OH0xD"
      ],
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
