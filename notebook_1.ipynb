{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufd2y4shgKa-",
        "outputId": "ebc886a9-563b-45db-f35f-67a248aa7766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing PyTorch Geometric\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing other libraries\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (3.2.1)\n",
            "Requirement already satisfied: lovely-tensors in ./.venv/lib/python3.10/site-packages (0.1.15)\n",
            "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from lovely-tensors) (2.1.0)\n",
            "Requirement already satisfied: lovely-numpy>=0.2.9 in ./.venv/lib/python3.10/site-packages (from lovely-tensors) (0.2.11)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.26.4)\n",
            "Requirement already satisfied: fastcore in ./.venv/lib/python3.10/site-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.5.29)\n",
            "Requirement already satisfied: ipython in ./.venv/lib/python3.10/site-packages (from lovely-numpy>=0.2.9->lovely-tensors) (8.21.0)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from lovely-numpy>=0.2.9->lovely-tensors) (3.8.2)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (4.9.0)\n",
            "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (1.12)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (3.1.3)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in ./.venv/lib/python3.10/site-packages (from torch->lovely-tensors) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->lovely-tensors) (12.3.101)\n",
            "Requirement already satisfied: pip in ./.venv/lib/python3.10/site-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (24.0)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (23.2)\n",
            "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.17.2)\n",
            "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (5.14.1)\n",
            "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (1.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch->lovely-tensors) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.4.5)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch->lovely-tensors) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.16.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required python libraries\n",
        "import os\n",
        "\n",
        "# Install PyTorch Geometric and other libraries\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    print(\"Installing PyTorch Geometric\")\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-geometric\n",
        "    print(\"Installing other libraries\")\n",
        "    !pip install networkx\n",
        "    !pip install lovely-tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLyuVVPLgOfR",
        "outputId": "56384d33-7956-453d-fdde-fe93867313a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports succeeded.\n",
            "Python version 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "PyTorch version 2.1.0+cu121\n",
            "PyG version 2.4.0\n"
          ]
        }
      ],
      "source": [
        "# Import python modules\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, TransformerEncoder, TransformerEncoderLayer, LayerNorm, Module, ModuleList\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import GNNBenchmarkDataset\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
        "\n",
        "import lovely_tensors as lt\n",
        "lt.monkey_patch()\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iNWHUdDGR5q",
        "outputId": "b62451c0-42a1-40e5-8ecb-43b68d8a7d1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2829945/3034156839.py:12: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports succeeded.\n",
            "Python version 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "PyTorch version 2.1.0+cu121\n",
            "PyG version 2.4.0\n"
          ]
        }
      ],
      "source": [
        "#@title [RUN] Import python modules\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.stats import ortho_group\n",
        "from scipy.linalg import block_diag\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, Module, ModuleList, Sequential\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, to_dense_adj, dense_to_sparse\n",
        "from torch_geometric.datasets import Planetoid, QM9\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# from google.colab import files\n",
        "from IPython.display import HTML\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocE2TaQ7gUhC",
        "outputId": "d364ae4d-ba76-4ee9-b44e-afe70c5f430a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All seeds set.\n"
          ]
        }
      ],
      "source": [
        "# Set random seed for deterministic results\n",
        "\n",
        "def seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(0)\n",
        "print(\"All seeds set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BeKO33tXK1Cq"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] Helper functions for unit testing\n",
        "\n",
        "################################\n",
        "# Dummy data helpers for Part 1\n",
        "################################\n",
        "\n",
        "def get_dummy_data_transductive():\n",
        "    input_dim = 64\n",
        "    A = torch.tensor([[0, 1, 1, 0],\n",
        "        [1, 0, 0, 0],\n",
        "        [1, 1, 0, 1],\n",
        "        [1, 1, 1, 0]])\n",
        "    x = torch.rand(A.shape[0], input_dim)\n",
        "    y = torch.tensor([[0.1680148244],\n",
        "        [0.3310719728],\n",
        "        [0.2041909844],\n",
        "        [0.2041909844]])\n",
        "    return A, x, y\n",
        "\n",
        "def get_dummy_data_inductive_layer():\n",
        "    input_dim = 64\n",
        "    A = torch.tensor([[0, 1, 1, 0],\n",
        "        [1, 0, 0, 0],\n",
        "        [1, 1, 0, 1],\n",
        "        [1, 1, 1, 0]])\n",
        "    x = torch.rand(A.shape[0], input_dim)\n",
        "    y = torch.tensor([[0.1086086035],\n",
        "        [0.1543375552],\n",
        "        [0.1992474943],\n",
        "        [0.1992474943]])\n",
        "    return A, x, y\n",
        "\n",
        "def get_dummy_data_inductive_model():\n",
        "    input_dim = 64\n",
        "    A = torch.tensor([[0, 1, 1, 0],\n",
        "        [1, 0, 0, 0],\n",
        "        [1, 1, 0, 1],\n",
        "        [1, 1, 1, 0]])\n",
        "    x = torch.rand(A.shape[0], input_dim)\n",
        "    y = torch.tensor([[-0.0814725384]])\n",
        "    return A, x, y\n",
        "\n",
        "################################\n",
        "# Dummy data helpers for Part 2\n",
        "################################\n",
        "\n",
        "def get_dummy_data():\n",
        "    yield Batch(\n",
        "        x=torch.Tensor([[1], [1]]),\n",
        "        pos=torch.Tensor([[0, 0, 0], [0, 0, 1]]),\n",
        "        edge_index=torch.LongTensor([[0, 1], [1, 0]]),\n",
        "        edge_attr=torch.Tensor([[1], [1]]),\n",
        "    )\n",
        "    yield Batch(\n",
        "        x=torch.Tensor([[1], [1]]),\n",
        "        pos=torch.Tensor([[0, 0, 0], [0, 0, 2]]),\n",
        "        edge_index=torch.LongTensor([[0, 1], [1, 0]]),\n",
        "        edge_attr=torch.Tensor([[1], [1]]),\n",
        "    )\n",
        "    yield Batch(\n",
        "        x=torch.Tensor([[1], [1]]),\n",
        "        pos=torch.Tensor([[0, 0, 0], [1, 2, 3]]),\n",
        "        edge_index=torch.LongTensor([[0, 1], [1, 0]]),\n",
        "        edge_attr=torch.Tensor([[1], [1]]),\n",
        "    )\n",
        "\n",
        "\n",
        "# Invariant Dummies\n",
        "def dummy_not_invariant(x, pos, edge_index, edge_attr):\n",
        "    return pos\n",
        "\n",
        "def dummy_invariant(x, pos, edge_index, edge_attr):\n",
        "    return x\n",
        "\n",
        "def dummy_only_trans_invariant(x, pos, edge_index, edge_attr):\n",
        "    return pos - torch.unsqueeze(pos[0], dim=0)\n",
        "\n",
        "def dummy_only_rot_invariant(x, pos, edge_index, edge_attr):\n",
        "    return torch.sum(pos * torch.unsqueeze(pos[0], dim=0), dim=-1)\n",
        "\n",
        "\n",
        "# Equivariant Dummies\n",
        "def dummy_equivariant(x, pos, edge_index, edge_attr):\n",
        "    return x, pos\n",
        "\n",
        "def dummy_not_equivariant(x, pos, edge_index, edge_attr):\n",
        "    return pos, 2 * pos + 2\n",
        "\n",
        "def dummy_h_not_invariant(x, pos, edge_index, edge_attr):\n",
        "    return pos, pos\n",
        "\n",
        "def dummy_x_not_equivariant(x, pos, edge_index, edge_attr):\n",
        "    return x, 2 * pos + 2\n",
        "\n",
        "def dummy_h_only_trans_invariant(x, pos, edge_index, edge_attr):\n",
        "    return dummy_only_trans_invariant(x, pos, edge_index, edge_attr), pos\n",
        "\n",
        "def dummy_h_only_rot_invariant(x, pos, edge_index, edge_attr):\n",
        "    return dummy_only_rot_invariant(x, pos, edge_index, edge_attr), pos\n",
        "\n",
        "def dummy_x_only_trans_equivariant(x, pos, edge_index, edge_attr):\n",
        "    return x, pos + 2\n",
        "\n",
        "def dummy_x_only_rot_equivariant(x, pos, edge_index, edge_attr):\n",
        "    return x, 2 * pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qz4WMK5F205",
        "outputId": "79a4e26b-b123-4741-f560-d1532431fbd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions loaded.\n"
          ]
        }
      ],
      "source": [
        "# @title [RUN] Helper functions for managing experiments, training, and evaluating models\n",
        "\n",
        "def draw_one_graph(ax, edges, label=None, node_emb=None, layout=None, special_color=False):\n",
        "    \"\"\"draw a graph with networkx based on adjacency matrix (edges)\n",
        "    graph labels could be displayed as a title for each graph\n",
        "    node_emb could be displayed in colors\n",
        "    \"\"\"\n",
        "    graph = nx.Graph()\n",
        "    edges = zip(edges[0], edges[1])\n",
        "    graph.add_edges_from(edges)\n",
        "    node_pos = layout(graph)\n",
        "    #add colors according to node embeding\n",
        "    if (node_emb is not None) or special_color:\n",
        "        color_map = []\n",
        "        node_list = [node[0] for node in graph.nodes(data = True)]\n",
        "        for i,node in enumerate(node_list):\n",
        "            #just ignore this branch\n",
        "            if special_color:\n",
        "                if len(node_list) == 3:\n",
        "                    crt_color = (1,0,0)\n",
        "                elif len(node_list) == 5:\n",
        "                    crt_color = (0,1,0)\n",
        "                elif len(node_list) == 4:\n",
        "                    crt_color = (1,1,0)\n",
        "                else:\n",
        "                  special_list = [(1,0,0)] * 3 + [(0,1,0)] * 5 + [(1,1,0)] * 4\n",
        "                  crt_color = special_list[i]\n",
        "            else:\n",
        "                crt_node_emb = node_emb[node]\n",
        "                #map float number (node embeding) to a color\n",
        "                crt_color = cm.gist_rainbow(crt_node_emb, bytes=True)\n",
        "                crt_color = (crt_color[0]/255.0, crt_color[1]/255.0, crt_color[2]/255.0, crt_color[3]/255.0)\n",
        "            color_map.append(crt_color)\n",
        "\n",
        "        nx.draw_networkx_nodes(graph,node_pos, node_color=color_map,\n",
        "                        nodelist = node_list, ax=ax)\n",
        "        nx.draw_networkx_edges(graph, node_pos, ax=ax)\n",
        "        nx.draw_networkx_labels(graph,node_pos, ax=ax)\n",
        "    else:\n",
        "        nx.draw_networkx(graph, node_pos, ax=ax)\n",
        "\n",
        "\n",
        "def gallery(graphs, labels=None, node_emb=None, special_color=False, max_graphs=4, max_fig_size=(40, 10), layout=nx.layout.kamada_kawai_layout):\n",
        "    ''' Draw multiple graphs as a gallery\n",
        "    Args:\n",
        "      graphs: torch_geometrics.dataset object/ List of Graph objects\n",
        "      labels: num_graphs\n",
        "      node_emb: num_graphs* [num_nodes x num_ch]\n",
        "      max_graphs: maximum graphs display\n",
        "    '''\n",
        "    num_graphs = min(len(graphs), max_graphs)\n",
        "    ff, axes = plt.subplots(1, num_graphs,\n",
        "                            figsize=max_fig_size,\n",
        "                            subplot_kw={'xticks': [], 'yticks': []})\n",
        "    if num_graphs == 1:\n",
        "        axes = [axes]\n",
        "    if node_emb is None:\n",
        "        node_emb = num_graphs*[None]\n",
        "    if labels is None:\n",
        "        labels = num_graphs * [\" \"]\n",
        "\n",
        "\n",
        "    for i in range(num_graphs):\n",
        "        draw_one_graph(axes[i], graphs[i].edge_index.numpy(), labels[i], node_emb[i], layout, special_color)\n",
        "        if labels[i] != \" \":\n",
        "            axes[i].set_title(f\"Target: {labels[i]}\", fontsize=28)\n",
        "        axes[i].set_axis_off()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def hash_node_embedings(node_emb):\n",
        "  \"\"\" Hash the tensor representing nodes' features\n",
        "  to a number in [0,1] used to represent a color\n",
        "\n",
        "  Args:\n",
        "    node_emb: list of num_graphs arrays, each of dim (num_nodes x num_feats)\n",
        "  Returns:\n",
        "    list of num_graphs arrays in [0,1], each of dim (num_nodes)\n",
        "  \"\"\"\n",
        "  chunk_size_graph = [x.shape[0] for x in node_emb]\n",
        "  start_idx_graph = [0] + list(itertools.accumulate(chunk_size_graph))[:-1]\n",
        "\n",
        "  node_emb_flatten = np.concatenate(node_emb).mean(-1)\n",
        "\n",
        "  min_emb = node_emb_flatten.min()\n",
        "  max_emb = node_emb_flatten.max()\n",
        "  node_emb_flatten = (node_emb_flatten-min_emb)/(max_emb-min_emb)\n",
        "\n",
        "  #split in graphs again according to (start_idx_graph, chunk_size_graph)\n",
        "  node_emb_hashed = [node_emb_flatten[i:i+l] for (i,l) in zip(start_idx_graph, chunk_size_graph)]\n",
        "  return node_emb_hashed\n",
        "\n",
        "\n",
        "def update_stats(training_stats, epoch_stats):\n",
        "    \"\"\" Store metrics along the training\n",
        "    Args:\n",
        "      epoch_stats: dict containg metrics about one epoch\n",
        "      training_stats: dict containing lists of metrics along training\n",
        "    Returns:\n",
        "      updated training_stats\n",
        "    \"\"\"\n",
        "    if training_stats is None:\n",
        "        training_stats = {}\n",
        "        for key in epoch_stats.keys():\n",
        "            training_stats[key] = []\n",
        "    for key,val in epoch_stats.items():\n",
        "        training_stats[key].append(val)\n",
        "    return training_stats\n",
        "\n",
        "\n",
        "def plot_stats(training_stats, figsize=(5, 5), name=\"\"):\n",
        "    \"\"\" Create one plot for each metric stored in training_stats\n",
        "    \"\"\"\n",
        "    stats_names = [key[6:] for key in training_stats.keys() if key.startswith('train_')]\n",
        "    f, ax = plt.subplots(len(stats_names), 1, figsize=figsize)\n",
        "    if len(stats_names)==1:\n",
        "        ax = np.array([ax])\n",
        "    for key, axx in zip(stats_names, ax.reshape(-1,)):\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'train_{key}'],\n",
        "            label=f\"Training {key}\")\n",
        "        axx.plot(\n",
        "            training_stats['epoch'],\n",
        "            training_stats[f'val_{key}'],\n",
        "            label=f\"Validation {key}\")\n",
        "        axx.set_xlabel(\"Training epoch\")\n",
        "        axx.set_ylabel(key)\n",
        "        axx.legend()\n",
        "    plt.title(name)\n",
        "\n",
        "\n",
        "def get_color_coded_str(i, color):\n",
        "    return \"\\033[3{}m{}\\033[0m\".format(int(color), int(i))\n",
        "\n",
        "\n",
        "def print_color_numpy(map, list_graphs):\n",
        "    \"\"\" print matrix map in color according to list_graphs\n",
        "    \"\"\"\n",
        "    list_blocks = []\n",
        "    for i,graph in enumerate(list_graphs):\n",
        "        block_i = (i+1)*np.ones((graph.num_nodes,graph.num_nodes))\n",
        "        list_blocks += [block_i]\n",
        "    block_color = block_diag(*list_blocks)\n",
        "\n",
        "    map_modified = np.vectorize(get_color_coded_str)(map, block_color)\n",
        "    print(\"\\n\".join([\" \".join([\"{}\"]*map.shape[0])]*map.shape[1]).format(*[x for y in map_modified.tolist() for x in y]))\n",
        "\n",
        "print(\"Helper functions loaded.\")\n",
        "\n",
        "\n",
        "def train_gnn_cora(X, y, mask, model, optimiser):\n",
        "    model.train()\n",
        "    optimiser.zero_grad()\n",
        "    y_hat = model(X)[mask]\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    return loss.data\n",
        "\n",
        "\n",
        "def evaluate_gnn_cora(X, y, mask, model):\n",
        "    model.eval()\n",
        "    y_hat = model(X)[mask]\n",
        "    y_hat = y_hat.data.max(1)[1]\n",
        "    num_correct = y_hat.eq(y.data).sum()\n",
        "    num_total = len(y)\n",
        "    accuracy = 100.0 * (num_correct/num_total)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train_eval_loop_gnn_cora(model, train_x, train_y, train_mask,\n",
        "                        valid_x, valid_y, valid_mask,\n",
        "                        test_x, test_y, test_mask\n",
        "                    ):\n",
        "    optimiser = Adam(model.parameters(), lr=LR)\n",
        "    training_stats = None\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train_gnn_cora(train_x, train_y, train_mask, model, optimiser)\n",
        "        train_acc = evaluate_gnn_cora(train_x, train_y, train_mask, model)\n",
        "        valid_acc = evaluate_gnn_cora(valid_x, valid_y, valid_mask, model)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch} with train loss: {train_loss:.3f} train accuracy: {train_acc:.3f} validation accuracy: {valid_acc:.3f}\")\n",
        "        # store the loss and the accuracy for the final plot\n",
        "        epoch_stats = {'train_acc': train_acc, 'val_acc': valid_acc, 'epoch':epoch}\n",
        "        training_stats = update_stats(training_stats, epoch_stats)\n",
        "    # Lets look at our final test performance\n",
        "    test_acc = evaluate_gnn_cora(test_x, test_y, test_mask, model)\n",
        "    print(f\"Our final test accuracy for the SimpleGNN is: {test_acc:.3f}\")\n",
        "    return training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CVWk1k0OESAM"
      },
      "outputs": [],
      "source": [
        "# @title [RUN] `CoraDataset` implementation\n",
        "# Let's get the Planetoid Cora dataset from\n",
        "# â€œFastGCN: Fast Learning with Graph Convolutional\n",
        "# Networks via Importance Samplingâ€ (https://arxiv.org/abs/1801.10247)\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "\n",
        "class CoraDataset(object):\n",
        "    def __init__(self):\n",
        "        super(CoraDataset, self).__init__()\n",
        "        cora_pyg = Planetoid(root='/tmp/Cora', name='Cora', split=\"full\")\n",
        "        self.cora_data = cora_pyg[0]\n",
        "        self.train_mask = self.cora_data.train_mask\n",
        "        self.valid_mask = self.cora_data.val_mask\n",
        "        self.test_mask = self.cora_data.test_mask\n",
        "\n",
        "    def train_val_test_split(self):\n",
        "        train_x = self.cora_data.x[self.cora_data.train_mask]\n",
        "        train_y = self.cora_data.y[self.cora_data.train_mask]\n",
        "\n",
        "        valid_x = self.cora_data.x[self.cora_data.val_mask]\n",
        "        valid_y = self.cora_data.y[self.cora_data.val_mask]\n",
        "\n",
        "        test_x = self.cora_data.x[self.cora_data.test_mask]\n",
        "        test_y = self.cora_data.y[self.cora_data.test_mask]\n",
        "        return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
        "\n",
        "    def get_fullx(self):\n",
        "        return self.cora_data.x\n",
        "\n",
        "    def get_adjacency_matrix(self):\n",
        "        # We will ignore this for the first part\n",
        "        adj = to_dense_adj(self.cora_data.edge_index)[0]\n",
        "        return adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmSHOy-IESGl",
        "outputId": "c8614a91-b450-46e7-8716-1ed471d440db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape x: torch.Size([1208, 1433]), y: torch.Size([1208])\n",
            "Val shape x: torch.Size([500, 1433]), y: torch.Size([500])\n",
            "Test shape x: torch.Size([1000, 1433]), y: torch.Size([1000])\n"
          ]
        }
      ],
      "source": [
        "# Lets download our cora dataset and get the splits\n",
        "cora_data = CoraDataset()\n",
        "train_x, train_y, valid_x, valid_y, test_x, test_y = cora_data.train_val_test_split()\n",
        "\n",
        "# Always check and confirm our data shapes match our expectations\n",
        "print(f\"Train shape x: {train_x.shape}, y: {train_y.shape}\")\n",
        "print(f\"Val shape x: {valid_x.shape}, y: {valid_y.shape}\")\n",
        "print(f\"Test shape x: {test_x.shape}, y: {test_y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qJsvuuphESJf"
      },
      "outputs": [],
      "source": [
        "# Fill in the initialisation and forward method the GCNLayer below\n",
        "\n",
        "class GCNLayer(Module):\n",
        "    \"\"\"Graph Convolutional Network layer from Kipf & Welling.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Dimensionality of the input feature vectors\n",
        "        output_dim (int): Dimensionality of the output softmax distribution\n",
        "        A (torch.Tensor): 2-D adjacency matrix\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, A):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.A = A\n",
        "\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        self.A_hat = A + torch.eye(A.size(0))                                 # A + I\n",
        "        D_hat = torch.diag(torch.pow(self.A_hat.sum(1), -0.5))                # D^(-1/2)\n",
        "        self.adj_norm = torch.matmul(torch.matmul(D_hat, self.A_hat), D_hat)  # D^(-1/2) A D^(-1/2)\n",
        "        self.linear = Linear(input_dim, output_dim)                           # W\n",
        "        # ===========================================\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Implements the forward pass for the layer\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): input node feature matrix\n",
        "        \"\"\"\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        print(x)\n",
        "        x = torch.matmul(self.adj_norm, x)  # Aggregate neighbor information\n",
        "        x = self.linear(x)  # Apply linear transformation W\n",
        "        x = F.relu(x)  # Apply non-linearity\n",
        "        # ===========================================\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCWshjveK6wz",
        "outputId": "7ebfe55a-831b-45ed-eb8a-f34280262bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor[4, 64] n=256 (1Kb) xâˆˆ[0.001, 0.997] Î¼=0.477 Ïƒ=0.285\n",
            "tensor[4, 64] n=256 (1Kb) xâˆˆ[0.001, 0.997] Î¼=0.477 Ïƒ=0.285\n",
            "âœ… All seems good!!!\n"
          ]
        }
      ],
      "source": [
        "# @title âœ… [RUN] **Please run this unit test to validate your code. The output would be used to mark your practical.**\n",
        "def testing_gcn():\n",
        "  torch.random.manual_seed(0)\n",
        "  np.random.seed(0)\n",
        "  A,x,y = get_dummy_data_transductive()\n",
        "\n",
        "  input_dim = x.shape[-1]\n",
        "  output_dim = y.shape[-1]\n",
        "\n",
        "  torch.random.manual_seed(0)\n",
        "  model = GCNLayer(input_dim, output_dim, A)\n",
        "  out = model(x)\n",
        "\n",
        "  assert(out.shape == (A.shape[0], output_dim)), \"Oops! ðŸ¤­ Output shape is wrong\"\n",
        "\n",
        "  perm = np.random.permutation(x.shape[0])\n",
        "  perm_x = x[perm]\n",
        "  perm_out = out[perm]\n",
        "  A_perm = A[perm, :][:, perm]\n",
        "\n",
        "  torch.random.manual_seed(0)\n",
        "  model_perm = GCNLayer(input_dim, output_dim, A_perm)\n",
        "\n",
        "  out_model_perm = model_perm(perm_x)\n",
        "  assert (torch.allclose(perm_out, out_model_perm, atol=1e-6)), \"ðŸ¤” Something is wrong in the model! The output is not permutation equivariant anymore ðŸ¥º\"\n",
        "\n",
        "  assert (torch.allclose(out, y, atol=1e-6)), \"ðŸ¤” Something is wrong in the model! The output is wrong.\"\n",
        "  print(\"âœ… All seems good!!!\")\n",
        "\n",
        "\n",
        "# run unit test function\n",
        "testing_gcn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UDGFNRjRESOA"
      },
      "outputs": [],
      "source": [
        "# Lets see the GCNLayer in action!\n",
        "\n",
        "class SimpleGNN(Module):\n",
        "    \"\"\"A Simple GNN model using the GCNLayer for node classification\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Dimensionality of the input feature vectors\n",
        "        output_dim (int): Dimensionality of the output softmax distribution\n",
        "        A (torch.Tensor): 2-D adjacency matrix\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_gcn_layers, A):\n",
        "        super(SimpleGNN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.A = A\n",
        "\n",
        "        # Note: if a single layer is used hidden_dim should be the same as input_dim\n",
        "        if num_gcn_layers > 1:\n",
        "          self.gcn_layers = [GCNLayer(input_dim, hidden_dim, A)]\n",
        "          self.gcn_layers += [GCNLayer(hidden_dim, hidden_dim, A) for i in range(num_gcn_layers-2)]\n",
        "          self.gcn_layers += [GCNLayer(hidden_dim, output_dim, A)]\n",
        "        else:\n",
        "          self.gcn_layers = [GCNLayer(input_dim, output_dim, A)]\n",
        "\n",
        "        self.gcn_layers = ModuleList(self.gcn_layers)\n",
        "        self.num_gcn_layers = num_gcn_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through SimpleGNN on input x\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): input node features\n",
        "        \"\"\"\n",
        "        for j in range(self.num_gcn_layers-1):\n",
        "          x = self.gcn_layers[j](x)\n",
        "          x = F.relu(x)\n",
        "\n",
        "        x = self.gcn_layers[-1](x)\n",
        "\n",
        "        y_hat = x\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Wsxg2ymuGm8z"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS =  100 #@param {type:\"integer\"}\n",
        "LR         = 0.001 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "vLa5PwkhFoQl",
        "outputId": "c57bb451-d3ce-4e2b-e439-fee3ec566b6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "Epoch 0 with train loss: 1.943 train accuracy: 26.325 validation accuracy: 29.200\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "Epoch 10 with train loss: 1.868 train accuracy: 55.298 validation accuracy: 51.400\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "Epoch 20 with train loss: 1.785 train accuracy: 66.474 validation accuracy: 60.200\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "Epoch 30 with train loss: 1.704 train accuracy: 74.172 validation accuracy: 66.800\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "Epoch 40 with train loss: 1.624 train accuracy: 79.222 validation accuracy: 71.200\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "Epoch 50 with train loss: 1.544 train accuracy: 83.030 validation accuracy: 73.800\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "Epoch 60 with train loss: 1.468 train accuracy: 85.844 validation accuracy: 76.200\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n",
            "tensor[2708, 1433] n=3880564 (15Mb) xâˆˆ[0., 1.000] Î¼=0.013 Ïƒ=0.112\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m test_mask \u001b[38;5;241m=\u001b[39m cora_data\u001b[38;5;241m.\u001b[39mtest_mask\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run training loop\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_stats_gnn_cora \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval_loop_gnn_cora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m plot_stats(train_stats_gnn_cora, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGNN_Cora\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[8], line 182\u001b[0m, in \u001b[0;36mtrain_eval_loop_gnn_cora\u001b[0;34m(model, train_x, train_y, train_mask, valid_x, valid_y, valid_mask, test_x, test_y, test_mask)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m    181\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_gnn_cora(train_x, train_y, train_mask, model, optimiser)\n\u001b[0;32m--> 182\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_gnn_cora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     valid_acc \u001b[38;5;241m=\u001b[39m evaluate_gnn_cora(valid_x, valid_y, valid_mask, model)\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "Cell \u001b[0;32mIn[8], line 164\u001b[0m, in \u001b[0;36mevaluate_gnn_cora\u001b[0;34m(X, y, mask, model)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_gnn_cora\u001b[39m(X, y, mask, model):\n\u001b[1;32m    163\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 164\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[mask]\n\u001b[1;32m    165\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    166\u001b[0m     num_correct \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39meq(y\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39msum()\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[13], line 38\u001b[0m, in \u001b[0;36mSimpleGNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_layers[j](x)\n\u001b[1;32m     36\u001b[0m   x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[11], line 31\u001b[0m, in \u001b[0;36mGCNLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implements the forward pass for the layer\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    x (torch.Tensor): input node feature matrix\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ============ YOUR CODE HERE ==============\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj_norm, x)  \u001b[38;5;66;03m# Aggregate neighbor information\u001b[39;00m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)  \u001b[38;5;66;03m# Apply linear transformation W\u001b[39;00m\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/lovely_tensors/patch.py:33\u001b[0m, in \u001b[0;36mmonkey_patch.<locals>.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@patch_to\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m*\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):        \n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mStrProxy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/lovely_tensors/repr_str.py:190\u001b[0m, in \u001b[0;36mStrProxy.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlvl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/lovely_tensors/repr_str.py:138\u001b[0m, in \u001b[0;36mto_str\u001b[0;34m(t, plain, verbose, depth, lvl, color)\u001b[0m\n\u001b[1;32m    136\u001b[0m     common \u001b[38;5;241m=\u001b[39m np_to_str_common(to_numpy(t), color\u001b[38;5;241m=\u001b[39mcolor, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     common \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_to_str_common\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m numel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m nbytes \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m*\u001b[39m t\u001b[38;5;241m.\u001b[39melement_size()\n",
            "File \u001b[0;32m~/l65_be301_dc755/.venv/lib/python3.10/site-packages/lovely_tensors/repr_str.py:73\u001b[0m, in \u001b[0;36mtorch_to_str_common\u001b[0;34m(t, color)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m ansi_color(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m\"\u001b[39m, color)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Unlike .min()/.max(), amin/amax do not allocate extra GPU memory.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m amin, amax \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu(), t\u001b[38;5;241m.\u001b[39mamax()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     75\u001b[0m zeros \u001b[38;5;241m=\u001b[39m ansi_color(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_zeros\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m\"\u001b[39m, color) \u001b[38;5;28;01mif\u001b[39;00m amin\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m amax\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# pinf = ansi_color(\"+Inf!\", \"red\", color) if amax.isposinf() else None\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# ninf = ansi_color(\"-Inf!\", \"red\", color) if amin.isneginf() else None\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# nan = ansi_color(\"NaN!\", \"red\", color) if amin.isnan() else None\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# attention = sparse_join([zeros,pinf,ninf,nan])\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# numel = f\"n={t.numel()}\" if t.numel() > 5 and max(t.shape) != t.numel() else None\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Instantiate our model and optimiser\n",
        "A = cora_data.get_adjacency_matrix()\n",
        "X = cora_data.get_fullx()\n",
        "model = SimpleGNN(input_dim=train_x.shape[-1], output_dim=7, A=A, hidden_dim=train_x.shape[-1], num_gcn_layers=1)\n",
        "train_mask = cora_data.train_mask\n",
        "valid_mask = cora_data.valid_mask\n",
        "test_mask = cora_data.test_mask\n",
        "\n",
        "# Run training loop\n",
        "train_stats_gnn_cora = train_eval_loop_gnn_cora(model, X, train_y, train_mask,\n",
        "                                          X, valid_y, valid_mask,\n",
        "                                          X, test_y, test_mask\n",
        "                                       )\n",
        "plot_stats(train_stats_gnn_cora, name=\"GNN_Cora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O19jAscEHQY_"
      },
      "outputs": [],
      "source": [
        "# Fill in the initialisation and forward method the GCNLayer below\n",
        "\n",
        "class GCNLayer_Transformer(Module):\n",
        "    \"\"\"Graph Convolutional Network layer from Kipf & Welling.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Dimensionality of the input feature vectors\n",
        "        output_dim (int): Dimensionality of the output softmax distribution\n",
        "        A (torch.Tensor): 2-D adjacency matrix\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, A):\n",
        "        super(GCNLayer_Transformer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.A = A\n",
        "\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        self.multihead_attn = torch.nn.MultiheadAttention(input_dim, 1)\n",
        "        self.linear = Linear(input_dim, output_dim)\n",
        "        # ===========================================\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Implements the forward pass for the layer\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): input node feature matrix\n",
        "        \"\"\"\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        x = self.multihead_attn(x, x, x)[0]\n",
        "        # x = self.transformer_encoder(x, A)\n",
        "        x = self.linear(x)\n",
        "        # x = F.relu(x)\n",
        "        # ===========================================\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "YjwKWAYmKlEg",
        "outputId": "18fffc96-c17c-4c2c-bbc0-ba88d53ced97"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "ðŸ¤” Something is wrong in the model! The output is wrong.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… All seems good!!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# run unit test function\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtesting_gcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[15], line 27\u001b[0m, in \u001b[0;36mtesting_gcn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m out_model_perm \u001b[38;5;241m=\u001b[39m model_perm(perm_x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mallclose(perm_out, out_model_perm, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ¤” Something is wrong in the model! The output is not permutation equivariant anymore ðŸ¥º\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mallclose(out, y, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ¤” Something is wrong in the model! The output is wrong.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… All seems good!!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAssertionError\u001b[0m: ðŸ¤” Something is wrong in the model! The output is wrong."
          ]
        }
      ],
      "source": [
        "# @title âœ… [RUN] **Please run this unit test to validate your code. The output would be used to mark your practical.**\n",
        "def testing_gcn():\n",
        "  torch.random.manual_seed(0)\n",
        "  np.random.seed(0)\n",
        "  A,x,y = get_dummy_data_transductive()\n",
        "\n",
        "  input_dim = x.shape[-1]\n",
        "  output_dim = y.shape[-1]\n",
        "\n",
        "  torch.random.manual_seed(0)\n",
        "  model = GCNLayer_Transformer(input_dim, output_dim, A)\n",
        "  out = model(x)\n",
        "\n",
        "  assert(out.shape == (A.shape[0], output_dim)), \"Oops! ðŸ¤­ Output shape is wrong\"\n",
        "\n",
        "  perm = np.random.permutation(x.shape[0])\n",
        "  perm_x = x[perm]\n",
        "  perm_out = out[perm]\n",
        "  A_perm = A[perm, :][:, perm]\n",
        "\n",
        "  torch.random.manual_seed(0)\n",
        "  model_perm = GCNLayer_Transformer(input_dim, output_dim, A_perm)\n",
        "\n",
        "  out_model_perm = model_perm(perm_x)\n",
        "  assert (torch.allclose(perm_out, out_model_perm, atol=1e-6)), \"ðŸ¤” Something is wrong in the model! The output is not permutation equivariant anymore ðŸ¥º\"\n",
        "\n",
        "  assert (torch.allclose(out, y, atol=1e-6)), \"ðŸ¤” Something is wrong in the model! The output is wrong.\"\n",
        "  print(\"âœ… All seems good!!!\")\n",
        "\n",
        "# run unit test function\n",
        "testing_gcn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FkTY3lVxJXTv"
      },
      "outputs": [],
      "source": [
        "# Lets see the GCNLayer in action!\n",
        "\n",
        "class SimpleGNN_Transformer(Module):\n",
        "    \"\"\"A Simple GNN model using the GCNLayer for node classification\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Dimensionality of the input feature vectors\n",
        "        output_dim (int): Dimensionality of the output softmax distribution\n",
        "        A (torch.Tensor): 2-D adjacency matrix\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_gcn_layers, A):\n",
        "        super(SimpleGNN_Transformer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.A = A\n",
        "\n",
        "        # Note: if a single layer is used hidden_dim should be the same as input_dim\n",
        "        if num_gcn_layers > 1:\n",
        "          self.gcn_layers = [GCNLayer_Transformer(input_dim, hidden_dim, A)]\n",
        "          self.gcn_layers += [GCNLayer_Transformer(hidden_dim, hidden_dim, A) for i in range(num_gcn_layers-2)]\n",
        "          self.gcn_layers += [GCNLayer_Transformer(hidden_dim, output_dim, A)]\n",
        "        else:\n",
        "          self.gcn_layers = [GCNLayer_Transformer(input_dim, output_dim, A)]\n",
        "\n",
        "        self.gcn_layers = ModuleList(self.gcn_layers)\n",
        "        self.num_gcn_layers = num_gcn_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through SimpleGNN on input x\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): input node features\n",
        "        \"\"\"\n",
        "        for j in range(self.num_gcn_layers-1):\n",
        "          x = self.gcn_layers[j](x)\n",
        "          x = F.relu(x)\n",
        "          \n",
        "\n",
        "        x = self.gcn_layers[-1](x)\n",
        "\n",
        "        y_hat = x\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "asFFYMJWFu9i"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS =  100 #@param {type:\"integer\"}\n",
        "LR         = 0.001 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "DZeBUHKzKRfC",
        "outputId": "7a4bae1c-5076-4672-a4a5-30ca5e621f2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 with train loss: 1.946 train accuracy: 28.228 validation accuracy: 31.600\n",
            "Epoch 10 with train loss: 1.841 train accuracy: 28.228 validation accuracy: 31.600\n",
            "Epoch 20 with train loss: 1.735 train accuracy: 28.974 validation accuracy: 31.600\n",
            "Epoch 30 with train loss: 1.300 train accuracy: 46.772 validation accuracy: 37.000\n",
            "Epoch 40 with train loss: 1.023 train accuracy: 55.795 validation accuracy: 37.800\n",
            "Epoch 50 with train loss: 0.785 train accuracy: 68.626 validation accuracy: 35.800\n",
            "Epoch 60 with train loss: 0.539 train accuracy: 82.781 validation accuracy: 34.200\n",
            "Epoch 70 with train loss: 0.311 train accuracy: 89.735 validation accuracy: 33.200\n",
            "Epoch 80 with train loss: 0.136 train accuracy: 96.275 validation accuracy: 35.600\n",
            "Epoch 90 with train loss: 0.048 train accuracy: 99.255 validation accuracy: 35.600\n",
            "Our final test accuracy for the SimpleGNN is: 40.000\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Instantiate our model and optimiser\n",
        "A = cora_data.get_adjacency_matrix()\n",
        "X = cora_data.get_fullx()\n",
        "model = SimpleGNN_Transformer(input_dim=train_x.shape[-1], output_dim=7, A=A, hidden_dim=train_x.shape[-1], num_gcn_layers=1)\n",
        "train_mask = cora_data.train_mask\n",
        "valid_mask = cora_data.valid_mask\n",
        "test_mask = cora_data.test_mask\n",
        "\n",
        "# Run training loop\n",
        "train_stats_gnn_cora = train_eval_loop_gnn_cora(model, X, train_y, train_mask,\n",
        "                                          X, valid_y, valid_mask,\n",
        "                                          X, test_y, test_mask\n",
        "                                       )\n",
        "plot_stats(train_stats_gnn_cora, name=\"GNN_Cora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNwZuLEEKU8W"
      },
      "outputs": [],
      "source": [
        "# hetrophilic task -> attending to everythig is better\n",
        "# homophilic task -> attending to graph is good\n",
        "\n",
        "# dont use cora !! --- highly homophilic\n",
        "# control homophily\n",
        "\n",
        "# cyclic object\n",
        "\n",
        "# get some results - do not fail the project\n",
        "# logit you can fit the forward pass and\n",
        "\n",
        "\n",
        "# if you want to switch from inductive and transductive\n",
        "# only modify the loss and not the forward pass\n",
        "\n",
        "# start with inductive.\n",
        "# contibue with transductive.\n",
        "\n",
        "\n",
        "# Use CLRS\n",
        "# --> NAR dataset\n",
        "# --> graph trnsformers\n",
        "\n",
        "\n",
        "# how well GNNs deal with over squashing?\n",
        "\n",
        "\n",
        "\n",
        "# 1. homophilly in the ocntext of transformer and GNN\n",
        "\n",
        "# Graph former\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
