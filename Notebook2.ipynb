{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZxQnwjLYF0pf",
        "4fIhBp5OoJUT",
        "DdXaT4WroJz7",
        "5pwrKTSkqufb",
        "79eCjjVtslUx",
        "UQQwyTH0oKM_",
        "dxlFkxPmoZ-E",
        "t-paXlQjodik"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPYULwPM+k+Vyk5UbUQ86JO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batu-el/l65_be301_dc755/blob/main/Notebook2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ],
      "metadata": {
        "id": "ZxQnwjLYF0pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Outline ###\n",
        "\n",
        "# STEP 1. - Datasets\n",
        "\n",
        "# 1.1 Synthetic Datasets\n",
        "# 1.1.1 Homophilic Node Classification\n",
        "# 1.1.2 Heterophilic Node Classification\n",
        "# 1.1.3 Homophilic Graph Classification\n",
        "# 1.1.4 Heterophilic Graph Classification\n",
        "\n",
        "# 1.2 Real Datasets\n",
        "# 1.2.1 Homophilic Node Classification - Cora\n",
        "# 1.2.2 Heterophilic Node Classification - Texas\n",
        "# 1.2.3 Homophilic Graph Classification - QM9\n",
        "# 1.2.4 Heterophilic Graph Classification - (?)\n",
        "\n",
        "# STEP 2. Models\n",
        "\n",
        "# 2.1 Baselines to Compare Model Accuracies\n",
        "# 2.1.1 GCN\n",
        "# 2.1.2 Sparse Transformer\n",
        "# 2.1.3 MPNN\n",
        "# 2.1.4 Dense Transformer with Attention Mask\n",
        "# 2.1.5 Dense Transformer with Positional Encodings\n",
        "\n",
        "# 2.2 Comparison of 2 Models: Dense (w/ PosEnc) & Sparse Transformer\n",
        "# 2.2.1 1 Head 1 Layer\n",
        "# 2.2.1 4 Head 1 Layer\n",
        "# 2.2.1 1 Head 3 Layer\n",
        "# 2.2.1 4 Head 3 Layer\n",
        "\n",
        "# STEP 3. Evaluation\n",
        "\n",
        "# Comparisons:\n",
        "# A: Adjacency vs Sparse Attention\n",
        "# B: Adjacency vs Dense Attention\n",
        "# C: Sparse Attention vs Dense Attention\n",
        "\n",
        "# 3.1 Combining Multiple Attention Matrices from 2.2\n",
        "# 3.1.1 If Edge Exists\n",
        "# 3.1.2 PCA\n",
        "\n",
        "# 3.2 1D (Vector) Similarity Comparison\n",
        "# 3.2.1 Node Degree (histogram)\n",
        "# 3.2.2 Substructures (histogram)\n",
        "\n",
        "# 3.3 2D (Matrix) Similarity Comparison\n",
        "# 3.3.1 Adjacency Matrix (Graph Edit Dist & Kernel 1 WL)\n",
        "# 3.3.2 Shortest Path (Graph Edit Dist & Kernel 1 WL)\n",
        "\n",
        "# STEP 4. Discussion\n",
        "# Note: Future research can look at how attention evolves over the course of training\n"
      ],
      "metadata": {
        "id": "T8d2kxV5nipq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Missing Packages"
      ],
      "metadata": {
        "id": "4fIhBp5OoJUT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QUsPnPHmx1x"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required python libraries\n",
        "import os\n",
        "\n",
        "# Install PyTorch Geometric and other libraries\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    print(\"Installing PyTorch Geometric\")\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-geometric\n",
        "    print(\"Installing other libraries\")\n",
        "    !pip install networkx\n",
        "    !pip install lovely-tensors"
      ],
      "metadata": {
        "id": "6NRj2Jn1r9Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "\n",
        "from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n",
        "\n",
        "import lovely_tensors as lt\n",
        "lt.monkey_patch()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ],
      "metadata": {
        "id": "1qY20vL3sA80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "from torch_geometric.data import Data"
      ],
      "metadata": {
        "id": "obF13uFLq-wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for deterministic results\n",
        "\n",
        "def seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(0)\n",
        "print(\"All seeds set.\")"
      ],
      "metadata": {
        "id": "bmJzwKgssDko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Data Generation Utils"
      ],
      "metadata": {
        "id": "DdXaT4WroJz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Synthetic Data ###\n",
        "\n",
        "\"\"\"synthetic_structsim.py\n",
        "\n",
        "    Utilities for generating certain graph shapes.\n",
        "\"\"\"\n",
        "import math\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Following GraphWave's representation of structural similarity\n",
        "\n",
        "def clique(start, nb_nodes, nb_to_remove=0, role_start=0):\n",
        "    \"\"\" Defines a clique (complete graph on nb_nodes nodes,\n",
        "    with nb_to_remove  edges that will have to be removed),\n",
        "    index of nodes starting at start\n",
        "    and role_ids at role_start\n",
        "    INPUT:\n",
        "    -------------\n",
        "    start       :    starting index for the shape\n",
        "    nb_nodes    :    int correspondingraph to the nb of nodes in the clique\n",
        "    role_start  :    starting index for the roles\n",
        "    nb_to_remove:    int-- numb of edges to remove (unif at RDM)\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a house shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at\n",
        "                     role_start)\n",
        "    \"\"\"\n",
        "    a = np.ones((nb_nodes, nb_nodes))\n",
        "    np.fill_diagonal(a, 0)\n",
        "    graph = nx.from_numpy_matrix(a)\n",
        "    edge_list = graph.edges().keys()\n",
        "    roles = [role_start] * nb_nodes\n",
        "    if nb_to_remove > 0:\n",
        "        lst = np.random.choice(len(edge_list), nb_to_remove, replace=False)\n",
        "        print(edge_list, lst)\n",
        "        to_delete = [edge_list[e] for e in lst]\n",
        "        graph.remove_edges_from(to_delete)\n",
        "        for e in lst:\n",
        "            print(edge_list[e][0])\n",
        "            print(len(roles))\n",
        "            roles[edge_list[e][0]] += 1\n",
        "            roles[edge_list[e][1]] += 1\n",
        "    mapping_graph = {k: (k + start) for k in range(nb_nodes)}\n",
        "    graph = nx.relabel_nodes(graph, mapping_graph)\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def cycle(start, len_cycle, role_start=0):\n",
        "    \"\"\"Builds a cycle graph, with index of nodes starting at start\n",
        "    and role_ids at role_start\n",
        "    INPUT:\n",
        "    -------------\n",
        "    start       :    starting index for the shape\n",
        "    role_start  :    starting index for the roles\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a house shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at\n",
        "                     role_start)\n",
        "    \"\"\"\n",
        "    graph = nx.Graph()\n",
        "    graph.add_nodes_from(range(start, start + len_cycle))\n",
        "    for i in range(len_cycle - 1):\n",
        "        graph.add_edges_from([(start + i, start + i + 1)])\n",
        "    graph.add_edges_from([(start + len_cycle - 1, start)])\n",
        "    roles = [role_start] * len_cycle\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def diamond(start, role_start=0):\n",
        "    \"\"\"Builds a diamond graph, with index of nodes starting at start\n",
        "    and role_ids at role_start\n",
        "    INPUT:\n",
        "    -------------\n",
        "    start       :    starting index for the shape\n",
        "    role_start  :    starting index for the roles\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a house shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at\n",
        "                     role_start)\n",
        "    \"\"\"\n",
        "    graph = nx.Graph()\n",
        "    graph.add_nodes_from(range(start, start + 6))\n",
        "    graph.add_edges_from(\n",
        "        [\n",
        "            (start, start + 1),\n",
        "            (start + 1, start + 2),\n",
        "            (start + 2, start + 3),\n",
        "            (start + 3, start),\n",
        "        ]\n",
        "    )\n",
        "    graph.add_edges_from(\n",
        "        [\n",
        "            (start + 4, start),\n",
        "            (start + 4, start + 1),\n",
        "            (start + 4, start + 2),\n",
        "            (start + 4, start + 3),\n",
        "        ]\n",
        "    )\n",
        "    graph.add_edges_from(\n",
        "        [\n",
        "            (start + 5, start),\n",
        "            (start + 5, start + 1),\n",
        "            (start + 5, start + 2),\n",
        "            (start + 5, start + 3),\n",
        "        ]\n",
        "    )\n",
        "    roles = [role_start] * 6\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def tree(start, height, r=2, role_start=0):\n",
        "    \"\"\"Builds a balanced r-tree of height h\n",
        "    INPUT:\n",
        "    -------------\n",
        "    start       :    starting index for the shape\n",
        "    height      :    int height of the tree\n",
        "    r           :    int number of branches per node\n",
        "    role_start  :    starting index for the roles\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a tree shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at role_start)\n",
        "    \"\"\"\n",
        "    graph = nx.balanced_tree(r, height)\n",
        "    roles = [0] * graph.number_of_nodes()\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def fan(start, nb_branches, role_start=0):\n",
        "    \"\"\"Builds a fan-like graph, with index of nodes starting at start\n",
        "    and role_ids at role_start\n",
        "    INPUT:\n",
        "    -------------\n",
        "    nb_branches :    int correspondingraph to the nb of fan branches\n",
        "    start       :    starting index for the shape\n",
        "    role_start  :    starting index for the roles\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a house shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at\n",
        "                     role_start)\n",
        "    \"\"\"\n",
        "    graph, roles = star(start, nb_branches, role_start=role_start)\n",
        "    for k in range(1, nb_branches - 1):\n",
        "        roles[k] += 1\n",
        "        roles[k + 1] += 1\n",
        "        graph.add_edges_from([(start + k, start + k + 1)])\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def ba(start, width, role_start=0, m=5):\n",
        "    \"\"\"Builds a BA preferential attachment graph, with index of nodes starting at start\n",
        "    and role_ids at role_start\n",
        "    INPUT:\n",
        "    -------------\n",
        "    start       :    starting index for the shape\n",
        "    width       :    int size of the graph\n",
        "    role_start  :    starting index for the roles\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a house shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at\n",
        "                     role_start)\n",
        "    \"\"\"\n",
        "    graph = nx.barabasi_albert_graph(width, m)\n",
        "    graph.add_nodes_from(range(start, start + width))\n",
        "    nids = sorted(graph)\n",
        "    mapping = {nid: start + i for i, nid in enumerate(nids)}\n",
        "    graph = nx.relabel_nodes(graph, mapping)\n",
        "    roles = [role_start for i in range(width)]\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def house(start, role_start=0):\n",
        "    \"\"\"Builds a house-like  graph, with index of nodes starting at start\n",
        "    and role_ids at role_start\n",
        "    INPUT:\n",
        "    -------------\n",
        "    start       :    starting index for the shape\n",
        "    role_start  :    starting index for the roles\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a house shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at\n",
        "                     role_start)\n",
        "    \"\"\"\n",
        "    graph = nx.Graph()\n",
        "    graph.add_nodes_from(range(start, start + 5))\n",
        "    graph.add_edges_from(\n",
        "        [\n",
        "            (start, start + 1),\n",
        "            (start + 1, start + 2),\n",
        "            (start + 2, start + 3),\n",
        "            (start + 3, start),\n",
        "        ]\n",
        "    )\n",
        "    # graph.add_edges_from([(start, start + 2), (start + 1, start + 3)])\n",
        "    graph.add_edges_from([(start + 4, start), (start + 4, start + 1)])\n",
        "    roles = [role_start, role_start, role_start + 1, role_start + 1, role_start + 2]\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def grid(start, dim=2, role_start=0):\n",
        "    \"\"\" Builds a 2by2 grid\n",
        "    \"\"\"\n",
        "    grid_G = nx.grid_graph([dim, dim])\n",
        "    grid_G = nx.convert_node_labels_to_integers(grid_G, first_label=start)\n",
        "    roles = [role_start for i in grid_G.nodes()]\n",
        "    return grid_G, roles\n",
        "\n",
        "\n",
        "def star(start, nb_branches, role_start=0):\n",
        "    \"\"\"Builds a star graph, with index of nodes starting at start\n",
        "    and role_ids at role_start\n",
        "    INPUT:\n",
        "    -------------\n",
        "    nb_branches :    int correspondingraph to the nb of star branches\n",
        "    start       :    starting index for the shape\n",
        "    role_start  :    starting index for the roles\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a house shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at\n",
        "                     role_start)\n",
        "    \"\"\"\n",
        "    graph = nx.Graph()\n",
        "    graph.add_nodes_from(range(start, start + nb_branches + 1))\n",
        "    for k in range(1, nb_branches + 1):\n",
        "        graph.add_edges_from([(start, start + k)])\n",
        "    roles = [role_start + 1] * (nb_branches + 1)\n",
        "    roles[0] = role_start\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def path(start, width, role_start=0):\n",
        "    \"\"\"Builds a path graph, with index of nodes starting at start\n",
        "    and role_ids at role_start\n",
        "    INPUT:\n",
        "    -------------\n",
        "    start       :    starting index for the shape\n",
        "    width       :    int length of the path\n",
        "    role_start  :    starting index for the roles\n",
        "    OUTPUT:\n",
        "    -------------\n",
        "    graph       :    a house shape graph, with ids beginning at start\n",
        "    roles       :    list of the roles of the nodes (indexed starting at\n",
        "                     role_start)\n",
        "    \"\"\"\n",
        "    graph = nx.Graph()\n",
        "    graph.add_nodes_from(range(start, start + width))\n",
        "    for i in range(width - 1):\n",
        "        graph.add_edges_from([(start + i, start + i + 1)])\n",
        "    roles = [role_start] * width\n",
        "    roles[0] = role_start + 1\n",
        "    roles[-1] = role_start + 1\n",
        "    return graph, roles\n",
        "\n",
        "\n",
        "def build_graph(\n",
        "    width_basis,\n",
        "    basis_type,\n",
        "    list_shapes,\n",
        "    start=0,\n",
        "    rdm_basis_plugins=False,\n",
        "    add_random_edges=0,\n",
        "    m=5,\n",
        "):\n",
        "    \"\"\"This function creates a basis (scale-free, path, or cycle)\n",
        "    and attaches elements of the type in the list randomly along the basis.\n",
        "    Possibility to add random edges afterwards.\n",
        "    INPUT:\n",
        "    --------------------------------------------------------------------------------------\n",
        "    width_basis      :      width (in terms of number of nodes) of the basis\n",
        "    basis_type       :      (torus, string, or cycle)\n",
        "    shapes           :      list of shape list (1st arg: type of shape,\n",
        "                            next args:args for building the shape,\n",
        "                            except for the start)\n",
        "    start            :      initial nb for the first node\n",
        "    rdm_basis_plugins:      boolean. Should the shapes be randomly placed\n",
        "                            along the basis (True) or regularly (False)?\n",
        "    add_random_edges :      nb of edges to randomly add on the structure\n",
        "    m                :      number of edges to attach to existing node (for BA graph)\n",
        "    OUTPUT:\n",
        "    --------------------------------------------------------------------------------------\n",
        "    basis            :      a nx graph with the particular shape\n",
        "    role_ids         :      labels for each role\n",
        "    plugins          :      node ids with the attached shapes\n",
        "    \"\"\"\n",
        "    if basis_type == \"ba\":\n",
        "        basis, role_id = eval(basis_type)(start, width_basis, m=m)\n",
        "    else:\n",
        "        basis, role_id = eval(basis_type)(start, width_basis)\n",
        "\n",
        "    n_basis, n_shapes = nx.number_of_nodes(basis), len(list_shapes)\n",
        "    start += n_basis  # indicator of the id of the next node\n",
        "\n",
        "    # Sample (with replacement) where to attach the new motifs\n",
        "    if rdm_basis_plugins is True:\n",
        "        plugins = np.random.choice(n_basis, n_shapes, replace=False)\n",
        "    else:\n",
        "        spacing = math.floor(n_basis / n_shapes)\n",
        "        plugins = [int(k * spacing) for k in range(n_shapes)]\n",
        "    seen_shapes = {\"basis\": [0, n_basis]}\n",
        "\n",
        "    for shape_id, shape in enumerate(list_shapes):\n",
        "        shape_type = shape[0]\n",
        "        args = [start]\n",
        "        if len(shape) > 1:\n",
        "            args += shape[1:]\n",
        "        args += [0]\n",
        "        graph_s, roles_graph_s = eval(shape_type)(*args)\n",
        "        n_s = nx.number_of_nodes(graph_s)\n",
        "        try:\n",
        "            col_start = seen_shapes[shape_type][0]\n",
        "        except:\n",
        "            col_start = np.max(role_id) + 1\n",
        "            seen_shapes[shape_type] = [col_start, n_s]\n",
        "        # Attach the shape to the basis\n",
        "        basis.add_nodes_from(graph_s.nodes())\n",
        "        basis.add_edges_from(graph_s.edges())\n",
        "        basis.add_edges_from([(start, plugins[shape_id])])\n",
        "        if shape_type == \"cycle\":\n",
        "            if np.random.random() > 0.5:\n",
        "                a = np.random.randint(1, 4)\n",
        "                b = np.random.randint(1, 4)\n",
        "                basis.add_edges_from([(a + start, b + plugins[shape_id])])\n",
        "        temp_labels = [r + col_start for r in roles_graph_s]\n",
        "        # temp_labels[0] += 100 * seen_shapes[shape_type][0]\n",
        "        role_id += temp_labels\n",
        "        start += n_s\n",
        "\n",
        "    if add_random_edges > 0:\n",
        "        # add random edges between nodes:\n",
        "        for p in range(add_random_edges):\n",
        "            src, dest = np.random.choice(nx.number_of_nodes(basis), 2, replace=False)\n",
        "            print(src, dest)\n",
        "            basis.add_edges_from([(src, dest)])\n",
        "\n",
        "    return basis, role_id, plugins"
      ],
      "metadata": {
        "id": "lSbnRJkbpeVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Feature Generation ###\n",
        "\n",
        "\"\"\" featgen.py\n",
        "\n",
        "Node feature generators.\n",
        "\n",
        "\"\"\"\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import abc\n",
        "\n",
        "\n",
        "class FeatureGen(metaclass=abc.ABCMeta):\n",
        "    \"\"\"Feature Generator base class.\"\"\"\n",
        "    @abc.abstractmethod\n",
        "    def gen_node_features(self, G):\n",
        "        pass\n",
        "\n",
        "\n",
        "class ConstFeatureGen(FeatureGen):\n",
        "    \"\"\"Constant Feature class.\"\"\"\n",
        "    def __init__(self, val):\n",
        "        self.val = val\n",
        "\n",
        "    def gen_node_features(self, G):\n",
        "        feat_dict = {i:{'feat': np.array(self.val, dtype=np.float32)} for i in G.nodes()}\n",
        "        print ('feat_dict[0][\"feat\"]:', feat_dict[0]['feat'].dtype)\n",
        "        nx.set_node_attributes(G, feat_dict)\n",
        "        print ('G.nodes[0][\"feat\"]:', G.nodes[0]['feat'].dtype)\n",
        "\n",
        "\n",
        "class GaussianFeatureGen(FeatureGen):\n",
        "    \"\"\"Gaussian Feature class.\"\"\"\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = mu\n",
        "        if sigma.ndim < 2:\n",
        "            self.sigma = np.diag(sigma)\n",
        "        else:\n",
        "            self.sigma = sigma\n",
        "\n",
        "    def gen_node_features(self, G):\n",
        "        feat = np.random.multivariate_normal(self.mu, self.sigma, G.number_of_nodes())\n",
        "        feat_dict = {\n",
        "                i: {\"feat\": feat[i]} for i in range(feat.shape[0])\n",
        "            }\n",
        "        nx.set_node_attributes(G, feat_dict)\n",
        "\n",
        "\n",
        "class GridFeatureGen(FeatureGen):\n",
        "    \"\"\"Grid Feature class.\"\"\"\n",
        "    def __init__(self, mu, sigma, com_choices):\n",
        "        self.mu = mu                    # Mean\n",
        "        self.sigma = sigma              # Variance\n",
        "        self.com_choices = com_choices  # List of possible community labels\n",
        "\n",
        "    def gen_node_features(self, G):\n",
        "        # Generate community assignment\n",
        "        community_dict = {\n",
        "            n: self.com_choices[0] if G.degree(n) < 4 else self.com_choices[1]\n",
        "            for n in G.nodes()\n",
        "        }\n",
        "\n",
        "        # Generate random variable\n",
        "        s = np.random.normal(self.mu, self.sigma, G.number_of_nodes())\n",
        "\n",
        "        # Generate features\n",
        "        feat_dict = {\n",
        "            n: {\"feat\": np.asarray([community_dict[n], s[i]])}\n",
        "            for i, n in enumerate(G.nodes())\n",
        "        }\n",
        "\n",
        "        nx.set_node_attributes(G, feat_dict)\n",
        "        return community_dict"
      ],
      "metadata": {
        "id": "gnXV4akvqd2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### IO Utils ###\n",
        "\n",
        "\"\"\" io_utils.py\n",
        "\n",
        "    Utilities for reading and writing logs.\n",
        "\"\"\"\n",
        "import os\n",
        "import statistics\n",
        "import re\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sc\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import networkx as nx\n",
        "import tensorboardX\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Only necessary to rebuild the Chemistry example\n",
        "# from rdkit import Chem\n",
        "\n",
        "# import utils.featgen as featgen\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "def gen_prefix(args):\n",
        "    '''Generate label prefix for a graph model.\n",
        "    '''\n",
        "    if args.bmname is not None:\n",
        "        name = args.bmname\n",
        "    else:\n",
        "        name = args.dataset\n",
        "    name += \"_\" + args.method\n",
        "\n",
        "    name += \"_h\" + str(args.hidden_dim) + \"_o\" + str(args.output_dim)\n",
        "    if not args.bias:\n",
        "        name += \"_nobias\"\n",
        "    if len(args.name_suffix) > 0:\n",
        "        name += \"_\" + args.name_suffix\n",
        "    return name\n",
        "\n",
        "\n",
        "def gen_explainer_prefix(args):\n",
        "    '''Generate label prefix for a graph explainer model.\n",
        "    '''\n",
        "    name = gen_prefix(args) + \"_explain\"\n",
        "    if len(args.explainer_suffix) > 0:\n",
        "        name += \"_\" + args.explainer_suffix\n",
        "    return name\n",
        "\n",
        "\n",
        "def create_filename(save_dir, args, isbest=False, num_epochs=-1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        args        :  the arguments parsed in the parser\n",
        "        isbest      :  whether the saved model is the best-performing one\n",
        "        num_epochs  :  epoch number of the model (when isbest=False)\n",
        "    \"\"\"\n",
        "    filename = os.path.join(save_dir, gen_prefix(args))\n",
        "    os.makedirs(filename, exist_ok=True)\n",
        "\n",
        "    if isbest:\n",
        "        filename = os.path.join(filename, \"best\")\n",
        "    elif num_epochs > 0:\n",
        "        filename = os.path.join(filename, str(num_epochs))\n",
        "\n",
        "    return filename + \".pth.tar\"\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, args, num_epochs=-1, isbest=False, cg_dict=None):\n",
        "    \"\"\"Save pytorch model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        - model         : The PyTorch model to save.\n",
        "        - optimizer     : The optimizer used to train the model.\n",
        "        - args          : A dict of meta-data about the model.\n",
        "        - num_epochs    : Number of training epochs.\n",
        "        - isbest        : True if the model has the highest accuracy so far.\n",
        "        - cg_dict       : A dictionary of the sampled computation graphs.\n",
        "    \"\"\"\n",
        "    filename = create_filename(args.ckptdir, args, isbest, num_epochs=num_epochs)\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": num_epochs,\n",
        "            \"model_type\": args.method,\n",
        "            \"optimizer\": optimizer,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"cg\": cg_dict,\n",
        "        },\n",
        "        filename,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_ckpt(args, isbest=False):\n",
        "    '''Load a pre-trained pytorch model from checkpoint.\n",
        "    '''\n",
        "    print(\"loading model\")\n",
        "    filename = create_filename(args.ckptdir, args, isbest)\n",
        "    print(filename)\n",
        "    if os.path.isfile(filename):\n",
        "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "        ckpt = torch.load(filename)\n",
        "    else:\n",
        "        print(\"Checkpoint does not exist!\")\n",
        "        print(\"Checked path -- {}\".format(filename))\n",
        "        print(\"Make sure you have provided the correct path!\")\n",
        "        print(\"You may have forgotten to train a model for this dataset.\")\n",
        "        print()\n",
        "        print(\"To train one of the paper's models, run the following\")\n",
        "        print(\">> python train.py --dataset=DATASET_NAME\")\n",
        "        print()\n",
        "        raise Exception(\"File not found.\")\n",
        "    return ckpt\n",
        "\n",
        "def preprocess_cg(cg):\n",
        "    \"\"\"Pre-process computation graph.\"\"\"\n",
        "    if use_cuda:\n",
        "        preprocessed_cg_tensor = torch.from_numpy(cg).cuda()\n",
        "    else:\n",
        "        preprocessed_cg_tensor = torch.from_numpy(cg)\n",
        "\n",
        "    preprocessed_cg_tensor.unsqueeze_(0)\n",
        "    return Variable(preprocessed_cg_tensor, requires_grad=False)\n",
        "\n",
        "def load_model(path):\n",
        "    \"\"\"Load a pytorch model.\"\"\"\n",
        "    model = torch.load(path)\n",
        "    model.eval()\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    for p in model.features.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_cg(path):\n",
        "    \"\"\"Load a computation graph.\"\"\"\n",
        "    cg = pickle.load(open(path))\n",
        "    return cg\n",
        "\n",
        "\n",
        "def save(mask_cg):\n",
        "    \"\"\"Save a rendering of the computation graph mask.\"\"\"\n",
        "    mask = mask_cg.cpu().data.numpy()[0]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "\n",
        "    mask = (mask - np.min(mask)) / np.max(mask)\n",
        "    mask = 1 - mask\n",
        "\n",
        "    cv2.imwrite(\"mask.png\", np.uint8(255 * mask))\n",
        "\n",
        "def log_matrix(writer, mat, name, epoch, fig_size=(8, 6), dpi=200):\n",
        "    \"\"\"Save an image of a matrix to disk.\n",
        "\n",
        "    Args:\n",
        "        - writer    :  A file writer.\n",
        "        - mat       :  The matrix to write.\n",
        "        - name      :  Name of the file to save.\n",
        "        - epoch     :  Epoch number.\n",
        "        - fig_size  :  Size to of the figure to save.\n",
        "        - dpi       :  Resolution.\n",
        "    \"\"\"\n",
        "    plt.switch_backend(\"agg\")\n",
        "    fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
        "    mat = mat.cpu().detach().numpy()\n",
        "    if mat.ndim == 1:\n",
        "        mat = mat[:, np.newaxis]\n",
        "    plt.imshow(mat, cmap=plt.get_cmap(\"BuPu\"))\n",
        "    cbar = plt.colorbar()\n",
        "    cbar.solids.set_edgecolor(\"face\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "    writer.add_image(name, tensorboardX.utils.figure_to_image(fig), epoch)\n",
        "\n",
        "\n",
        "def denoise_graph(adj, node_idx, feat=None, label=None, threshold=None, threshold_num=None, max_component=True):\n",
        "    \"\"\"Cleaning a graph by thresholding its node values.\n",
        "\n",
        "    Args:\n",
        "        - adj               :  Adjacency matrix.\n",
        "        - node_idx          :  Index of node to highlight (TODO ?)\n",
        "        - feat              :  An array of node features.\n",
        "        - label             :  A list of node labels.\n",
        "        - threshold         :  The weight threshold.\n",
        "        - theshold_num      :  The maximum number of nodes to threshold.\n",
        "        - max_component     :  TODO\n",
        "    \"\"\"\n",
        "    num_nodes = adj.shape[-1]\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(num_nodes))\n",
        "    G.nodes[node_idx][\"self\"] = 1\n",
        "    if feat is not None:\n",
        "        for node in G.nodes():\n",
        "            G.nodes[node][\"feat\"] = feat[node]\n",
        "    if label is not None:\n",
        "        for node in G.nodes():\n",
        "            G.nodes[node][\"label\"] = label[node]\n",
        "\n",
        "    if threshold_num is not None:\n",
        "        # this is for symmetric graphs: edges are repeated twice in adj\n",
        "        adj_threshold_num = threshold_num * 2\n",
        "        #adj += np.random.rand(adj.shape[0], adj.shape[1]) * 1e-4\n",
        "        neigh_size = len(adj[adj > 0])\n",
        "        threshold_num = min(neigh_size, adj_threshold_num)\n",
        "        threshold = np.sort(adj[adj > 0])[-threshold_num]\n",
        "\n",
        "    if threshold is not None:\n",
        "        weighted_edge_list = [\n",
        "            (i, j, adj[i, j])\n",
        "            for i in range(num_nodes)\n",
        "            for j in range(num_nodes)\n",
        "            if adj[i, j] >= threshold\n",
        "        ]\n",
        "    else:\n",
        "        weighted_edge_list = [\n",
        "            (i, j, adj[i, j])\n",
        "            for i in range(num_nodes)\n",
        "            for j in range(num_nodes)\n",
        "            if adj[i, j] > 1e-6\n",
        "        ]\n",
        "    G.add_weighted_edges_from(weighted_edge_list)\n",
        "    if max_component:\n",
        "        largest_cc = max(nx.connected_components(G), key=len)\n",
        "        G = G.subgraph(largest_cc).copy()\n",
        "    else:\n",
        "        # remove zero degree nodes\n",
        "        G.remove_nodes_from(list(nx.isolates(G)))\n",
        "    return G\n",
        "\n",
        "# TODO: unify log_graph and log_graph2\n",
        "def log_graph(\n",
        "    writer,\n",
        "    Gc,\n",
        "    name,\n",
        "    identify_self=True,\n",
        "    nodecolor=\"label\",\n",
        "    epoch=0,\n",
        "    fig_size=(4, 3),\n",
        "    dpi=300,\n",
        "    label_node_feat=False,\n",
        "    edge_vmax=None,\n",
        "    args=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        nodecolor: the color of node, can be determined by 'label', or 'feat'. For feat, it needs to\n",
        "            be one-hot'\n",
        "    \"\"\"\n",
        "    cmap = plt.get_cmap(\"Set1\")\n",
        "    plt.switch_backend(\"agg\")\n",
        "    fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
        "\n",
        "    node_colors = []\n",
        "    # edge_colors = [min(max(w, 0.0), 1.0) for (u,v,w) in Gc.edges.data('weight', default=1)]\n",
        "    edge_colors = [w for (u, v, w) in Gc.edges.data(\"weight\", default=1)]\n",
        "\n",
        "    # maximum value for node color\n",
        "    vmax = 8\n",
        "    for i in Gc.nodes():\n",
        "        if nodecolor == \"feat\" and \"feat\" in Gc.nodes[i]:\n",
        "            num_classes = Gc.nodes[i][\"feat\"].size()[0]\n",
        "            if num_classes >= 10:\n",
        "                cmap = plt.get_cmap(\"tab20\")\n",
        "                vmax = 19\n",
        "            elif num_classes >= 8:\n",
        "                cmap = plt.get_cmap(\"tab10\")\n",
        "                vmax = 9\n",
        "            break\n",
        "\n",
        "    feat_labels = {}\n",
        "    for i in Gc.nodes():\n",
        "        if identify_self and \"self\" in Gc.nodes[i]:\n",
        "            node_colors.append(0)\n",
        "        elif nodecolor == \"label\" and \"label\" in Gc.nodes[i]:\n",
        "            node_colors.append(Gc.nodes[i][\"label\"] + 1)\n",
        "        elif nodecolor == \"feat\" and \"feat\" in Gc.nodes[i]:\n",
        "            # print(Gc.nodes[i]['feat'])\n",
        "            feat = Gc.nodes[i][\"feat\"].detach().numpy()\n",
        "            # idx with pos val in 1D array\n",
        "            feat_class = 0\n",
        "            for j in range(len(feat)):\n",
        "                if feat[j] == 1:\n",
        "                    feat_class = j\n",
        "                    break\n",
        "            node_colors.append(feat_class)\n",
        "            feat_labels[i] = feat_class\n",
        "        else:\n",
        "            node_colors.append(1)\n",
        "    if not label_node_feat:\n",
        "        feat_labels = None\n",
        "\n",
        "    plt.switch_backend(\"agg\")\n",
        "    fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
        "\n",
        "    if Gc.number_of_nodes() == 0:\n",
        "        raise Exception(\"empty graph\")\n",
        "    if Gc.number_of_edges() == 0:\n",
        "        raise Exception(\"empty edge\")\n",
        "    # remove_nodes = []\n",
        "    # for u in Gc.nodes():\n",
        "    #    if Gc\n",
        "    pos_layout = nx.kamada_kawai_layout(Gc, weight=None)\n",
        "    # pos_layout = nx.spring_layout(Gc, weight=None)\n",
        "\n",
        "    weights = [d for (u, v, d) in Gc.edges(data=\"weight\", default=1)]\n",
        "    if edge_vmax is None:\n",
        "        edge_vmax = statistics.median_high(\n",
        "            [d for (u, v, d) in Gc.edges(data=\"weight\", default=1)]\n",
        "        )\n",
        "    min_color = min([d for (u, v, d) in Gc.edges(data=\"weight\", default=1)])\n",
        "    # color range: gray to black\n",
        "    edge_vmin = 2 * min_color - edge_vmax\n",
        "    nx.draw(\n",
        "        Gc,\n",
        "        pos=pos_layout,\n",
        "        with_labels=False,\n",
        "        font_size=4,\n",
        "        labels=feat_labels,\n",
        "        node_color=node_colors,\n",
        "        vmin=0,\n",
        "        vmax=vmax,\n",
        "        cmap=cmap,\n",
        "        edge_color=edge_colors,\n",
        "        edge_cmap=plt.get_cmap(\"Greys\"),\n",
        "        edge_vmin=edge_vmin,\n",
        "        edge_vmax=edge_vmax,\n",
        "        width=1.0,\n",
        "        node_size=50,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "    fig.axes[0].xaxis.set_visible(False)\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    logdir = \"log\" if not hasattr(args, \"logdir\") or not args.logdir else str(args.logdir)\n",
        "    if nodecolor != \"feat\":\n",
        "        name += gen_explainer_prefix(args)\n",
        "    save_path = os.path.join(logdir, name  + \"_\" + str(epoch) + \".pdf\")\n",
        "    print(logdir + \"/\" + name + gen_explainer_prefix(args) + \"_\" + str(epoch) + \".pdf\")\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    plt.savefig(save_path, format=\"pdf\")\n",
        "\n",
        "    img = tensorboardX.utils.figure_to_image(fig)\n",
        "    writer.add_image(name, img, epoch)\n",
        "\n",
        "\n",
        "def plot_cmap(cmap, ncolor):\n",
        "    \"\"\"\n",
        "    A convenient function to plot colors of a matplotlib cmap\n",
        "    Credit goes to http://gvallver.perso.univ-pau.fr/?p=712\n",
        "\n",
        "    Args:\n",
        "        ncolor (int): number of color to show\n",
        "        cmap: a cmap object or a matplotlib color name\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(cmap, str):\n",
        "        name = cmap\n",
        "        try:\n",
        "            cm = plt.get_cmap(cmap)\n",
        "        except ValueError:\n",
        "            print(\"WARNINGS :\", cmap, \" is not a known colormap\")\n",
        "            cm = plt.cm.gray\n",
        "    else:\n",
        "        cm = cmap\n",
        "        name = cm.name\n",
        "\n",
        "    with matplotlib.rc_context(matplotlib.rcParamsDefault):\n",
        "        fig = plt.figure(figsize=(12, 1), frameon=False)\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.pcolor(np.linspace(1, ncolor, ncolor).reshape(1, ncolor), cmap=cm)\n",
        "        ax.set_title(name)\n",
        "        xt = ax.set_xticks([])\n",
        "        yt = ax.set_yticks([])\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_cmap_tb(writer, cmap, ncolor, name):\n",
        "    \"\"\"Plot the color map used for plot.\"\"\"\n",
        "    fig = plot_cmap(cmap, ncolor)\n",
        "    img = tensorboardX.utils.figure_to_image(fig)\n",
        "    writer.add_image(name, img, 0)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
        "    )\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "def numpy_to_torch(img, requires_grad=True):\n",
        "    if len(img.shape) < 3:\n",
        "        output = np.float32([img])\n",
        "    else:\n",
        "        output = np.transpose(img, (2, 0, 1))\n",
        "\n",
        "    output = torch.from_numpy(output)\n",
        "    if use_cuda:\n",
        "        output = output.cuda()\n",
        "\n",
        "    output.unsqueeze_(0)\n",
        "    v = Variable(output, requires_grad=requires_grad)\n",
        "    return v\n",
        "\n",
        "\n",
        "def read_graphfile(datadir, dataname, max_nodes=None, edge_labels=False):\n",
        "    \"\"\" Read data from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n",
        "        graph index starts with 1 in file\n",
        "\n",
        "    Returns:\n",
        "        List of networkx objects with graph and node labels\n",
        "    \"\"\"\n",
        "    prefix = os.path.join(datadir, dataname, dataname)\n",
        "    filename_graph_indic = prefix + \"_graph_indicator.txt\"\n",
        "    # index of graphs that a given node belongs to\n",
        "    graph_indic = {}\n",
        "    with open(filename_graph_indic) as f:\n",
        "        i = 1\n",
        "        for line in f:\n",
        "            line = line.strip(\"\\n\")\n",
        "            graph_indic[i] = int(line)\n",
        "            i += 1\n",
        "\n",
        "    filename_nodes = prefix + \"_node_labels.txt\"\n",
        "    node_labels = []\n",
        "    min_label_val = None\n",
        "    try:\n",
        "        with open(filename_nodes) as f:\n",
        "            for line in f:\n",
        "                line = line.strip(\"\\n\")\n",
        "                l = int(line)\n",
        "                node_labels += [l]\n",
        "                if min_label_val is None or min_label_val > l:\n",
        "                    min_label_val = l\n",
        "        # assume that node labels are consecutive\n",
        "        num_unique_node_labels = max(node_labels) - min_label_val + 1\n",
        "        node_labels = [l - min_label_val for l in node_labels]\n",
        "    except IOError:\n",
        "        print(\"No node labels\")\n",
        "\n",
        "    filename_node_attrs = prefix + \"_node_attributes.txt\"\n",
        "    node_attrs = []\n",
        "    try:\n",
        "        with open(filename_node_attrs) as f:\n",
        "            for line in f:\n",
        "                line = line.strip(\"\\s\\n\")\n",
        "                attrs = [\n",
        "                    float(attr) for attr in re.split(\"[,\\s]+\", line) if not attr == \"\"\n",
        "                ]\n",
        "                node_attrs.append(np.array(attrs))\n",
        "    except IOError:\n",
        "        print(\"No node attributes\")\n",
        "\n",
        "    label_has_zero = False\n",
        "    filename_graphs = prefix + \"_graph_labels.txt\"\n",
        "    graph_labels = []\n",
        "\n",
        "    label_vals = []\n",
        "    with open(filename_graphs) as f:\n",
        "        for line in f:\n",
        "            line = line.strip(\"\\n\")\n",
        "            val = int(line)\n",
        "            if val not in label_vals:\n",
        "                label_vals.append(val)\n",
        "            graph_labels.append(val)\n",
        "\n",
        "    label_map_to_int = {val: i for i, val in enumerate(label_vals)}\n",
        "    graph_labels = np.array([label_map_to_int[l] for l in graph_labels])\n",
        "\n",
        "    if edge_labels:\n",
        "        # For Tox21_AHR we want to know edge labels\n",
        "        filename_edges = prefix + \"_edge_labels.txt\"\n",
        "        edge_labels = []\n",
        "\n",
        "        edge_label_vals = []\n",
        "        with open(filename_edges) as f:\n",
        "            for line in f:\n",
        "                line = line.strip(\"\\n\")\n",
        "                val = int(line)\n",
        "                if val not in edge_label_vals:\n",
        "                    edge_label_vals.append(val)\n",
        "                edge_labels.append(val)\n",
        "\n",
        "        edge_label_map_to_int = {val: i for i, val in enumerate(edge_label_vals)}\n",
        "\n",
        "    filename_adj = prefix + \"_A.txt\"\n",
        "    adj_list = {i: [] for i in range(1, len(graph_labels) + 1)}\n",
        "    # edge_label_list={i:[] for i in range(1,len(graph_labels)+1)}\n",
        "    index_graph = {i: [] for i in range(1, len(graph_labels) + 1)}\n",
        "    num_edges = 0\n",
        "    with open(filename_adj) as f:\n",
        "        for line in f:\n",
        "            line = line.strip(\"\\n\").split(\",\")\n",
        "            e0, e1 = (int(line[0].strip(\" \")), int(line[1].strip(\" \")))\n",
        "            adj_list[graph_indic[e0]].append((e0, e1))\n",
        "            index_graph[graph_indic[e0]] += [e0, e1]\n",
        "            # edge_label_list[graph_indic[e0]].append(edge_labels[num_edges])\n",
        "            num_edges += 1\n",
        "    for k in index_graph.keys():\n",
        "        index_graph[k] = [u - 1 for u in set(index_graph[k])]\n",
        "\n",
        "    graphs = []\n",
        "    for i in range(1, 1 + len(adj_list)):\n",
        "        # indexed from 1 here\n",
        "        G = nx.from_edgelist(adj_list[i])\n",
        "\n",
        "        if max_nodes is not None and G.number_of_nodes() > max_nodes:\n",
        "            continue\n",
        "\n",
        "        # add features and labels\n",
        "        G.graph[\"label\"] = graph_labels[i - 1]\n",
        "\n",
        "        # Special label for aromaticity experiment\n",
        "        # aromatic_edge = 2\n",
        "        # G.graph['aromatic'] = aromatic_edge in edge_label_list[i]\n",
        "\n",
        "        for u in G.nodes():\n",
        "            if len(node_labels) > 0:\n",
        "                node_label_one_hot = [0] * num_unique_node_labels\n",
        "                node_label = node_labels[u - 1]\n",
        "                node_label_one_hot[node_label] = 1\n",
        "                G.nodes[u][\"label\"] = node_label_one_hot\n",
        "            if len(node_attrs) > 0:\n",
        "                G.nodes[u][\"feat\"] = node_attrs[u - 1]\n",
        "        if len(node_attrs) > 0:\n",
        "            G.graph[\"feat_dim\"] = node_attrs[0].shape[0]\n",
        "\n",
        "        # relabeling\n",
        "        mapping = {}\n",
        "        it = 0\n",
        "        if float(nx.__version__) < 2.0:\n",
        "            for n in G.nodes():\n",
        "                mapping[n] = it\n",
        "                it += 1\n",
        "        else:\n",
        "            for n in G.nodes:\n",
        "                mapping[n] = it\n",
        "                it += 1\n",
        "\n",
        "        # indexed from 0\n",
        "        graphs.append(nx.relabel_nodes(G, mapping))\n",
        "    return graphs\n",
        "\n",
        "\n",
        "def read_biosnap(datadir, edgelist_file, label_file, feat_file=None, concat=True):\n",
        "    \"\"\" Read data from BioSnap\n",
        "\n",
        "    Returns:\n",
        "        List of networkx objects with graph and node labels\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    delimiter = \"\\t\" if \"tsv\" in edgelist_file else \",\"\n",
        "    print(delimiter)\n",
        "    df = pd.read_csv(\n",
        "        os.path.join(datadir, edgelist_file), delimiter=delimiter, header=None\n",
        "    )\n",
        "    data = list(map(tuple, df.values.tolist()))\n",
        "    G.add_edges_from(data)\n",
        "    print(\"Total nodes: \", G.number_of_nodes())\n",
        "\n",
        "    G = max(nx.connected_component_subgraphs(G), key=len)\n",
        "    print(\"Total nodes in largest connected component: \", G.number_of_nodes())\n",
        "\n",
        "    df = pd.read_csv(os.path.join(datadir, label_file), delimiter=\"\\t\", usecols=[0, 1])\n",
        "    data = list(map(tuple, df.values.tolist()))\n",
        "\n",
        "    missing_node = 0\n",
        "    for line in data:\n",
        "        if int(line[0]) not in G:\n",
        "            missing_node += 1\n",
        "        else:\n",
        "            G.nodes[int(line[0])][\"label\"] = int(line[1] == \"Essential\")\n",
        "\n",
        "    print(\"missing node: \", missing_node)\n",
        "\n",
        "    missing_label = 0\n",
        "    remove_nodes = []\n",
        "    for u in G.nodes():\n",
        "        if \"label\" not in G.nodes[u]:\n",
        "            missing_label += 1\n",
        "            remove_nodes.append(u)\n",
        "    G.remove_nodes_from(remove_nodes)\n",
        "    print(\"missing_label: \", missing_label)\n",
        "\n",
        "    if feat_file is None:\n",
        "        feature_generator = ConstFeatureGen(np.ones(10, dtype=float))\n",
        "        feature_generator.gen_node_features(G)\n",
        "    else:\n",
        "        df = pd.read_csv(os.path.join(datadir, feat_file), delimiter=\",\")\n",
        "        data = np.array(df.values)\n",
        "        print(\"Feat shape: \", data.shape)\n",
        "\n",
        "        for row in data:\n",
        "            if int(row[0]) in G:\n",
        "                if concat:\n",
        "                    node = int(row[0])\n",
        "                    onehot = np.zeros(10)\n",
        "                    onehot[min(G.degree[node], 10) - 1] = 1.0\n",
        "                    G.nodes[node][\"feat\"] = np.hstack(\n",
        "                        (np.log(row[1:] + 0.1), [1.0], onehot)\n",
        "                    )\n",
        "                else:\n",
        "                    G.nodes[int(row[0])][\"feat\"] = np.log(row[1:] + 0.1)\n",
        "\n",
        "        missing_feat = 0\n",
        "        remove_nodes = []\n",
        "        for u in G.nodes():\n",
        "            if \"feat\" not in G.nodes[u]:\n",
        "                missing_feat += 1\n",
        "                remove_nodes.append(u)\n",
        "        G.remove_nodes_from(remove_nodes)\n",
        "        print(\"missing feat: \", missing_feat)\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def build_aromaticity_dataset():\n",
        "    filename = \"data/tox21_10k_data_all.sdf\"\n",
        "    basename = filename.split(\".\")[0]\n",
        "    collector = []\n",
        "    sdprovider = Chem.SDMolSupplier(filename)\n",
        "    for i,mol in enumerate(sdprovider):\n",
        "        try:\n",
        "            moldict = {}\n",
        "            moldict['smiles'] = Chem.MolToSmiles(mol)\n",
        "            #Parse Data\n",
        "            for propname in mol.GetPropNames():\n",
        "                moldict[propname] = mol.GetProp(propname)\n",
        "            nb_bonds = len(mol.GetBonds())\n",
        "            is_aromatic = False; aromatic_bonds = []\n",
        "            for j in range(nb_bonds):\n",
        "                if mol.GetBondWithIdx(j).GetIsAromatic():\n",
        "                    aromatic_bonds.append(j)\n",
        "                    is_aromatic = True\n",
        "            moldict['aromaticity'] = is_aromatic\n",
        "            moldict['aromatic_bonds'] = aromatic_bonds\n",
        "            collector.append(moldict)\n",
        "        except:\n",
        "            print(\"Molecule %s failed\"%i)\n",
        "    data = pd.DataFrame(collector)\n",
        "    data.to_csv(basename + '_pandas.csv')\n",
        "\n",
        "\n",
        "def gen_train_plt_name(args):\n",
        "    return \"results/\" + gen_prefix(args) + \".png\"\n",
        "\n",
        "\n",
        "def log_assignment(assign_tensor, writer, epoch, batch_idx):\n",
        "    plt.switch_backend(\"agg\")\n",
        "    fig = plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "    # has to be smaller than args.batch_size\n",
        "    for i in range(len(batch_idx)):\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.imshow(\n",
        "            assign_tensor.cpu().data.numpy()[batch_idx[i]], cmap=plt.get_cmap(\"BuPu\")\n",
        "        )\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.solids.set_edgecolor(\"face\")\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    writer.add_image(\"assignment\", data, epoch)\n",
        "\n",
        "# TODO: unify log_graph and log_graph2\n",
        "def log_graph2(adj, batch_num_nodes, writer, epoch, batch_idx, assign_tensor=None):\n",
        "    plt.switch_backend(\"agg\")\n",
        "    fig = plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "    for i in range(len(batch_idx)):\n",
        "        ax = plt.subplot(2, 2, i + 1)\n",
        "        num_nodes = batch_num_nodes[batch_idx[i]]\n",
        "        adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
        "        G = nx.from_numpy_matrix(adj_matrix)\n",
        "        nx.draw(\n",
        "            G,\n",
        "            pos=nx.spring_layout(G),\n",
        "            with_labels=True,\n",
        "            node_color=\"#336699\",\n",
        "            edge_color=\"grey\",\n",
        "            width=0.5,\n",
        "            node_size=300,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "        ax.xaxis.set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    writer.add_image(\"graphs\", data, epoch)\n",
        "\n",
        "    # log a label-less version\n",
        "    # fig = plt.figure(figsize=(8,6), dpi=300)\n",
        "    # for i in range(len(batch_idx)):\n",
        "    #    ax = plt.subplot(2, 2, i+1)\n",
        "    #    num_nodes = batch_num_nodes[batch_idx[i]]\n",
        "    #    adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
        "    #    G = nx.from_numpy_matrix(adj_matrix)\n",
        "    #    nx.draw(G, pos=nx.spring_layout(G), with_labels=False, node_color='#336699',\n",
        "    #            edge_color='grey', width=0.5, node_size=25,\n",
        "    #            alpha=0.8)\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    # fig.canvas.draw()\n",
        "\n",
        "    # data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "    # data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    # writer.add_image('graphs_no_label', data, epoch)\n",
        "\n",
        "    # colored according to assignment\n",
        "    assignment = assign_tensor.cpu().data.numpy()\n",
        "    fig = plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "    num_clusters = assignment.shape[2]\n",
        "    all_colors = np.array(range(num_clusters))\n",
        "\n",
        "    for i in range(len(batch_idx)):\n",
        "        ax = plt.subplot(2, 2, i + 1)\n",
        "        num_nodes = batch_num_nodes[batch_idx[i]]\n",
        "        adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
        "\n",
        "        label = np.argmax(assignment[batch_idx[i]], axis=1).astype(int)\n",
        "        label = label[: batch_num_nodes[batch_idx[i]]]\n",
        "        node_colors = all_colors[label]\n",
        "\n",
        "        G = nx.from_numpy_matrix(adj_matrix)\n",
        "        nx.draw(\n",
        "            G,\n",
        "            pos=nx.spring_layout(G),\n",
        "            with_labels=False,\n",
        "            node_color=node_colors,\n",
        "            edge_color=\"grey\",\n",
        "            width=0.4,\n",
        "            node_size=50,\n",
        "            cmap=plt.get_cmap(\"Set1\"),\n",
        "            vmin=0,\n",
        "            vmax=num_clusters - 1,\n",
        "            alpha=0.8,\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    writer.add_image(\"graphs_colored\", data, epoch)"
      ],
      "metadata": {
        "id": "h4hOuuFeqgvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate Graphs ###\n",
        "\n",
        "\"\"\"gengraph.py\n",
        "\n",
        "   Generating and manipulaton the synthetic graphs needed for the paper's experiments.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from matplotlib.figure import Figure\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "# Set matplotlib backend to file writing\n",
        "plt.switch_backend(\"agg\")\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# from utils import synthetic_structsim\n",
        "# from utils import featgen\n",
        "# import utils.io_utils as io_utils\n",
        "\n",
        "\n",
        "####################################\n",
        "#\n",
        "# Experiment utilities\n",
        "#\n",
        "####################################\n",
        "def perturb(graph_list, p):\n",
        "    \"\"\" Perturb the list of (sparse) graphs by adding/removing edges.\n",
        "    Args:\n",
        "        p: proportion of added edges based on current number of edges.\n",
        "    Returns:\n",
        "        A list of graphs that are perturbed from the original graphs.\n",
        "    \"\"\"\n",
        "    perturbed_graph_list = []\n",
        "    for G_original in graph_list:\n",
        "        G = G_original.copy()\n",
        "        edge_count = int(G.number_of_edges() * p)\n",
        "        # randomly add the edges between a pair of nodes without an edge.\n",
        "        for _ in range(edge_count):\n",
        "            while True:\n",
        "                u = np.random.randint(0, G.number_of_nodes())\n",
        "                v = np.random.randint(0, G.number_of_nodes())\n",
        "                if (not G.has_edge(u, v)) and (u != v):\n",
        "                    break\n",
        "            G.add_edge(u, v)\n",
        "        perturbed_graph_list.append(G)\n",
        "    return perturbed_graph_list\n",
        "\n",
        "\n",
        "def join_graph(G1, G2, n_pert_edges):\n",
        "    \"\"\" Join two graphs along matching nodes, then perturb the resulting graph.\n",
        "    Args:\n",
        "        G1, G2: Networkx graphs to be joined.\n",
        "        n_pert_edges: number of perturbed edges.\n",
        "    Returns:\n",
        "        A new graph, result of merging and perturbing G1 and G2.\n",
        "    \"\"\"\n",
        "    assert n_pert_edges > 0\n",
        "    F = nx.compose(G1, G2)\n",
        "    edge_cnt = 0\n",
        "    while edge_cnt < n_pert_edges:\n",
        "        node_1 = np.random.choice(G1.nodes())\n",
        "        node_2 = np.random.choice(G2.nodes())\n",
        "        F.add_edge(node_1, node_2)\n",
        "        edge_cnt += 1\n",
        "    return F\n",
        "\n",
        "\n",
        "def preprocess_input_graph(G, labels, normalize_adj=False):\n",
        "    \"\"\" Load an existing graph to be converted for the experiments.\n",
        "    Args:\n",
        "        G: Networkx graph to be loaded.\n",
        "        labels: Associated node labels.\n",
        "        normalize_adj: Should the method return a normalized adjacency matrix.\n",
        "    Returns:\n",
        "        A dictionary containing adjacency, node features and labels\n",
        "    \"\"\"\n",
        "    adj = np.array(nx.to_numpy_matrix(G))\n",
        "    if normalize_adj:\n",
        "        sqrt_deg = np.diag(1.0 / np.sqrt(np.sum(adj, axis=0, dtype=float).squeeze()))\n",
        "        adj = np.matmul(np.matmul(sqrt_deg, adj), sqrt_deg)\n",
        "\n",
        "    existing_node = list(G.nodes)[-1]\n",
        "    feat_dim = G.nodes[existing_node][\"feat\"].shape[0]\n",
        "    f = np.zeros((G.number_of_nodes(), feat_dim), dtype=float)\n",
        "    for i, u in enumerate(G.nodes()):\n",
        "        f[i, :] = G.nodes[u][\"feat\"]\n",
        "\n",
        "    # add batch dim\n",
        "    adj = np.expand_dims(adj, axis=0)\n",
        "    f = np.expand_dims(f, axis=0)\n",
        "    labels = np.expand_dims(labels, axis=0)\n",
        "    return {\"adj\": adj, \"feat\": f, \"labels\": labels}\n",
        "\n",
        "\n",
        "####################################\n",
        "#\n",
        "# Generating synthetic graphs\n",
        "#\n",
        "###################################\n",
        "def gen_syn1(nb_shapes=80, width_basis=300, feature_generator=None, m=5):\n",
        "    \"\"\" Synthetic Graph #1:\n",
        "\n",
        "    Start with Barabasi-Albert graph and attach house-shaped subgraphs.\n",
        "\n",
        "    Args:\n",
        "        nb_shapes         :  The number of shapes (here 'houses') that should be added to the base graph.\n",
        "        width_basis       :  The width of the basis graph (here 'Barabasi-Albert' random graph).\n",
        "        feature_generator :  A `FeatureGenerator` for node features. If `None`, add constant features to nodes.\n",
        "        m                 :  number of edges to attach to existing node (for BA graph)\n",
        "\n",
        "    Returns:\n",
        "        G                 :  A networkx graph\n",
        "        role_id           :  A list with length equal to number of nodes in the entire graph (basis\n",
        "                          :  + shapes). role_id[i] is the ID of the role of node i. It is the label.\n",
        "        name              :  A graph identifier\n",
        "    \"\"\"\n",
        "    basis_type = \"ba\"\n",
        "    list_shapes = [[\"house\"]] * nb_shapes\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "    G, role_id, _ = build_graph(\n",
        "        width_basis, basis_type, list_shapes, start=0, m=5\n",
        "    )\n",
        "    G = perturb([G], 0.01)[0]\n",
        "\n",
        "    if feature_generator is None:\n",
        "        feature_generator = ConstFeatureGen(1)\n",
        "    feature_generator.gen_node_features(G)\n",
        "\n",
        "    name = basis_type + \"_\" + str(width_basis) + \"_\" + str(nb_shapes)\n",
        "    return G, role_id, name\n",
        "\n",
        "\n",
        "def gen_syn2(nb_shapes=100, width_basis=350):\n",
        "    \"\"\" Synthetic Graph #2:\n",
        "\n",
        "    Start with Barabasi-Albert graph and add node features indicative of a community label.\n",
        "\n",
        "    Args:\n",
        "        nb_shapes         :  The number of shapes (here 'houses') that should be added to the base graph.\n",
        "        width_basis       :  The width of the basis graph (here 'Barabasi-Albert' random graph).\n",
        "\n",
        "    Returns:\n",
        "        G                 :  A networkx graph\n",
        "        label             :  Label of the nodes (determined by role_id and community)\n",
        "        name              :  A graph identifier\n",
        "    \"\"\"\n",
        "    basis_type = \"ba\"\n",
        "\n",
        "    random_mu = [0.0] * 8\n",
        "    random_sigma = [1.0] * 8\n",
        "\n",
        "    # Create two grids\n",
        "    mu_1, sigma_1 = np.array([-1.0] * 2 + random_mu), np.array([0.5] * 2 + random_sigma)\n",
        "    mu_2, sigma_2 = np.array([1.0] * 2 + random_mu), np.array([0.5] * 2 + random_sigma)\n",
        "    feat_gen_G1 = GaussianFeatureGen(mu=mu_1, sigma=sigma_1)\n",
        "    feat_gen_G2 = GaussianFeatureGen(mu=mu_2, sigma=sigma_2)\n",
        "    G1, role_id1, name = gen_syn1(feature_generator=feat_gen_G1, m=4)\n",
        "    G2, role_id2, name = gen_syn1(feature_generator=feat_gen_G2, m=4)\n",
        "    G1_size = G1.number_of_nodes()\n",
        "    num_roles = max(role_id1) + 1\n",
        "    role_id2 = [r + num_roles for r in role_id2]\n",
        "    label = role_id1 + role_id2\n",
        "\n",
        "    # Edit node ids to avoid collisions on join\n",
        "    g1_map = {n: i for i, n in enumerate(G1.nodes())}\n",
        "    G1 = nx.relabel_nodes(G1, g1_map)\n",
        "    g2_map = {n: i + G1_size for i, n in enumerate(G2.nodes())}\n",
        "    G2 = nx.relabel_nodes(G2, g2_map)\n",
        "\n",
        "    # Join\n",
        "    n_pert_edges = width_basis\n",
        "    G = join_graph(G1, G2, n_pert_edges)\n",
        "\n",
        "    name = basis_type + \"_\" + str(width_basis) + \"_\" + str(nb_shapes) + \"_2comm\"\n",
        "\n",
        "    return G, label, name\n",
        "\n",
        "\n",
        "def gen_syn3(nb_shapes=80, width_basis=300, feature_generator=None, m=5):\n",
        "    \"\"\" Synthetic Graph #3:\n",
        "\n",
        "    Start with Barabasi-Albert graph and attach grid-shaped subgraphs.\n",
        "\n",
        "    Args:\n",
        "        nb_shapes         :  The number of shapes (here 'grid') that should be added to the base graph.\n",
        "        width_basis       :  The width of the basis graph (here 'Barabasi-Albert' random graph).\n",
        "        feature_generator :  A `FeatureGenerator` for node features. If `None`, add constant features to nodes.\n",
        "        m                 :  number of edges to attach to existing node (for BA graph)\n",
        "\n",
        "    Returns:\n",
        "        G                 :  A networkx graph\n",
        "        role_id           :  Role ID for each node in synthetic graph.\n",
        "        name              :  A graph identifier\n",
        "    \"\"\"\n",
        "    basis_type = \"ba\"\n",
        "    list_shapes = [[\"grid\", 3]] * nb_shapes\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "    G, role_id, _ = build_graph(\n",
        "        width_basis, basis_type, list_shapes, start=0, m=5\n",
        "    )\n",
        "    G = perturb([G], 0.01)[0]\n",
        "\n",
        "    if feature_generator is None:\n",
        "        feature_generator = ConstFeatureGen(1)\n",
        "    feature_generator.gen_node_features(G)\n",
        "\n",
        "    name = basis_type + \"_\" + str(width_basis) + \"_\" + str(nb_shapes)\n",
        "    return G, role_id, name\n",
        "\n",
        "\n",
        "def gen_syn4(nb_shapes=60, width_basis=8, feature_generator=None, m=4):\n",
        "    \"\"\" Synthetic Graph #4:\n",
        "\n",
        "    Start with a tree and attach cycle-shaped subgraphs.\n",
        "\n",
        "    Args:\n",
        "        nb_shapes         :  The number of shapes (here 'houses') that should be added to the base graph.\n",
        "        width_basis       :  The width of the basis graph (here a random 'Tree').\n",
        "        feature_generator :  A `FeatureGenerator` for node features. If `None`, add constant features to nodes.\n",
        "        m                 :  The tree depth.\n",
        "\n",
        "    Returns:\n",
        "        G                 :  A networkx graph\n",
        "        role_id           :  Role ID for each node in synthetic graph\n",
        "        name              :  A graph identifier\n",
        "    \"\"\"\n",
        "    basis_type = \"tree\"\n",
        "    list_shapes = [[\"cycle\", 6]] * nb_shapes\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "    G, role_id, plugins = build_graph(\n",
        "        width_basis, basis_type, list_shapes, start=0\n",
        "    )\n",
        "    G = perturb([G], 0.01)[0]\n",
        "\n",
        "    if feature_generator is None:\n",
        "        feature_generator = ConstFeatureGen(1)\n",
        "    feature_generator.gen_node_features(G)\n",
        "\n",
        "    name = basis_type + \"_\" + str(width_basis) + \"_\" + str(nb_shapes)\n",
        "\n",
        "    path = os.path.join(\"log/syn4_base_h20_o20\")\n",
        "    writer = SummaryWriter(path)\n",
        "    log_graph(writer, G, \"graph/full\")\n",
        "\n",
        "    return G, role_id, name\n",
        "\n",
        "\n",
        "def gen_syn5(nb_shapes=80, width_basis=8, feature_generator=None, m=3):\n",
        "    \"\"\" Synthetic Graph #5:\n",
        "\n",
        "    Start with a tree and attach grid-shaped subgraphs.\n",
        "\n",
        "    Args:\n",
        "        nb_shapes         :  The number of shapes (here 'houses') that should be added to the base graph.\n",
        "        width_basis       :  The width of the basis graph (here a random 'grid').\n",
        "        feature_generator :  A `FeatureGenerator` for node features. If `None`, add constant features to nodes.\n",
        "        m                 :  The tree depth.\n",
        "\n",
        "    Returns:\n",
        "        G                 :  A networkx graph\n",
        "        role_id           :  Role ID for each node in synthetic graph\n",
        "        name              :  A graph identifier\n",
        "    \"\"\"\n",
        "    basis_type = \"tree\"\n",
        "    list_shapes = [[\"grid\", m]] * nb_shapes\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "    G, role_id, _ = build_graph(\n",
        "        width_basis, basis_type, list_shapes, start=0\n",
        "    )\n",
        "    G = perturb([G], 0.1)[0]\n",
        "\n",
        "    if feature_generator is None:\n",
        "        feature_generator = ConstFeatureGen(1)\n",
        "    feature_generator.gen_node_features(G)\n",
        "\n",
        "    name = basis_type + \"_\" + str(width_basis) + \"_\" + str(nb_shapes)\n",
        "\n",
        "    path = os.path.join(\"log/syn5_base_h20_o20\")\n",
        "    writer = SummaryWriter(path)\n",
        "\n",
        "    return G, role_id, name\n"
      ],
      "metadata": {
        "id": "HWD36LtWqhg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data, train_ratio = 0.7, val_ratio = 0.15, test_ratio = 0.15):\n",
        "    G = data[0]\n",
        "    y = data[1]\n",
        "\n",
        "    num_nodes = len(y)\n",
        "    indices = torch.randperm(num_nodes)\n",
        "\n",
        "    num_train, num_val = int(num_nodes * train_ratio), int(num_nodes * val_ratio)\n",
        "    num_test = num_nodes - num_train - num_val\n",
        "\n",
        "    train_mask, val_mask, test_mask = torch.zeros(num_nodes, dtype=torch.bool), torch.zeros(num_nodes, dtype=torch.bool), torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    train_mask[indices[:num_train]] = True\n",
        "    val_mask[indices[num_train:num_train+num_val]] = True\n",
        "    test_mask[indices[num_train+num_val:]] = True\n",
        "\n",
        "    # Convert NetworkX graph to edge list\n",
        "    edge_list = list(G.edges)\n",
        "    # Create a set for symmetric edges to avoid duplicates\n",
        "    symmetric_edges = set()\n",
        "\n",
        "    # Add each edge and its reverse to the set\n",
        "    for u, v in edge_list:\n",
        "        symmetric_edges.add((u, v))\n",
        "        symmetric_edges.add((v, u))\n",
        "    edge_list = list(symmetric_edges)\n",
        "\n",
        "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "    node_features = [G.nodes[node]['feat'] for node in G.nodes()]\n",
        "    # Create a Data object\n",
        "    if len(np.array(node_features).shape) == 1:\n",
        "      data = Data(x=torch.tensor(np.array(node_features)).unsqueeze(1), edge_index=torch.tensor(np.array(edge_index)), y=torch.tensor(np.array(y)), train_mask=torch.tensor(np.array(train_mask)), val_mask=torch.tensor(np.array(val_mask)), test_mask=torch.tensor(np.array(test_mask)))\n",
        "    else:\n",
        "      data = Data(x=torch.tensor(np.array(node_features, dtype=float)).float(), edge_index=torch.tensor(np.array(edge_index)), y=torch.tensor(np.array(y)), train_mask=torch.tensor(np.array(train_mask)), val_mask=torch.tensor(np.array(val_mask)), test_mask=torch.tensor(np.array(test_mask)))\n",
        "    return data"
      ],
      "metadata": {
        "id": "KXSSL5Y-q6bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "5pwrKTSkqufb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Syn1 = preprocess(gen_syn1(nb_shapes=2, width_basis=10, feature_generator=None, m=5))\n",
        "Syn2 = preprocess(gen_syn2(nb_shapes=2, width_basis=10))\n",
        "Syn3 = preprocess(gen_syn3(nb_shapes=2, width_basis=10, feature_generator=None, m=5))\n",
        "data = Syn2"
      ],
      "metadata": {
        "id": "8bex-UOcq0Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "79eCjjVtslUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n",
        "\n",
        "class GNNModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                # GCNConv(hidden_dim, hidden_dim)\n",
        "                GATConv(hidden_dim, hidden_dim // num_heads, num_heads)\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # conv -> activation ->  dropout -> residual\n",
        "            x_in = x\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "class SparseGraphTransformerModel(Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, dense_adj):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = ~dense_adj.bool())\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # boolean mask enforces graph structure\n",
        "            # x = layer(x, src_mask = ~dense_adj.bool())\n",
        "\n",
        "            # MHSA layer\n",
        "            # boolean mask enforces graph structure\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = ~dense_adj.bool(),\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "class DenseGraphTransformerModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        # x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "            (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "            nan=0, posinf=0, neginf=0\n",
        "        )\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # float mask adds learnable additive attention bias\n",
        "            # x = layer(x, src_mask = attn_bias)\n",
        "\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "class DenseGraphTransformerModel_V2(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        # x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        # attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "        #     (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "        #     nan=0, posinf=0, neginf=0\n",
        "        # )\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # float mask adds learnable additive attention bias\n",
        "            # x = layer(x, src_mask = attn_bias)\n",
        "\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                # attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)"
      ],
      "metadata": {
        "id": "2ThXcVuhsnY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train GCN"
      ],
      "metadata": {
        "id": "UQQwyTH0oKM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = GNNModel().to(device)\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred, accs = model(data.x, data.edge_index).argmax(dim=-1), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "times = []\n",
        "for epoch in range(1, 100):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "          f'Final Test: {test_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
      ],
      "metadata": {
        "id": "V1mIp1WEniCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Sparse Transformer"
      ],
      "metadata": {
        "id": "dxlFkxPmoZ-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = SparseGraphTransformerModel().to(device)\n",
        "\n",
        "data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.dense_adj)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred, accs = model(data.x, data.dense_adj).argmax(dim=-1), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "times = []\n",
        "for epoch in range(1, 100):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "          f'Final Test: {test_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
      ],
      "metadata": {
        "id": "1zszPptIodFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Dense Transformer"
      ],
      "metadata": {
        "id": "t-paXlQjodik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_shortest_path_matrix(adjacency_matrix):\n",
        "    # Convert the adjacency matrix to a NetworkX graph\n",
        "    graph = nx.from_numpy_array(adjacency_matrix.cpu().numpy(), create_using=nx.DiGraph)\n",
        "    # Compute the shortest path matrix using Floyd-Warshall algorithm in NetworkX\n",
        "    shortest_path_matrix = nx.floyd_warshall_numpy(graph)\n",
        "    # Convert numpy array back to torch tensor\n",
        "    shortest_path_matrix = torch.tensor(shortest_path_matrix).float()\n",
        "    return shortest_path_matrix\n",
        "\n",
        "dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "dense_shortest_path_matrix = get_shortest_path_matrix(dense_adj)  # takes about 1-2 mins\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DenseGraphTransformerModel().to(device)\n",
        "\n",
        "data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n",
        "# data = T.AddRandomWalkPE(walk_length = 16, attr_name = 'pos_enc')(data)\n",
        "data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "data.dense_sp_matrix = dense_shortest_path_matrix.float()  # pre-computed in previous cell\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "times = []\n",
        "for epoch in range(1, 100):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "          f'Final Test: {test_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "\n",
        "# Notes\n",
        "# - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "# - Node positional encodings are not particularly useful\n",
        "# - Edge distance encodings are very useful\n",
        "# - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes"
      ],
      "metadata": {
        "id": "H-tSsBE-ofcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZ5sYAb3vq6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Dense Transformer V2"
      ],
      "metadata": {
        "id": "AiPKQcFXvrK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DenseGraphTransformerModel_V2(num_heads=4, num_layers=3).to(device)\n",
        "\n",
        "data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n",
        "# data = T.AddRandomWalkPE(walk_length = 16, attr_name = 'pos_enc')(data)\n",
        "data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "data.dense_sp_matrix = dense_shortest_path_matrix.float()  # pre-computed in previous cell\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "times = []\n",
        "for epoch in range(1, 100):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "          f'Final Test: {test_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "\n",
        "# Notes\n",
        "# - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "# - Node positional encodings are not particularly useful\n",
        "# - Edge distance encodings are very useful\n",
        "# - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0D6U89_vtiZ",
        "outputId": "4339de72-117c-452c-8606-7a7e1be1c0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0033, Loss: 1.3293 Train: 0.4402, Val: 0.3571, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0034, Loss: 1.3165 Train: 0.4464, Val: 0.3714, Test: 0.3886, Final Test: 0.4076\n",
            "Epoch: 0035, Loss: 1.3267 Train: 0.4433, Val: 0.3810, Test: 0.3886, Final Test: 0.4076\n",
            "Epoch: 0036, Loss: 1.3111 Train: 0.4443, Val: 0.3714, Test: 0.3886, Final Test: 0.4076\n",
            "Epoch: 0037, Loss: 1.3114 Train: 0.4372, Val: 0.3714, Test: 0.3839, Final Test: 0.4076\n",
            "Epoch: 0038, Loss: 1.3144 Train: 0.4402, Val: 0.3667, Test: 0.3886, Final Test: 0.4076\n",
            "Epoch: 0039, Loss: 1.3147 Train: 0.4474, Val: 0.3524, Test: 0.3886, Final Test: 0.4076\n",
            "Epoch: 0040, Loss: 1.2980 Train: 0.4525, Val: 0.3524, Test: 0.3981, Final Test: 0.4076\n",
            "Epoch: 0041, Loss: 1.2976 Train: 0.4484, Val: 0.3524, Test: 0.3981, Final Test: 0.4076\n",
            "Epoch: 0042, Loss: 1.3064 Train: 0.4484, Val: 0.3619, Test: 0.3886, Final Test: 0.4076\n",
            "Epoch: 0043, Loss: 1.3013 Train: 0.4464, Val: 0.3619, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0044, Loss: 1.3031 Train: 0.4494, Val: 0.3667, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0045, Loss: 1.3026 Train: 0.4494, Val: 0.3667, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0046, Loss: 1.2909 Train: 0.4525, Val: 0.3571, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0047, Loss: 1.2955 Train: 0.4515, Val: 0.3619, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0048, Loss: 1.2885 Train: 0.4515, Val: 0.3667, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0049, Loss: 1.3028 Train: 0.4515, Val: 0.3714, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0050, Loss: 1.2833 Train: 0.4597, Val: 0.3524, Test: 0.3981, Final Test: 0.4076\n",
            "Epoch: 0051, Loss: 1.2823 Train: 0.4648, Val: 0.3524, Test: 0.4123, Final Test: 0.4076\n",
            "Epoch: 0052, Loss: 1.2943 Train: 0.4688, Val: 0.3429, Test: 0.4028, Final Test: 0.4076\n",
            "Epoch: 0053, Loss: 1.2827 Train: 0.4617, Val: 0.3476, Test: 0.4123, Final Test: 0.4076\n",
            "Epoch: 0054, Loss: 1.2646 Train: 0.4617, Val: 0.3381, Test: 0.3981, Final Test: 0.4076\n",
            "Epoch: 0055, Loss: 1.2767 Train: 0.4607, Val: 0.3571, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0056, Loss: 1.2666 Train: 0.4617, Val: 0.3619, Test: 0.3981, Final Test: 0.4076\n",
            "Epoch: 0057, Loss: 1.2821 Train: 0.4770, Val: 0.3619, Test: 0.4076, Final Test: 0.4076\n",
            "Epoch: 0058, Loss: 1.2611 Train: 0.4729, Val: 0.3524, Test: 0.3981, Final Test: 0.4076\n",
            "Epoch: 0059, Loss: 1.2640 Train: 0.4699, Val: 0.3524, Test: 0.3886, Final Test: 0.4076\n",
            "Epoch: 0060, Loss: 1.2617 Train: 0.4607, Val: 0.3571, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0061, Loss: 1.2538 Train: 0.4637, Val: 0.3571, Test: 0.4028, Final Test: 0.4076\n",
            "Epoch: 0062, Loss: 1.2505 Train: 0.4648, Val: 0.3619, Test: 0.3934, Final Test: 0.4076\n",
            "Epoch: 0063, Loss: 1.2511 Train: 0.4699, Val: 0.3571, Test: 0.3791, Final Test: 0.4076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Szb1exUev_1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}