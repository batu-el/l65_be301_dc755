{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nRR5bUiLRZ5a",
        "Z39ODu9oT2f9",
        "Fxj1ye0Y1ZwC",
        "sWeKuQTt1k_M",
        "2hMUb2yK1pEB",
        "ZXuPgGbGa2Nd",
        "670dtom5sl10"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1nYy2OQQ4os8qAJRaYQ2kwsbNjzw3zFyx",
      "authorship_tag": "ABX9TyOWnsfm2HMUCDFSkKQXyfRt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batu-el/l65_be301_dc755/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the Attention Mask?\n",
        "# Train 4 Head 3 Layer Model\n",
        "# Combining Attention values across heads - Avg\n",
        "# Combining Attention values across layers - Matrix Multiply\n",
        "\n",
        "# Claim 1: when combining attention heads in the same layer, we can just take average attention\n",
        "# Claim 2: when combining attention layers, we can take the matrix multiply of the attentions"
      ],
      "metadata": {
        "id": "tVe55J-aw7aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "nRR5bUiLRZ5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl torch_geometric torch\n",
        "\n",
        "# Install required python libraries\n",
        "import os\n",
        "\n",
        "# Install PyTorch Geometric and other libraries\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    print(\"Installing PyTorch Geometric\")\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "    !pip install -q torch-geometric\n",
        "    print(\"Installing other libraries\")\n",
        "    !pip install networkx\n",
        "    !pip install lovely-tensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLx8ARPERaaB",
        "outputId": "7716ef34-ee47-462d-b340-c7030e432bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch_geometric, nvidia-cusolver-cu12, dgl\n",
            "Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 torch_geometric-2.5.2\n",
            "Installing PyTorch Geometric\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling other libraries\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting lovely-tensors\n",
            "  Downloading lovely_tensors-0.1.15-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lovely-tensors) (2.2.1+cu121)\n",
            "Collecting lovely-numpy>=0.2.9 (from lovely-tensors)\n",
            "  Downloading lovely_numpy-0.2.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.25.2)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (1.5.29)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (7.34.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lovely-numpy>=0.2.9->lovely-tensors) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->lovely-tensors) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->lovely-tensors) (12.4.99)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->lovely-numpy>=0.2.9->lovely-tensors) (24.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->lovely-numpy>=0.2.9->lovely-tensors)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->lovely-numpy>=0.2.9->lovely-tensors) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lovely-tensors) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lovely-tensors) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->lovely-numpy>=0.2.9->lovely-tensors) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lovely-numpy>=0.2.9->lovely-tensors) (1.16.0)\n",
            "Installing collected packages: jedi, lovely-numpy, lovely-tensors\n",
            "Successfully installed jedi-0.19.1 lovely-numpy-0.2.11 lovely-tensors-0.1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from typing import Mapping, Tuple, Sequence, List\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
        "\n",
        "# from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n",
        "\n",
        "import lovely_tensors as lt\n",
        "lt.monkey_patch()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMiwOILkRgEP",
        "outputId": "3e65df33-9615-4d6a-dbee-bc0c31fc207c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports succeeded.\n",
            "Python version 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "PyTorch version 2.2.1+cu121\n",
            "PyG version 2.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for deterministic results\n",
        "\n",
        "def seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(0)\n",
        "print(\"All seeds set.\")"
      ],
      "metadata": {
        "id": "zuVTx4otRi6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041c1b2d-2b73-4a91-b81f-cfa7c28c5534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All seeds set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "g6R5GR7hRdEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import WebKB, WikipediaNetwork\n",
        "\n",
        "DATASETS = {}\n",
        "\n",
        "# Chamelion & Squirrel\n",
        "# Cora & Citeseer\n",
        "# Cornell & Texas & Wisconsin\n",
        "\n",
        "## Mid Size Datasets\n",
        "# Citation Networks\n",
        "dataset = 'Cora'\n",
        "dataset = Planetoid('/tmp/Cora', dataset)\n",
        "data = dataset[0]\n",
        "DATASETS['Cora'] = data\n",
        "dataset = 'Citeseer'\n",
        "dataset = Planetoid('/tmp/Citeseer', dataset)\n",
        "data = dataset[0]\n",
        "DATASETS['Citeseer'] = data\n",
        "# Wikipedia Pages\n",
        "dataset = 'Chameleon'\n",
        "dataset = WikipediaNetwork(root='/tmp/Chameleon', name='Chameleon')\n",
        "data = dataset[0]\n",
        "DATASETS['Chameleon'] = data\n",
        "dataset = 'Squirrel'\n",
        "dataset = WikipediaNetwork(root='/tmp/Squirrel', name='Squirrel')\n",
        "data = dataset[0]\n",
        "DATASETS['Squirrel'] = data\n",
        "### Small Sized Datasets\n",
        "# Web Pages\n",
        "dataset = WebKB(root='/tmp/Cornell', name='Cornell')\n",
        "data = dataset[0]\n",
        "DATASETS['Cornell'] = data\n",
        "dataset = WebKB(root='/tmp/Texas', name='Texas')\n",
        "data = dataset[0]\n",
        "DATASETS['Texas'] = data\n",
        "dataset = WebKB(root='/tmp/Wisconsin', name='Wisconsin')\n",
        "data = dataset[0]\n",
        "DATASETS['Wisconsin'] = data"
      ],
      "metadata": {
        "id": "8PVfelu6ReJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32e4149-577a-4c5c-8190-6a5c2c8e0ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/cornell/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/cornell/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/wisconsin/out1_node_feature_label.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/wisconsin/out1_graph_edges.txt\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_0.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_1.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_2.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_3.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_4.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_5.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_6.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_7.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_8.npz\n",
            "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_9.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tqdm\n",
        "# ### Shortest Paths ###\n",
        "# def get_shortest_path_matrix(adjacency_matrix):\n",
        "#     graph = nx.from_numpy_array(adjacency_matrix.cpu().numpy(), create_using=nx.DiGraph)\n",
        "#     shortest_path_matrix = nx.floyd_warshall_numpy(graph)\n",
        "#     shortest_path_matrix = torch.tensor(shortest_path_matrix).float()\n",
        "#     return shortest_path_matrix\n",
        "\n",
        "# SHORTEST_PATHS = {}\n",
        "# for data_key in tqdm.tqdm(DATASETS):\n",
        "#   print(data_key)\n",
        "#   data = DATASETS[data_key]\n",
        "#   dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "#   dense_shortest_path_matrix = get_shortest_path_matrix(dense_adj)\n",
        "#   SHORTEST_PATHS[data_key] = dense_shortest_path_matrix\n",
        "\n",
        "# ### Save the Shortest Paths\n",
        "# import pickle\n",
        "# with open('sp_dict.pkl', 'wb') as f:\n",
        "#     pickle.dump(SHORTEST_PATHS, f)\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/shortest_paths/sp_dict.pkl', 'rb') as f:\n",
        "    SHORTEST_PATHS = pickle.load(f)"
      ],
      "metadata": {
        "id": "PqcrQ7FuVa8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data_key in DATASETS:\n",
        "  data = DATASETS[data_key]\n",
        "  data.dense_sp_matrix = SHORTEST_PATHS[data_key]\n",
        "  data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n",
        "  data.dense_adj = data.dense_adj.cuda() + torch.eye(data.dense_adj.shape[0]).cuda()\n",
        "  data.dense_adj[data.dense_adj == 2] = 1\n",
        "  data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n",
        "  DATASETS[data_key] = data"
      ],
      "metadata": {
        "id": "hzEswGTMXoOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Masks ###\n",
        "\n",
        "def generate_masks(num_nodes=None,num_runs=None,train_ratio=None, val_ratio=None):\n",
        "    masks = { 'train_mask': np.zeros((num_nodes, num_runs), dtype=int),\n",
        "              'val_mask': np.zeros((num_nodes, num_runs), dtype=int),\n",
        "              'test_mask': np.zeros((num_nodes, num_runs), dtype=int)}\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        indices = np.arange(num_nodes)\n",
        "        np.random.shuffle(indices)\n",
        "        train_end = int(train_ratio * num_nodes)\n",
        "        val_end = train_end + int(val_ratio * num_nodes)\n",
        "        masks['train_mask'][indices[:train_end], run] = 1\n",
        "        masks['val_mask'][indices[train_end:val_end], run] = 1\n",
        "        masks['test_mask'][indices[val_end:], run] = 1\n",
        "\n",
        "    tensor_masks = {'train_mask': torch.tensor(masks['train_mask']),\n",
        "                    'val_mask':torch.tensor(masks['val_mask']),\n",
        "                    'test_mask':torch.tensor(masks['test_mask'])}\n",
        "    return tensor_masks\n",
        "\n",
        "for data_key in DATASETS:\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    masks = generate_masks(num_nodes=data.x.shape[0], num_runs=10, train_ratio=0.4, val_ratio=0.3)\n",
        "    data.train_mask = masks['train_mask'].bool()\n",
        "    data.val_mask = masks['val_mask'].bool()\n",
        "    data.test_mask = masks['test_mask'].bool()\n",
        "\n",
        "    if len(data.train_mask.shape)==1:\n",
        "      print('Add 10 Masks')\n",
        "    else:\n",
        "      print('We have 10 Masks')\n",
        "      print('Train Ratio:',(data.train_mask[:,0].sum() / len(data.train_mask[:,0])).item())\n",
        "      print('Val Ratio:',(data.val_mask[:,0].sum() / len(data.val_mask[:,0])).item())\n",
        "      print('Test Ratio:',(data.test_mask[:,0].sum() / len(data.test_mask[:,0])).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsqv_LTEU-2H",
        "outputId": "201e48c3-2cf6-44e8-b33e-e4b1984f31f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 10 Masks\n",
            "Train Ratio: 0.39992615580558777\n",
            "Val Ratio: 0.29985228180885315\n",
            "Test Ratio: 0.3002215623855591\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.39975953102111816\n",
            "Val Ratio: 0.29996994137763977\n",
            "Test Ratio: 0.30027052760124207\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.39964866638183594\n",
            "Val Ratio: 0.2999560832977295\n",
            "Test Ratio: 0.30039525032043457\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.39992308616638184\n",
            "Val Ratio: 0.2999423146247864\n",
            "Test Ratio: 0.3001345992088318\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.3989070951938629\n",
            "Val Ratio: 0.2950819730758667\n",
            "Test Ratio: 0.3060109317302704\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.3989070951938629\n",
            "Val Ratio: 0.2950819730758667\n",
            "Test Ratio: 0.3060109317302704\n",
            "We have 10 Masks\n",
            "Train Ratio: 0.39840638637542725\n",
            "Val Ratio: 0.29880478978157043\n",
            "Test Ratio: 0.3027888536453247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table 1: Dataset Statistics"
      ],
      "metadata": {
        "id": "tPGL9tFORJCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Table 1 ###\n",
        "### Dataset Statistics ###\n",
        "import dgl\n",
        "Homophily_Levels = {}\n",
        "\n",
        "for data_key in DATASETS:\n",
        "  data = DATASETS[data_key]\n",
        "  edge_index_tensor = torch.tensor(data.edge_index.cpu().numpy(), dtype=torch.long)\n",
        "  g = dgl.graph((edge_index_tensor[0], edge_index_tensor[1]), num_nodes=data.x.shape[0])\n",
        "  g.ndata['y'] = torch.tensor(data.y.cpu().numpy(), dtype=torch.long)\n",
        "  Homophily_Levels[data_key] = {'Node Homophily':dgl.node_homophily(g, g.ndata['y'])*100,\n",
        "                                'Edge Homophily':dgl.edge_homophily(g, g.ndata['y'])*100,\n",
        "                                'Adjusted Homophily':dgl.adjusted_homophily(g, g.ndata['y'])*100,\n",
        "                                'Number of Nodes': int(g.num_nodes()),\n",
        "                                'Number of Edges': int(g.num_edges())\n",
        "                                }\n",
        "df = pd.DataFrame(Homophily_Levels).round(1)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "O34_wDE-RLre",
        "outputId": "d3467bea-9f77-41e8-8983-4906d18bc9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       Cora  Citeseer  Chameleon  Squirrel  Cornell  Texas  \\\n",
              "Node Homophily         82.5      70.6       10.4       8.9     10.6    6.5   \n",
              "Edge Homophily         81.0      73.6       23.5      22.4     13.1   10.8   \n",
              "Adjusted Homophily     77.1      67.1        3.3       0.7    -21.1  -25.9   \n",
              "Number of Nodes      2708.0    3327.0     2277.0    5201.0    183.0  183.0   \n",
              "Number of Edges     10556.0    9104.0    36101.0  217073.0    298.0  325.0   \n",
              "\n",
              "                    Wisconsin  \n",
              "Node Homophily           17.2  \n",
              "Edge Homophily           19.6  \n",
              "Adjusted Homophily      -15.2  \n",
              "Number of Nodes         251.0  \n",
              "Number of Edges         515.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22163bf1-36fb-47c6-821f-2ae92766c353\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cora</th>\n",
              "      <th>Citeseer</th>\n",
              "      <th>Chameleon</th>\n",
              "      <th>Squirrel</th>\n",
              "      <th>Cornell</th>\n",
              "      <th>Texas</th>\n",
              "      <th>Wisconsin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Node Homophily</th>\n",
              "      <td>82.5</td>\n",
              "      <td>70.6</td>\n",
              "      <td>10.4</td>\n",
              "      <td>8.9</td>\n",
              "      <td>10.6</td>\n",
              "      <td>6.5</td>\n",
              "      <td>17.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Edge Homophily</th>\n",
              "      <td>81.0</td>\n",
              "      <td>73.6</td>\n",
              "      <td>23.5</td>\n",
              "      <td>22.4</td>\n",
              "      <td>13.1</td>\n",
              "      <td>10.8</td>\n",
              "      <td>19.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Adjusted Homophily</th>\n",
              "      <td>77.1</td>\n",
              "      <td>67.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>-25.9</td>\n",
              "      <td>-15.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Number of Nodes</th>\n",
              "      <td>2708.0</td>\n",
              "      <td>3327.0</td>\n",
              "      <td>2277.0</td>\n",
              "      <td>5201.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>251.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Number of Edges</th>\n",
              "      <td>10556.0</td>\n",
              "      <td>9104.0</td>\n",
              "      <td>36101.0</td>\n",
              "      <td>217073.0</td>\n",
              "      <td>298.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22163bf1-36fb-47c6-821f-2ae92766c353')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-22163bf1-36fb-47c6-821f-2ae92766c353 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-22163bf1-36fb-47c6-821f-2ae92766c353');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d78a06d9-2695-4872-ada8-c6c4e774bb09\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d78a06d9-2695-4872-ada8-c6c4e774bb09')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d78a06d9-2695-4872-ada8-c6c4e774bb09 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Cora\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4536.15661733587,\n        \"min\": 77.1,\n        \"max\": 10556.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          81.0,\n          10556.0,\n          77.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Citeseer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3937.038826072204,\n        \"min\": 67.1,\n        \"max\": 9104.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          73.6,\n          9104.0,\n          67.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chameleon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15916.360160947603,\n        \"min\": 3.3,\n        \"max\": 36101.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          23.5,\n          36101.0,\n          3.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Squirrel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96519.10016191094,\n        \"min\": 0.7,\n        \"max\": 217073.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          22.4,\n          217073.0,\n          0.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cornell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 138.065553270901,\n        \"min\": -21.1,\n        \"max\": 298.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          13.1,\n          298.0,\n          -21.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Texas\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 150.05301396506502,\n        \"min\": -25.9,\n        \"max\": 325.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          10.8,\n          325.0,\n          -25.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Wisconsin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 226.42564342406095,\n        \"min\": -15.2,\n        \"max\": 515.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          19.6,\n          515.0,\n          -15.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "7JV4ZtidSENR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n",
        "\n",
        "class GNNModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "        self.layers = ModuleList()\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                GCNConv(hidden_dim, hidden_dim)\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # conv -> activation ->  dropout -> residual\n",
        "            x_in = x\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "class SparseGraphTransformerModel(Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, dense_adj):\n",
        "\n",
        "        x = self.lin_in(x)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = ~dense_adj.bool(),\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "class DenseGraphTransformerModel(Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        # x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "            (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "            nan=0, posinf=0, neginf=0)\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "class DenseGraphTransformerModel_V2(Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int = data.x.shape[-1],\n",
        "            pos_enc_dim: int = 16,\n",
        "            hidden_dim: int = 128,\n",
        "            num_heads: int = 1,\n",
        "            num_layers: int = 1,\n",
        "            out_dim: int = len(data.y.unique()),\n",
        "            dropout: float = 0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_in = Linear(in_dim, hidden_dim)\n",
        "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
        "        self.lin_out = Linear(hidden_dim, out_dim)\n",
        "\n",
        "        self.layers = ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.layers.append(\n",
        "                MultiheadAttention(\n",
        "                    embed_dim = hidden_dim,\n",
        "                    num_heads = num_heads,\n",
        "                    dropout = dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
        "\n",
        "        x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
        "        # x = self.lin_in(x)  # no node positional encoding\n",
        "\n",
        "        # attention bias\n",
        "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
        "        # diagonals -> self connection, set to 0\n",
        "        # disconnected nodes -> -1\n",
        "        # attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
        "        #     (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
        "        #     nan=0, posinf=0, neginf=0\n",
        "        # )\n",
        "        #attn_bias = torch.ones_like(attn_bias)\n",
        "\n",
        "        # TransformerEncoder\n",
        "        # x = self.encoder(x, mask = attn_bias)\n",
        "\n",
        "        self.attn_weights_list = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # # TransformerEncoderLayer\n",
        "            # # float mask adds learnable additive attention bias\n",
        "            # x = layer(x, src_mask = attn_bias)\n",
        "\n",
        "            # MHSA layer\n",
        "            # float mask adds learnable additive attention bias\n",
        "            x_in = x\n",
        "            x, attn_weights = layer(\n",
        "                x, x, x,\n",
        "                # attn_mask = attn_bias,\n",
        "                average_attn_weights = False\n",
        "            )\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "            x = x_in + x\n",
        "\n",
        "            self.attn_weights_list.append(attn_weights)\n",
        "\n",
        "        x = self.lin_out(x)\n",
        "\n",
        "        return x.log_softmax(dim=-1)"
      ],
      "metadata": {
        "id": "AKL9b7tCSDDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainers"
      ],
      "metadata": {
        "id": "Z39ODu9oT2f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Train_GCN(NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              data):\n",
        "\n",
        "    IN_DIM = data.x.shape[-1]\n",
        "    OUT_DIM = len(data.y.unique())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = GNNModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return float(loss)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test():\n",
        "        model.eval()\n",
        "        pred, accs = model(data.x, data.edge_index).argmax(dim=-1), []\n",
        "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "        return accs\n",
        "\n",
        "    best_val_acc = test_acc = 0\n",
        "    times = []\n",
        "    for epoch in range(1, 100):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, val_acc, tmp_test_acc = test()\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc = tmp_test_acc\n",
        "        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "        #       f'Final Test: {test_acc:.4f}')\n",
        "        times.append(time.time() - start)\n",
        "    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, None\n",
        "\n",
        "def Train_SparseGraphTransformerModel(NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              data):\n",
        "\n",
        "    IN_DIM = data.x.shape[-1]\n",
        "    OUT_DIM = len(data.y.unique())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = SparseGraphTransformerModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.dense_adj)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return float(loss)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test():\n",
        "        model.eval()\n",
        "        pred, accs = model(data.x, data.dense_adj).argmax(dim=-1), []\n",
        "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "        return accs\n",
        "\n",
        "    best_val_acc = test_acc = 0\n",
        "    times = []\n",
        "    for epoch in range(1, 100):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, val_acc, tmp_test_acc = test()\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc = tmp_test_acc\n",
        "        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "        #       f'Final Test: {test_acc:.4f}')\n",
        "        times.append(time.time() - start)\n",
        "    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n",
        "\n",
        "def Train_DenseGraphTransformerModel(NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              data):\n",
        "\n",
        "    IN_DIM = data.x.shape[-1]\n",
        "    OUT_DIM = len(data.y.unique())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = DenseGraphTransformerModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        # print(data.pos_enc)\n",
        "        out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return float(loss)\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test():\n",
        "        model.eval()\n",
        "        pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n",
        "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "        return accs\n",
        "\n",
        "    best_val_acc = test_acc = 0\n",
        "    times = []\n",
        "    for epoch in range(1, 100):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, val_acc, tmp_test_acc = test()\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc = tmp_test_acc\n",
        "        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "        #       f'Final Test: {test_acc:.4f}')\n",
        "        times.append(time.time() - start)\n",
        "    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n",
        "\n",
        "    # Notes\n",
        "    # - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "    # - Node positional encodings are not particularly useful\n",
        "    # - Edge distance encodings are very useful\n",
        "    # - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes\n",
        "\n",
        "def Train_DenseGraphTransformerModel_V2(NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              data):\n",
        "\n",
        "    IN_DIM = data.x.shape[-1]\n",
        "    OUT_DIM = len(data.y.unique())\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model = DenseGraphTransformerModel_V2(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM,out_dim=OUT_DIM).to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.pos_enc, data.dense_sp_matrix)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return float(loss)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test():\n",
        "        model.eval()\n",
        "        pred, accs = model(data.x, data.pos_enc, data.dense_sp_matrix).argmax(dim=-1), []\n",
        "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "            accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
        "        return accs\n",
        "\n",
        "    best_val_acc = test_acc = 0\n",
        "    times = []\n",
        "    for epoch in range(1, 100):\n",
        "        start = time.time()\n",
        "        loss = train()\n",
        "        train_acc, val_acc, tmp_test_acc = test()\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc = tmp_test_acc\n",
        "        # print(f'Epoch: {epoch:04d}, Loss: {loss:.4f} Train: {train_acc:.4f}, '\n",
        "        #       f'Val: {val_acc:.4f}, Test: {tmp_test_acc:.4f}, '\n",
        "        #       f'Final Test: {test_acc:.4f}')\n",
        "        times.append(time.time() - start)\n",
        "    # print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "    return {'train_acc':train_acc,'val_acc':val_acc,'test_acc':test_acc}, model.attn_weights_list\n",
        "\n",
        "    # Notes\n",
        "    # - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
        "    # - Node positional encodings are not particularly useful\n",
        "    # - Edge distance encodings are very useful\n",
        "    # - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes"
      ],
      "metadata": {
        "id": "bgXWrDZ5V_al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "veDrDxJIaO7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 1 Layer, 1 Head"
      ],
      "metadata": {
        "id": "X3wGJPl3YJcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "NUM_LAYERS = 1\n",
        "NUM_HEADS = 1\n",
        "NUM_RUNS = 10\n",
        "all_stats = {}\n",
        "for data_key in DATASETS:\n",
        "    print(f'Training on {data_key}')\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    TRAIN_MASKS = data.train_mask\n",
        "    VAL_MASKS = data.val_mask\n",
        "    TEST_MASKS = data.test_mask\n",
        "\n",
        "    run_stats = {}\n",
        "\n",
        "    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n",
        "        data.train_mask = TRAIN_MASKS[:,mask_idx]\n",
        "        data.val_mask = VAL_MASKS[:,mask_idx]\n",
        "        data.test_mask = TEST_MASKS[:,mask_idx]\n",
        "\n",
        "        accuracy_statistics = {}\n",
        "        attn_weights = {}\n",
        "\n",
        "        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n",
        "        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n",
        "        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n",
        "    all_stats[data_key] = run_stats\n",
        "    data.train_mask = TRAIN_MASKS\n",
        "    data.val_mask = VAL_MASKS\n",
        "    data.test_mask = TEST_MASKS\n",
        "\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n",
        "    pickle.dump(all_stats, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6GoLurKW57q",
        "outputId": "5a2e6a7c-8104-4d38-cdff-fb422102fd9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:21<00:00,  2.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Citeseer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:22<00:00,  2.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Chameleon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:18<00:00,  1.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Squirrel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:36<00:00,  3.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cornell\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Texas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:17<00:00,  1.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Wisconsin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 1 Layer, 2 Heads"
      ],
      "metadata": {
        "id": "Fxj1ye0Y1ZwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "NUM_LAYERS = 1\n",
        "NUM_HEADS = 2\n",
        "NUM_RUNS = 10\n",
        "all_stats = {}\n",
        "for data_key in DATASETS:\n",
        "    print(f'Training on {data_key}')\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    TRAIN_MASKS = data.train_mask\n",
        "    VAL_MASKS = data.val_mask\n",
        "    TEST_MASKS = data.test_mask\n",
        "\n",
        "    run_stats = {}\n",
        "\n",
        "    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n",
        "        data.train_mask = TRAIN_MASKS[:,mask_idx]\n",
        "        data.val_mask = VAL_MASKS[:,mask_idx]\n",
        "        data.test_mask = TEST_MASKS[:,mask_idx]\n",
        "\n",
        "        accuracy_statistics = {}\n",
        "        attn_weights = {}\n",
        "\n",
        "        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n",
        "        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n",
        "        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n",
        "    all_stats[data_key] = run_stats\n",
        "    data.train_mask = TRAIN_MASKS\n",
        "    data.val_mask = VAL_MASKS\n",
        "    data.test_mask = TEST_MASKS\n",
        "\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n",
        "    pickle.dump(all_stats, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1igyz0Pr1dRN",
        "outputId": "007dcfb4-f029-465a-81fe-05b0ac2fb881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:27<00:00,  2.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Citeseer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:32<00:00,  3.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Chameleon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:24<00:00,  2.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Squirrel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:52<00:00,  5.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cornell\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Texas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:18<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Wisconsin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:18<00:00,  1.82s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 2 Layers, 1 Head"
      ],
      "metadata": {
        "id": "sWeKuQTt1k_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "### Train Cora ###\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 1\n",
        "NUM_RUNS = 10\n",
        "# data_key = 'Wisconsin'\n",
        "all_stats = {}\n",
        "for data_key in DATASETS:\n",
        "    print(f'Training on {data_key}')\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    TRAIN_MASKS = data.train_mask\n",
        "    VAL_MASKS = data.val_mask\n",
        "    TEST_MASKS = data.test_mask\n",
        "\n",
        "    run_stats = {}\n",
        "\n",
        "    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n",
        "        data.train_mask = TRAIN_MASKS[:,mask_idx]\n",
        "        data.val_mask = VAL_MASKS[:,mask_idx]\n",
        "        data.test_mask = TEST_MASKS[:,mask_idx]\n",
        "\n",
        "        accuracy_statistics = {}\n",
        "        attn_weights = {}\n",
        "\n",
        "        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n",
        "        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel'])\n",
        "        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel'])\n",
        "        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2'])\n",
        "        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n",
        "    all_stats[data_key] = run_stats\n",
        "    data.train_mask = TRAIN_MASKS\n",
        "    data.val_mask = VAL_MASKS\n",
        "    data.test_mask = TEST_MASKS\n",
        "\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n",
        "    pickle.dump(all_stats, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcAfw7My1oij",
        "outputId": "9a14a337-b5e8-458a-8573-4ce8a7a50711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:29<00:00,  2.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Citeseer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:33<00:00,  3.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Chameleon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Squirrel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:56<00:00,  5.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cornell\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Texas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:27<00:00,  2.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Wisconsin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:27<00:00,  2.74s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: 2 Layers, 2 Heads"
      ],
      "metadata": {
        "id": "2hMUb2yK1pEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import tqdm\n",
        "### Train Cora ###\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 2\n",
        "NUM_RUNS = 10\n",
        "# data_key = 'Wisconsin'\n",
        "all_stats = {}\n",
        "for data_key in DATASETS:\n",
        "    print(f'Training on {data_key}')\n",
        "    data = DATASETS[data_key]\n",
        "\n",
        "    TRAIN_MASKS = data.train_mask\n",
        "    VAL_MASKS = data.val_mask\n",
        "    TEST_MASKS = data.test_mask\n",
        "\n",
        "    run_stats = {}\n",
        "\n",
        "    for mask_idx in tqdm.tqdm(range(NUM_RUNS)):\n",
        "        gc.collect()\n",
        "        data.train_mask = TRAIN_MASKS[:,mask_idx]\n",
        "        data.val_mask = VAL_MASKS[:,mask_idx]\n",
        "        data.test_mask = TEST_MASKS[:,mask_idx]\n",
        "\n",
        "        accuracy_statistics = {}\n",
        "        attn_weights = {}\n",
        "\n",
        "        accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['SparseGraphTransformerModel'] , attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel'] , attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, data)\n",
        "        accuracy_statistics['DenseGraphTransformerModel_V2'] , attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, data)\n",
        "        attn_weights['SparseGraphTransformerModel'] = torch.stack(attn_weights['SparseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel'] = torch.stack(attn_weights['DenseGraphTransformerModel']).cpu()\n",
        "        attn_weights['DenseGraphTransformerModel_V2'] = torch.stack(attn_weights['DenseGraphTransformerModel_V2']).cpu()\n",
        "        run_stats[mask_idx] = {'accuracy': accuracy_statistics, 'attentions': attn_weights}\n",
        "\n",
        "    all_stats[data_key] = run_stats\n",
        "    data.train_mask = TRAIN_MASKS\n",
        "    data.val_mask = VAL_MASKS\n",
        "    data.test_mask = TEST_MASKS\n",
        "\n",
        "import pickle\n",
        "with open('drive/MyDrive/Colab Notebooks/L65_Project/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'wb') as f:\n",
        "    pickle.dump(all_stats, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYmZVaET1sKD",
        "outputId": "e7aec9f4-9e13-4f97-f939-e114ca8fc868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:48<00:00,  4.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Citeseer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:58<00:00,  5.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Chameleon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:42<00:00,  4.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Squirrel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:35<00:00,  9.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Cornell\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:30<00:00,  3.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Texas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:29<00:00,  2.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Wisconsin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:30<00:00,  3.03s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n"
      ],
      "metadata": {
        "id": "ZXuPgGbGa2Nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table 2: Accuracy Statistics"
      ],
      "metadata": {
        "id": "670dtom5sl10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Table 2 ###\n",
        "### Accuracy Statistics ###\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "all_stats_df = {}\n",
        "for data_key in all_stats:\n",
        "  run_stats = all_stats[data_key]\n",
        "  table1 = pd.concat({key : pd.DataFrame(run_stats[key]['accuracy']) for key in run_stats}, axis=0)\n",
        "  table1_train = pd.concat({'mean': table1.mean(level=1, axis=0).loc['train_acc'], 'std':table1.std(level=1).loc['train_acc']}, axis=1)\n",
        "  table1_test = pd.concat({'mean': table1.mean(level=1, axis=0).loc['test_acc'], 'std':table1.std(level=1).loc['test_acc']}, axis=1)\n",
        "  # table1 = pd.concat({'Train': table1_train, 'Test': table1_test}, axis=1)\n",
        "  table1 = table1_test\n",
        "  all_stats_df[data_key] = table1\n",
        "pd.concat(all_stats_df, axis=1).round(2)"
      ],
      "metadata": {
        "id": "ZNkbWu7zH06U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}